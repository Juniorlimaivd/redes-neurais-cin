{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projeto.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "22FDBji8AVbb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# IF702 - Redes Neurais\n",
        "\n",
        "#### Equipe\n",
        "* Bruno César - bcgs\n",
        "* Franclin Cabral - fcmo\n",
        "* Ítalo Rodrigo Barbosa Paulino - irbp\n",
        "* José Nilton de Oliveira Lima Júnior - jnolj\n",
        "\n",
        "Este notebook contém os scripts executados em cada passo do projeto da disciplina de Redes Neurais."
      ]
    },
    {
      "metadata": {
        "id": "_V1_Nx7F942d",
        "colab_type": "code",
        "outputId": "2afde630-4f1f-4f71-cd55-330f74fc360d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://cin.ufpe.br/~gcv/web_lci/TRN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-21 15:56:57--  http://cin.ufpe.br/~gcv/web_lci/TRN\n",
            "Resolving cin.ufpe.br (cin.ufpe.br)... 150.161.2.9\n",
            "Connecting to cin.ufpe.br (cin.ufpe.br)|150.161.2.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 766723352 (731M) [application/vnd.rim.cod]\n",
            "Saving to: ‘TRN’\n",
            "\n",
            "TRN                  33%[=====>              ] 245.10M  12.5MB/s    eta 41s    ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GwaC6c6zAVbh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Leitura e limpeza dos dados\n",
        "A leitura dos dados é feita utilizando a biblioteca pandas. Os trechos de código que seguem abaixo fazem a leitura do dataset de crédito \"TRN\" e a remoção de features que não nos interessam, mas estão presentes no dataset."
      ]
    },
    {
      "metadata": {
        "id": "ooX48t-jAVbk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/gdrive/My Drive/Projeto/dataset/\"\n",
        "MODELS_PATH = \"/content/gdrive/My Drive/Projeto/mlp_models/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLJewUO4AVbw",
        "colab_type": "code",
        "outputId": "fe9575d1-a02b-4615-e13b-eea87acda8b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Only for google colab\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "!pip3 install git+https://github.com/irbp/neuro-evolution.git\n",
        "!pip3 install -U imbalanced-learn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Collecting git+https://github.com/irbp/neuro-evolution.git\n",
            "  Cloning https://github.com/irbp/neuro-evolution.git to /tmp/pip-req-build-8dwx2itt\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from neuro-evolution==0.1) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from neuro-evolution==0.1) (1.14.6)\n",
            "Collecting logger (from neuro-evolution==0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/73/2f/b0d28eaa1e2c1cf64129f8da3fe701888d152677fec708cd0f13e8309e1e/logger-1.4.tar.gz\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from neuro-evolution==0.1) (2.2.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->neuro-evolution==0.1) (1.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->neuro-evolution==0.1) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->neuro-evolution==0.1) (1.0.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->neuro-evolution==0.1) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->neuro-evolution==0.1) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->neuro-evolution==0.1) (3.13)\n",
            "Building wheels for collected packages: neuro-evolution, logger\n",
            "  Running setup.py bdist_wheel for neuro-evolution ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-ko_l6urg/wheels/8c/4d/ee/df62a991bcd07cf7b03828263158b53fa8d14c760b24834f62\n",
            "  Running setup.py bdist_wheel for logger ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/91/d4/96/08341e2ac92c1ed4b760e4848e1acda3795f0257a83b94b42e\n",
            "Successfully built neuro-evolution logger\n",
            "Installing collected packages: logger, neuro-evolution\n",
            "Successfully installed logger-1.4 neuro-evolution-0.1\n",
            "Collecting imbalanced-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/4c/7557e1c2e791bd43878f8c82065bddc5798252084f26ef44527c02262af1/imbalanced_learn-0.4.3-py3-none-any.whl (166kB)\n",
            "\u001b[K    100% |████████████████████████████████| 174kB 6.6MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.20 (from imbalanced-learn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/26/d04320c3edf2d59b1fcd0720b46753d4d603a76e68d8ad10a9b92ab06db2/scikit_learn-0.20.1-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.4MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Installing collected packages: scikit-learn, imbalanced-learn\n",
            "  Found existing installation: scikit-learn 0.19.2\n",
            "    Uninstalling scikit-learn-0.19.2:\n",
            "      Successfully uninstalled scikit-learn-0.19.2\n",
            "Successfully installed imbalanced-learn-0.4.3 scikit-learn-0.20.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ip2hZJECO95",
        "colab_type": "code",
        "outputId": "d79796a2-e212-4cff-c6c6-e2d8ef6544ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Only for google colab\"\"\"\n",
        "!wget http://cin.ufpe.br/~gcv/web_lci/TRN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-03 13:45:18--  http://cin.ufpe.br/~gcv/web_lci/TRN\n",
            "Resolving cin.ufpe.br (cin.ufpe.br)... 150.161.2.9\n",
            "Connecting to cin.ufpe.br (cin.ufpe.br)|150.161.2.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 766723352 (731M) [application/vnd.rim.cod]\n",
            "Saving to: ‘TRN’\n",
            "\n",
            "TRN                 100%[===================>] 731.20M  6.41MB/s    in 84s     \n",
            "\n",
            "2018-12-03 13:46:42 (8.74 MB/s) - ‘TRN’ saved [766723352/766723352]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SsHHhVSgAVb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JGPtAuAiAVb_",
        "colab_type": "code",
        "outputId": "183035ea-09df-42e4-b6f5-3751c308d635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_table(\"TRN\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>INDEX</th>\n",
              "      <th>UF_1</th>\n",
              "      <th>UF_2</th>\n",
              "      <th>UF_3</th>\n",
              "      <th>UF_4</th>\n",
              "      <th>UF_5</th>\n",
              "      <th>UF_6</th>\n",
              "      <th>UF_7</th>\n",
              "      <th>IDADE</th>\n",
              "      <th>SEXO_1</th>\n",
              "      <th>...</th>\n",
              "      <th>CEP4_7</th>\n",
              "      <th>CEP4_8</th>\n",
              "      <th>CEP4_9</th>\n",
              "      <th>CEP4_10</th>\n",
              "      <th>CEP4_11</th>\n",
              "      <th>CEP4_12</th>\n",
              "      <th>CEP4_13</th>\n",
              "      <th>CEP4_14</th>\n",
              "      <th>IND_BOM_1_1</th>\n",
              "      <th>IND_BOM_1_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.135098</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.273504</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.281910</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.225741</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.480403</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 246 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   INDEX  UF_1  UF_2  UF_3  UF_4  UF_5  UF_6  UF_7     IDADE  SEXO_1  \\\n",
              "0      0     1     1     1     0     0     0     0  0.135098       1   \n",
              "1      1     1     0     1     0     0     1     0  0.273504       1   \n",
              "2      2     1     0     1     0     0     1     0  0.281910       0   \n",
              "3      3     1     1     1     0     0     0     0  0.225741       0   \n",
              "4      4     1     1     0     0     0     1     0  0.480403       0   \n",
              "\n",
              "      ...       CEP4_7  CEP4_8  CEP4_9  CEP4_10  CEP4_11  CEP4_12  CEP4_13  \\\n",
              "0     ...            0       0       1        1        0        1        1   \n",
              "1     ...            0       1       0        1        1        0        0   \n",
              "2     ...            1       1       0        0        0        0        1   \n",
              "3     ...            1       1       0        1        1        0        1   \n",
              "4     ...            1       1       1        0        0        1        0   \n",
              "\n",
              "   CEP4_14  IND_BOM_1_1  IND_BOM_1_2  \n",
              "0        1            0            1  \n",
              "1        0            1            0  \n",
              "2        0            1            0  \n",
              "3        0            1            0  \n",
              "4        1            1            0  \n",
              "\n",
              "[5 rows x 246 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "BNsZkdCLAVcD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Divisão dos dados em treino, teste e validação"
      ]
    },
    {
      "metadata": {
        "id": "IupHaHBBAVcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 1:-2].values\n",
        "y = df.iloc[:, -2].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=1/4,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
        "                                                  y_train,\n",
        "                                                  test_size=1/3,\n",
        "                                                  random_state=42,\n",
        "                                                  stratify=y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SWGXjumHAVcK",
        "colab_type": "code",
        "outputId": "adcb6626-24a4-4f7b-b1fe-66a2c35bd958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "cell_type": "code",
      "source": [
        "df[\"IND_BOM_1_1\"].value_counts().plot.bar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7eff79bbd5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD0CAYAAAB0KjqYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7eff7047ae10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "yOfdaCpYAVcR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sampling dos dados e normalização\n",
        "\n",
        "Como podemos ver no gráfico plotado acima, há um desbalanceamento entre as duas classes presentes no dataset. Para resolver este problema vamos utilizar uma técnica de oversampling bem simples que consiste em escolher algumas instâncias aleatórias da classe minoritária e replicá-las algumas vezes. Mas antes disso vamos normalizar a nossa base de dados. É interessante fazer a normalização antes do under/oversampling dos dados pois técnicas como o SMOTE fazem uso de algoritmos como o k-NN para gerar os samples, e estes obtém um melhor desempenho com dados normalizados."
      ]
    },
    {
      "metadata": {
        "id": "rlgNnGvmAVcV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0JOOskadAVce",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora vamos aplicar a técnica de oversampling nos conjuntos de treino, teste e validação, assim como verificar se a proporção está de 1/2 para cada classe dos conjuntos."
      ]
    },
    {
      "metadata": {
        "id": "LSbrMvJOAVcg",
        "colab_type": "code",
        "outputId": "3201e5d0-b2a0-4c3d-da4c-9e65d6241132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "# Oversampling\n",
        "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
        "X_test, y_test = ros.fit_resample(X_test, y_test)\n",
        "X_val, y_val = ros.fit_resample(X_val, y_val)\n",
        "\n",
        "print(\"***Train***\")\n",
        "print(pd.value_counts(pd.Series(y_train), normalize=True))\n",
        "print()\n",
        "print(\"***Test***\")\n",
        "print(pd.value_counts(pd.Series(y_test), normalize=True))\n",
        "print()\n",
        "print(\"***Validation***\")\n",
        "print(pd.value_counts(pd.Series(y_val), normalize=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***Train***\n",
            "1    0.5\n",
            "0    0.5\n",
            "dtype: float64\n",
            "\n",
            "***Test***\n",
            "1    0.5\n",
            "0    0.5\n",
            "dtype: float64\n",
            "\n",
            "***Validation***\n",
            "1    0.5\n",
            "0    0.5\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uxYVix6wAVcq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Seleção de features\n",
        "Como o dataset apresenta bastante features se torna importante fazer uma análise de quais delas são realmente relevantes para representar o nosso problema. Features irrelevantes além tornar o tempo de treinamento maior também podem diminuir a acurácia do nosso modelo. Sendo assim, nos trechos abaixo vamos fazer uma análise de quais features são mais relevantes para representar o nosso problema."
      ]
    },
    {
      "metadata": {
        "id": "V2agh7e-AVcs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Fitting the PCA algorithm with our Data\n",
        "pca = PCA().fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IKe-yOt9AVcv",
        "colab_type": "code",
        "outputId": "e2a24d46-aff3-4e48-bbed-e31d9a793ae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "# Plotting the Cumulative Summation of the Explained Variance\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Número de componentes')\n",
        "plt.ylabel('Variância (%)')\n",
        "plt.title('Variância do conjunto de treino')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEVCAYAAAACW4lMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7eff7047a518>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "poJ6ozIMAVc1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "n_components = np.arange(X_train.shape[1])[cumsum >= 0.9][0] + 1\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_val = pca.transform(X_val)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "np.save(DATASET_PATH + \"X_train.npy\", X_train)\n",
        "np.save(DATASET_PATH + \"y_train.npy\", y_train)\n",
        "np.save(DATASET_PATH + \"X_test.npy\", X_test)\n",
        "np.save(DATASET_PATH + \"y_test.npy\", y_test)\n",
        "np.save(DATASET_PATH + \"X_val.npy\", X_val)\n",
        "np.save(DATASET_PATH + \"y_val.npy\", y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Yq9a1QgAVc8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Definição dos parâmetros da MLP\n",
        "\n",
        "Pensando em obter os melhores parâmetros possíveis e ao mesmo tempo visando o desempenho, nós optamos por utilizar algoritmo genético, pois este apresenta resultado mais rápido do que o grid search. Ao invés de testar todas as possíveis combinações de hiperparâmetros, o algoritmo genético irá selecionar os melhores indivíduos onde cada indivíduo representa uma combinação de hiperparâmetros. A métrica de avaliação do indivíduo é a acurácia do conjunto de validação, ou seja, para cada indivíduo uma rede neural é criada e treinada utilizando os parâmetros que este indíviduo está codificando. Após treinada ela é avaliada e o resultado dessa avaliação é utilizado para saber quais indivíduos irão representar aquela geração e assim criar indivíduos ainda melhores.\n",
        "\n",
        "Para isso nós estamos fazendo uso da bilioteca neuro-evolution (https://github.com/irbp/neuro-evolution), pois ela já apresenta tudo o que é necessário para realizarmos o que foi descrito acima."
      ]
    },
    {
      "metadata": {
        "id": "g-2ntgaHAVdL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    X_train = np.load(DATASET_PATH + \"X_train.npy\")\n",
        "    y_train = np.load(DATASET_PATH + \"y_train.npy\")\n",
        "    X_val = np.load(DATASET_PATH + \"X_val.npy\")\n",
        "    y_val = np.load(DATASET_PATH + \"y_val.npy\")\n",
        "    X_test = np.load(DATASET_PATH + \"X_test.npy\")\n",
        "    y_test = np.load(DATASET_PATH + \"y_test.npy\")\n",
        "    \n",
        "    train_perm = np.random.permutation(X_train.shape[0])\n",
        "    X_train, y_train = X_train[train_perm], y_train[train_perm]\n",
        "    val_perm = np.random.permutation(X_val.shape[0])\n",
        "    X_val, y_val = X_val[val_perm], y_val[val_perm]\n",
        "    test_perm = np.random.permutation(X_test.shape[0])\n",
        "    X_test, y_test = X_test[test_perm], y_test[test_perm]\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Cx1Lr0IAVc-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from neuro_evolution.evolution import NeuroEvolution\n",
        "\n",
        "params = {\n",
        "    \"batch_size\": [1024],\n",
        "    \"n_layers\": [1, 2, 3],\n",
        "    \"n_neurons\": [256, 512, 1024],\n",
        "    \"dropout\": [0.1, 0.25, 0.5],\n",
        "    \"optimizers\": [\"rmsprop\", \"adam\"],\n",
        "    \"activations\": [\"relu\", \"sigmoid\", \"tanh\"],\n",
        "    \"last_layer_activations\": [\"sigmoid\"],\n",
        "    \"losses\": [\"binary_crossentropy\"],\n",
        "    \"metrics\": [\"accuracy\"],\n",
        "    \"verbose\": [0]\n",
        "}\n",
        "# search = NeuroEvolution(generations=10, population=10, params=params)\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfcrnwBsAVdE",
        "colab_type": "code",
        "outputId": "66380c95-842f-499c-c87d-0fdf2ae9a67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3267
        }
      },
      "cell_type": "code",
      "source": [
        "search.evolve(X_train, y_train, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:22<03:26, 22.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6302098033307588\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [00:41<02:53, 21.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6206447767692462\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [01:26<03:19, 28.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6172970174905681\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:04<03:08, 31.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6386928842568452\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [02:30<02:28, 29.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6229576316853176\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [02:56<01:54, 28.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6360899425649791\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [03:42<01:41, 33.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.5930159625077093\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [04:20<01:10, 35.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6272305328171479\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [04:46<00:32, 32.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6191943425781083\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [05:17<00:00, 32.15s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6371248472378807\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:49<07:26, 49.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6370229247234187\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:28<06:11, 46.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6391789758254388\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [02:01<04:55, 42.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6301941230566485\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:29<03:49, 38.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.623506444587747\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [03:07<03:10, 38.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6201351648137872\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [03:51<02:38, 39.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.5910794366395615\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [04:24<01:53, 37.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6251215229842088\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [04:56<01:12, 36.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6297707529695611\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [05:21<00:32, 32.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6348747138425698\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [05:44<00:00, 29.82s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6170696521495485\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:31<04:42, 31.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6403863644706088\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:06<04:20, 32.60s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6364035501538273\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [01:32<03:34, 30.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6340201336023713\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:01<03:00, 30.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6321149686039547\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [02:29<02:26, 29.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.635431366977386\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [02:54<01:52, 28.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6293552230830833\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [03:42<01:41, 33.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6384341580330097\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [04:19<01:09, 34.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6364584312907922\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [04:41<00:31, 31.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6354313669661705\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [05:13<00:00, 31.38s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6332282748548945\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:33<05:00, 33.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6384733590042804\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:30<05:24, 40.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6368347603611466\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [02:21<05:06, 43.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6343023803401809\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:59<04:11, 41.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6373992535189941\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [03:49<03:41, 44.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6381283909475889\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [04:24<02:45, 41.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6396180262184137\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [05:13<02:11, 43.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6380421487390144\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [05:54<01:26, 43.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6381989526726974\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [06:40<00:43, 43.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6351491203947238\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [07:23<00:00, 43.76s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6381127106304858\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:26<03:56, 26.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6386693636456703\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:10<04:12, 31.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.635713613649772\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [01:48<03:55, 33.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6391789756665529\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:42<03:57, 39.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6384733589538107\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [03:17<03:12, 38.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6368190799935739\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [03:49<02:25, 36.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6403863643621925\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [04:43<02:04, 41.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6359174586394419\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [05:16<01:18, 39.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6340671747125664\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [05:53<00:38, 38.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.639845391609903\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [06:42<00:00, 41.54s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6359096183995782\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:42<06:18, 42.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6390613729789197\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:13<05:10, 38.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6375952583155313\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [01:48<04:25, 37.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6399394738882398\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:23<03:40, 36.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6354078465755666\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [03:01<03:06, 37.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6383949573103488\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [03:43<02:34, 38.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6396807477036585\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [04:36<02:09, 43.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6349295951384205\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [05:40<01:38, 49.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6406764511267719\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [06:13<00:44, 44.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6390535326960634\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [06:53<00:00, 42.97s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6395004234915264\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [01:04<09:44, 64.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6414369491895727\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:39<07:26, 55.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6405902091350298\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [02:12<05:43, 49.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6405039670460871\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:57<04:47, 47.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6383714366991738\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [04:02<04:24, 52.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6397042683223104\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [05:20<04:01, 60.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6395396243076497\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [05:50<02:33, 51.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6404255651820542\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [06:40<01:41, 50.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6386301626631842\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [07:35<00:52, 52.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6394612225595099\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [08:28<00:00, 52.51s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6343572616977167\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:23<03:33, 23.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6344356634421179\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [00:56<03:32, 26.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6407156520513113\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [01:33<03:26, 29.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6384419982653965\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:27<03:41, 36.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6362467463902458\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [03:11<03:15, 39.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6392730580495675\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [03:42<02:26, 36.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6395317840173163\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [04:30<02:00, 40.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6397277888830156\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [05:11<01:20, 40.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6391319345488808\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [05:56<00:41, 41.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6380107880506002\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [06:46<00:00, 44.13s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6394298617084713\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:47<07:09, 47.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6407626932157146\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:35<06:21, 47.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6401746794504307\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [02:23<05:35, 47.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.640002195369746\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [03:04<04:34, 45.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6398375514242474\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [03:53<03:53, 46.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6385125599325584\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [04:24<02:48, 42.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6409900585529956\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [04:59<01:59, 39.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6407313325889854\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [05:52<01:27, 43.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6370856463058642\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [06:35<00:43, 43.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6402217204671635\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10/10 [07:11<00:00, 41.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.640449085897907\n",
            "[03/Dec/2018 15:01:12] INFO - best accuracy: 0.6409900585529956, best params: {'batch_size': 1024, 'n_layers': 3, 'n_neurons': 512, 'dropout': 0.5, 'optimizers': 'adam', 'activations': 'relu', 'last_layer_activations': 'sigmoid', 'losses': 'binary_crossentropy', 'metrics': 'accuracy', 'verbose': 0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "g7f_HOrjZBko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Treino da MLP\n",
        "\n",
        "Logo abaixo a arquitetura da nossa rede será definida utilizando os parâmetros escolhidos pelo algoritmo genético executado acima. Após definida a arquitetura, a rede será treinada utilizando o conjunto de treino e para ao early stopping será utilizado o conjunto de validação. Logo após o conjunto de teste será avaliado utilizando as métricas exigidas pelo projeto."
      ]
    },
    {
      "metadata": {
        "id": "HSwvsb0wAVdI",
        "colab_type": "code",
        "outputId": "c5e24243-1a30-41a1-80af-9628bcc2f8f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install scikit-plot\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "import scikitplot as skplt\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-plot\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/47/32520e259340c140a4ad27c1b97050dd3254fdc517b1d59974d47037510e/scikit_plot-0.3.7-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (0.20.1)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (0.13.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->scikit-plot) (1.14.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.11.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2018.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9pxeymIhAVdU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(input_dim, output_dim, params):\n",
        "    n_layers = params[\"n_layers\"]\n",
        "    n_neurons = params[\"n_neurons\"]\n",
        "    dropout = params[\"dropout\"]\n",
        "    optimizer = params[\"optimizer\"]\n",
        "    activation = params[\"activation\"]\n",
        "    last_layer_activation = params[\"last_layer_activation\"]\n",
        "    loss = params[\"loss\"]\n",
        "    metrics = params[\"metrics\"]\n",
        "    \n",
        "    model = Sequential()\n",
        "    # Hidden layers\n",
        "    for n in range(n_layers):\n",
        "        if n == 0:\n",
        "            model.add(Dense(n_neurons[n], activation=activation, input_dim=input_dim))\n",
        "        else:\n",
        "            model.add(Dense(n_neurons[n], activation=activation))\n",
        "        if dropout != 0.0:\n",
        "            model.add(Dropout(dropout))\n",
        "    # Output layer\n",
        "    model.add(Dense(output_dim, activation=last_layer_activation))\n",
        "    \n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MQDYdtXBp40N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"n_layers\": 2,\n",
        "    \"n_neurons\": [256, 256],\n",
        "    \"dropout\": 0.25,\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"activation\": \"sigmoid\",\n",
        "    \"last_layer_activation\": \"sigmoid\",\n",
        "    \"loss\": \"binary_crossentropy\",\n",
        "    \"metrics\": [\"accuracy\"],\n",
        "}\n",
        "model = create_model(X_train.shape[1], 1, params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v7dGGRc7qNB4",
        "colab_type": "code",
        "outputId": "9bd4c51c-e61b-4ce7-b4a2-bcafe751ec08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=1024,\n",
        "                    epochs=10000,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[EarlyStopping(patience=5)])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 255098 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "255098/255098 [==============================] - 3s 10us/step - loss: 0.6639 - acc: 0.6007 - val_loss: 0.6453 - val_acc: 0.6271\n",
            "Epoch 2/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6479 - acc: 0.6228 - val_loss: 0.6431 - val_acc: 0.6290\n",
            "Epoch 3/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6443 - acc: 0.6258 - val_loss: 0.6422 - val_acc: 0.6302\n",
            "Epoch 4/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6426 - acc: 0.6278 - val_loss: 0.6417 - val_acc: 0.6311\n",
            "Epoch 5/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6414 - acc: 0.6294 - val_loss: 0.6411 - val_acc: 0.6308\n",
            "Epoch 6/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6398 - acc: 0.6311 - val_loss: 0.6385 - val_acc: 0.6341\n",
            "Epoch 7/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6373 - acc: 0.6342 - val_loss: 0.6358 - val_acc: 0.6372\n",
            "Epoch 8/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6353 - acc: 0.6361 - val_loss: 0.6361 - val_acc: 0.6368\n",
            "Epoch 9/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6343 - acc: 0.6370 - val_loss: 0.6346 - val_acc: 0.6385\n",
            "Epoch 10/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6335 - acc: 0.6378 - val_loss: 0.6342 - val_acc: 0.6385\n",
            "Epoch 11/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6327 - acc: 0.6384 - val_loss: 0.6347 - val_acc: 0.6377\n",
            "Epoch 12/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6320 - acc: 0.6387 - val_loss: 0.6341 - val_acc: 0.6400\n",
            "Epoch 13/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6312 - acc: 0.6394 - val_loss: 0.6339 - val_acc: 0.6402\n",
            "Epoch 14/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6299 - acc: 0.6401 - val_loss: 0.6336 - val_acc: 0.6394\n",
            "Epoch 15/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6293 - acc: 0.6409 - val_loss: 0.6333 - val_acc: 0.6403\n",
            "Epoch 16/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6287 - acc: 0.6415 - val_loss: 0.6334 - val_acc: 0.6402\n",
            "Epoch 17/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6280 - acc: 0.6420 - val_loss: 0.6333 - val_acc: 0.6396\n",
            "Epoch 18/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6268 - acc: 0.6435 - val_loss: 0.6338 - val_acc: 0.6403\n",
            "Epoch 19/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6261 - acc: 0.6429 - val_loss: 0.6331 - val_acc: 0.6408\n",
            "Epoch 20/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6252 - acc: 0.6443 - val_loss: 0.6335 - val_acc: 0.6405\n",
            "Epoch 21/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6245 - acc: 0.6454 - val_loss: 0.6337 - val_acc: 0.6409\n",
            "Epoch 22/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6232 - acc: 0.6467 - val_loss: 0.6331 - val_acc: 0.6407\n",
            "Epoch 23/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6223 - acc: 0.6469 - val_loss: 0.6347 - val_acc: 0.6402\n",
            "Epoch 24/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6211 - acc: 0.6480 - val_loss: 0.6344 - val_acc: 0.6408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "arEeSG3msvo5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Predições no conjunto de teste"
      ]
    },
    {
      "metadata": {
        "id": "rdJvcUk9rJbo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_performance_metrics(y, y_pred_class, y_pred_scores=None):\n",
        "    accuracy = accuracy_score(y, y_pred_class)\n",
        "    recall = recall_score(y, y_pred_class)\n",
        "    precision = precision_score(y, y_pred_class)\n",
        "    f1 = f1_score(y, y_pred_class)\n",
        "    performance_metrics = (accuracy, recall, precision, f1)\n",
        "    if y_pred_scores is not None:\n",
        "        skplt.metrics.plot_ks_statistic(y, y_pred_scores)\n",
        "        plt.show()\n",
        "        y_pred_scores = y_pred_scores[:, 1]\n",
        "        auroc = roc_auc_score(y, y_pred_scores)\n",
        "        aupr = average_precision_score(y, y_pred_scores)\n",
        "        performance_metrics = performance_metrics + (auroc, aupr)\n",
        "    return performance_metrics\n",
        "\n",
        "def print_metrics_summary(accuracy, recall, precision, f1, auroc=None, aupr=None):\n",
        "    print()\n",
        "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Accuracy:\", value=accuracy))\n",
        "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Recall:\", value=recall))\n",
        "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Precision:\", value=precision))\n",
        "    print(\"{metric:<18}{value:.4f}\".format(metric=\"F1:\", value=f1))\n",
        "    if auroc is not None:\n",
        "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUROC:\", value=auroc))\n",
        "    if aupr is not None:\n",
        "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUPR:\", value=aupr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dr5kzA27r4xB",
        "colab_type": "code",
        "outputId": "276fe4d0-31d0-425f-c1f2-bdfb87847bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "cell_type": "code",
      "source": [
        "# Fazer predições no conjunto de teste\n",
        "y_pred_scores = model.predict(X_test)\n",
        "y_pred_class = model.predict_classes(X_test, verbose=0)\n",
        "y_pred_scores_0 = 1 - y_pred_scores\n",
        "y_pred_scores = np.concatenate([y_pred_scores_0, y_pred_scores], axis=1)\n",
        "\n",
        "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_test, y_pred_class, y_pred_scores)\n",
        "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEVCAYAAAALsCk2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH8NHdy80FLZMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7efdfae464a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy:         0.6412\n",
            "Recall:           0.5896\n",
            "Precision:        0.6574\n",
            "F1:               0.6217\n",
            "AUROC:            0.6939\n",
            "AUPR:             0.6923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WD_gLWZFaPtn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ensemble de MLP's\n",
        "Nosso próximo passo será criar um ensemble de MLP's para avaliarmos, para isso utilizaremos o método bagging. Várias redes serão treinadas de forma independentes utilizando subconjuntos do conjunto original de treino,  onde esse subconjuntos podem se sobrepôr. Além disso, cada subconjunto contém suas próprias features, ou seja, cada rede é treinada utilizando um conjuno de features definido de forma aleatória. Isso permite que cada rede consiga se \"especializar\" em características diferentes do dataset. No momento de avaliação cada rede neural é submetida ao conjunto de teste e suas predições são obtidas para compor a predição final do ensemble, onde esta predição final é definida pela maioria dos votos das redes."
      ]
    },
    {
      "metadata": {
        "id": "8cCq4siwAVda",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_emsemble(n_models, instances_ratio, features_ratio, params):\n",
        "    models = []\n",
        "    features_idx = []\n",
        "    for i in range(n_models):\n",
        "        features = np.random.choice(range(X_train.shape[1]),\n",
        "                                    round(X_train.shape[1] * features_ratio))\n",
        "        \n",
        "        _, xi_train, _, yi_train = train_test_split(X_train,\n",
        "                                                    y_train,\n",
        "                                                    test_size=instances_ratio,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_train)\n",
        "        \n",
        "        xi_train = xi_train[:, features]\n",
        "        xi_val = X_val[:, features]\n",
        "    \n",
        "        model = create_model(xi_train.shape[1], 1, params)\n",
        "        print()\n",
        "        print(\"-----------------Training MLP {}-----------------\".format(i))\n",
        "        print()\n",
        "        model.fit(xi_train, yi_train,\n",
        "                  batch_size=1024,\n",
        "                  epochs=10000,\n",
        "                  validation_data=(xi_val, y_val),\n",
        "                  callbacks=[EarlyStopping(patience=5)],\n",
        "                  verbose=1)\n",
        "        model.save_weights(MODELS_PATH + \"model-{}.hdf5\".format(i), overwrite=True)\n",
        "        \n",
        "        models.append(model)\n",
        "        features_idx.append(features)\n",
        "        \n",
        "        # Avoiding memory issues\n",
        "        K.clear_session()\n",
        "        \n",
        "    np.save(MODELS_PATH + \"features.npy\", np.array(features_idx))\n",
        "        \n",
        "    return models, features_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orEfjs9EAVdg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_model(features_idx, params):\n",
        "    predictions = []\n",
        "    for i, f in enumerate(features_idx):\n",
        "        xi_test = X_test[:, f]\n",
        "        model = create_model(xi_test.shape[1], 1, params)\n",
        "        model.load_weights(MODELS_PATH + \"model-{}.hdf5\".format(i))\n",
        "        pred = model.predict(xi_test, batch_size=1024, verbose=1)\n",
        "        predictions.append(pred)\n",
        "        \n",
        "        K.clear_session()\n",
        "    \n",
        "    pred_mean = np.round(np.array(predictions)[:,:,0].mean(axis=0))\n",
        "    pred_mean_score = np.array(predictions)[:,:,0].mean(axis=0)\n",
        "    pred_mean_score_0 = 1 - pred_mean_score\n",
        "    pred_mean_score = np.column_stack([pred_mean_score_0, pred_mean_score])\n",
        "    \n",
        "    accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_test, pred_mean, pred_mean_score)\n",
        "    \n",
        "    print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)\n",
        "\n",
        "    #accuracy = pred_mean[pred_mean == y_test].shape[0] / X_test.shape[0]\n",
        "    \n",
        "    #print(\"Accuray: {}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VeSggobSAVd4",
        "colab_type": "code",
        "outputId": "7ac8066d-a028-49ea-eadf-3c69a8ee5bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38797
        }
      },
      "cell_type": "code",
      "source": [
        "models, features = create_emsemble(20, 0.8, 0.5, params)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------------Training MLP 0-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 4s 21us/step - loss: 0.6950 - acc: 0.5393 - val_loss: 0.6806 - val_acc: 0.5671\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6817 - acc: 0.5634 - val_loss: 0.6808 - val_acc: 0.5657\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6797 - acc: 0.5671 - val_loss: 0.6787 - val_acc: 0.5698\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6783 - acc: 0.5714 - val_loss: 0.6777 - val_acc: 0.5717\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6774 - acc: 0.5721 - val_loss: 0.6775 - val_acc: 0.5716\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6763 - acc: 0.5744 - val_loss: 0.6758 - val_acc: 0.5753\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6751 - acc: 0.5757 - val_loss: 0.6746 - val_acc: 0.5784\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6742 - acc: 0.5782 - val_loss: 0.6731 - val_acc: 0.5807\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6723 - acc: 0.5826 - val_loss: 0.6717 - val_acc: 0.5842\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6709 - acc: 0.5862 - val_loss: 0.6709 - val_acc: 0.5860\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6703 - acc: 0.5867 - val_loss: 0.6695 - val_acc: 0.5895\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6696 - acc: 0.5877 - val_loss: 0.6688 - val_acc: 0.5908\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6694 - acc: 0.5894 - val_loss: 0.6687 - val_acc: 0.5928\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6685 - acc: 0.5904 - val_loss: 0.6684 - val_acc: 0.5929\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6678 - acc: 0.5913 - val_loss: 0.6683 - val_acc: 0.5911\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6676 - acc: 0.5919 - val_loss: 0.6675 - val_acc: 0.5935\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6669 - acc: 0.5930 - val_loss: 0.6670 - val_acc: 0.5952\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6663 - acc: 0.5944 - val_loss: 0.6674 - val_acc: 0.5938\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6657 - acc: 0.5948 - val_loss: 0.6665 - val_acc: 0.5954\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6653 - acc: 0.5956 - val_loss: 0.6661 - val_acc: 0.5973\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6649 - acc: 0.5957 - val_loss: 0.6659 - val_acc: 0.5976\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6638 - acc: 0.5973 - val_loss: 0.6653 - val_acc: 0.5976\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6638 - acc: 0.5974 - val_loss: 0.6653 - val_acc: 0.5975\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6632 - acc: 0.5988 - val_loss: 0.6645 - val_acc: 0.5994\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6629 - acc: 0.5989 - val_loss: 0.6642 - val_acc: 0.5998\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6622 - acc: 0.6008 - val_loss: 0.6638 - val_acc: 0.5998\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6615 - acc: 0.5996 - val_loss: 0.6637 - val_acc: 0.6009\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6611 - acc: 0.6020 - val_loss: 0.6637 - val_acc: 0.6007\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6603 - acc: 0.6024 - val_loss: 0.6630 - val_acc: 0.6016\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6600 - acc: 0.6035 - val_loss: 0.6635 - val_acc: 0.6011\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6595 - acc: 0.6038 - val_loss: 0.6633 - val_acc: 0.6019\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6589 - acc: 0.6052 - val_loss: 0.6629 - val_acc: 0.6023\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6586 - acc: 0.6054 - val_loss: 0.6627 - val_acc: 0.6017\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6574 - acc: 0.6072 - val_loss: 0.6622 - val_acc: 0.6025\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6576 - acc: 0.6074 - val_loss: 0.6623 - val_acc: 0.6031\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6568 - acc: 0.6077 - val_loss: 0.6621 - val_acc: 0.6038\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6563 - acc: 0.6093 - val_loss: 0.6615 - val_acc: 0.6040\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6555 - acc: 0.6092 - val_loss: 0.6621 - val_acc: 0.6031\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6546 - acc: 0.6105 - val_loss: 0.6624 - val_acc: 0.6027\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6543 - acc: 0.6114 - val_loss: 0.6611 - val_acc: 0.6050\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6538 - acc: 0.6104 - val_loss: 0.6615 - val_acc: 0.6045\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6530 - acc: 0.6123 - val_loss: 0.6609 - val_acc: 0.6047\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6526 - acc: 0.6128 - val_loss: 0.6609 - val_acc: 0.6041\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6517 - acc: 0.6136 - val_loss: 0.6609 - val_acc: 0.6045\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6517 - acc: 0.6155 - val_loss: 0.6605 - val_acc: 0.6050\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6506 - acc: 0.6148 - val_loss: 0.6606 - val_acc: 0.6053\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6501 - acc: 0.6163 - val_loss: 0.6608 - val_acc: 0.6048\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6498 - acc: 0.6151 - val_loss: 0.6607 - val_acc: 0.6050\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6487 - acc: 0.6176 - val_loss: 0.6607 - val_acc: 0.6052\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6484 - acc: 0.6171 - val_loss: 0.6604 - val_acc: 0.6046\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6476 - acc: 0.6183 - val_loss: 0.6604 - val_acc: 0.6056\n",
            "Epoch 52/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6472 - acc: 0.6192 - val_loss: 0.6600 - val_acc: 0.6050\n",
            "Epoch 53/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6467 - acc: 0.6203 - val_loss: 0.6604 - val_acc: 0.6055\n",
            "Epoch 54/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6464 - acc: 0.6202 - val_loss: 0.6601 - val_acc: 0.6054\n",
            "Epoch 55/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6456 - acc: 0.6207 - val_loss: 0.6602 - val_acc: 0.6061\n",
            "Epoch 56/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6453 - acc: 0.6205 - val_loss: 0.6602 - val_acc: 0.6058\n",
            "Epoch 57/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6445 - acc: 0.6228 - val_loss: 0.6600 - val_acc: 0.6070\n",
            "Epoch 58/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6441 - acc: 0.6232 - val_loss: 0.6601 - val_acc: 0.6063\n",
            "Epoch 59/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6436 - acc: 0.6240 - val_loss: 0.6605 - val_acc: 0.6042\n",
            "Epoch 60/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6431 - acc: 0.6248 - val_loss: 0.6601 - val_acc: 0.6066\n",
            "Epoch 61/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6422 - acc: 0.6239 - val_loss: 0.6603 - val_acc: 0.6061\n",
            "Epoch 62/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6413 - acc: 0.6261 - val_loss: 0.6598 - val_acc: 0.6076\n",
            "Epoch 63/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6409 - acc: 0.6260 - val_loss: 0.6603 - val_acc: 0.6067\n",
            "Epoch 64/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6412 - acc: 0.6259 - val_loss: 0.6609 - val_acc: 0.6060\n",
            "Epoch 65/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6398 - acc: 0.6270 - val_loss: 0.6611 - val_acc: 0.6055\n",
            "Epoch 66/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.6266 - val_loss: 0.6602 - val_acc: 0.6075\n",
            "Epoch 67/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6392 - acc: 0.6277 - val_loss: 0.6605 - val_acc: 0.6062\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efeaa5684e0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 1-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6915 - acc: 0.5502 - val_loss: 0.6739 - val_acc: 0.5770\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6769 - acc: 0.5719 - val_loss: 0.6728 - val_acc: 0.5823\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6734 - acc: 0.5786 - val_loss: 0.6708 - val_acc: 0.5833\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6712 - acc: 0.5819 - val_loss: 0.6691 - val_acc: 0.5854\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6690 - acc: 0.5860 - val_loss: 0.6673 - val_acc: 0.5909\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6676 - acc: 0.5887 - val_loss: 0.6662 - val_acc: 0.5913\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6663 - acc: 0.5906 - val_loss: 0.6651 - val_acc: 0.5938\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6651 - acc: 0.5931 - val_loss: 0.6642 - val_acc: 0.5965\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6640 - acc: 0.5943 - val_loss: 0.6628 - val_acc: 0.5985\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6626 - acc: 0.5970 - val_loss: 0.6613 - val_acc: 0.6002\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6618 - acc: 0.5988 - val_loss: 0.6604 - val_acc: 0.6044\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6604 - acc: 0.6011 - val_loss: 0.6593 - val_acc: 0.6048\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6595 - acc: 0.6024 - val_loss: 0.6590 - val_acc: 0.6060\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6584 - acc: 0.6046 - val_loss: 0.6578 - val_acc: 0.6069\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6574 - acc: 0.6061 - val_loss: 0.6574 - val_acc: 0.6087\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6566 - acc: 0.6077 - val_loss: 0.6565 - val_acc: 0.6101\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6558 - acc: 0.6082 - val_loss: 0.6557 - val_acc: 0.6095\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6547 - acc: 0.6097 - val_loss: 0.6559 - val_acc: 0.6106\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6543 - acc: 0.6102 - val_loss: 0.6549 - val_acc: 0.6112\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6537 - acc: 0.6114 - val_loss: 0.6543 - val_acc: 0.6133\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6531 - acc: 0.6116 - val_loss: 0.6539 - val_acc: 0.6131\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6523 - acc: 0.6136 - val_loss: 0.6542 - val_acc: 0.6134\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6511 - acc: 0.6150 - val_loss: 0.6533 - val_acc: 0.6139\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6509 - acc: 0.6150 - val_loss: 0.6529 - val_acc: 0.6142\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6503 - acc: 0.6161 - val_loss: 0.6530 - val_acc: 0.6148\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6496 - acc: 0.6175 - val_loss: 0.6522 - val_acc: 0.6158\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6485 - acc: 0.6177 - val_loss: 0.6523 - val_acc: 0.6155\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6482 - acc: 0.6192 - val_loss: 0.6521 - val_acc: 0.6156\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6476 - acc: 0.6193 - val_loss: 0.6515 - val_acc: 0.6177\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6471 - acc: 0.6211 - val_loss: 0.6518 - val_acc: 0.6180\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6464 - acc: 0.6202 - val_loss: 0.6512 - val_acc: 0.6184\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6456 - acc: 0.6215 - val_loss: 0.6516 - val_acc: 0.6177\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6453 - acc: 0.6220 - val_loss: 0.6513 - val_acc: 0.6175\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6450 - acc: 0.6221 - val_loss: 0.6510 - val_acc: 0.6191\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6440 - acc: 0.6239 - val_loss: 0.6513 - val_acc: 0.6176\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6441 - acc: 0.6233 - val_loss: 0.6503 - val_acc: 0.6199\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6431 - acc: 0.6246 - val_loss: 0.6504 - val_acc: 0.6187\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6433 - acc: 0.6242 - val_loss: 0.6504 - val_acc: 0.6191\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6418 - acc: 0.6266 - val_loss: 0.6503 - val_acc: 0.6205\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6414 - acc: 0.6276 - val_loss: 0.6503 - val_acc: 0.6186\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6408 - acc: 0.6276 - val_loss: 0.6502 - val_acc: 0.6193\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6403 - acc: 0.6276 - val_loss: 0.6501 - val_acc: 0.6198\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6398 - acc: 0.6289 - val_loss: 0.6496 - val_acc: 0.6207\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6397 - acc: 0.6291 - val_loss: 0.6503 - val_acc: 0.6200\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6396 - acc: 0.6284 - val_loss: 0.6497 - val_acc: 0.6201\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6384 - acc: 0.6294 - val_loss: 0.6497 - val_acc: 0.6201\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6379 - acc: 0.6305 - val_loss: 0.6498 - val_acc: 0.6210\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6375 - acc: 0.6314 - val_loss: 0.6505 - val_acc: 0.6190\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe65689f98>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 2-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.7000 - acc: 0.5271 - val_loss: 0.6846 - val_acc: 0.5525\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6881 - acc: 0.5413 - val_loss: 0.6840 - val_acc: 0.5534\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6855 - acc: 0.5494 - val_loss: 0.6833 - val_acc: 0.5544\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6840 - acc: 0.5515 - val_loss: 0.6812 - val_acc: 0.5584\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6805 - acc: 0.5604 - val_loss: 0.6756 - val_acc: 0.5726\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6766 - acc: 0.5697 - val_loss: 0.6736 - val_acc: 0.5784\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6750 - acc: 0.5740 - val_loss: 0.6730 - val_acc: 0.5807\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6736 - acc: 0.5775 - val_loss: 0.6724 - val_acc: 0.5821\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6726 - acc: 0.5788 - val_loss: 0.6711 - val_acc: 0.5842\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6716 - acc: 0.5816 - val_loss: 0.6696 - val_acc: 0.5860\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6705 - acc: 0.5822 - val_loss: 0.6688 - val_acc: 0.5890\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6692 - acc: 0.5860 - val_loss: 0.6680 - val_acc: 0.5889\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6683 - acc: 0.5874 - val_loss: 0.6670 - val_acc: 0.5910\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6672 - acc: 0.5898 - val_loss: 0.6666 - val_acc: 0.5925\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6669 - acc: 0.5900 - val_loss: 0.6659 - val_acc: 0.5924\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6659 - acc: 0.5901 - val_loss: 0.6653 - val_acc: 0.5955\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6650 - acc: 0.5925 - val_loss: 0.6646 - val_acc: 0.5950\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6646 - acc: 0.5945 - val_loss: 0.6645 - val_acc: 0.5967\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6635 - acc: 0.5949 - val_loss: 0.6641 - val_acc: 0.5967\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6629 - acc: 0.5960 - val_loss: 0.6633 - val_acc: 0.5987\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6624 - acc: 0.5981 - val_loss: 0.6627 - val_acc: 0.5978\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6616 - acc: 0.5982 - val_loss: 0.6626 - val_acc: 0.5984\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6611 - acc: 0.5996 - val_loss: 0.6622 - val_acc: 0.6000\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6602 - acc: 0.6004 - val_loss: 0.6621 - val_acc: 0.5990\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6596 - acc: 0.6016 - val_loss: 0.6616 - val_acc: 0.6001\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6589 - acc: 0.6027 - val_loss: 0.6607 - val_acc: 0.6018\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6580 - acc: 0.6031 - val_loss: 0.6604 - val_acc: 0.6022\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6572 - acc: 0.6049 - val_loss: 0.6603 - val_acc: 0.6023\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6570 - acc: 0.6050 - val_loss: 0.6596 - val_acc: 0.6040\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6558 - acc: 0.6075 - val_loss: 0.6592 - val_acc: 0.6044\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6557 - acc: 0.6074 - val_loss: 0.6590 - val_acc: 0.6048\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6549 - acc: 0.6079 - val_loss: 0.6593 - val_acc: 0.6048\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6544 - acc: 0.6093 - val_loss: 0.6589 - val_acc: 0.6054\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6542 - acc: 0.6100 - val_loss: 0.6584 - val_acc: 0.6055\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6532 - acc: 0.6110 - val_loss: 0.6584 - val_acc: 0.6052\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6526 - acc: 0.6119 - val_loss: 0.6584 - val_acc: 0.6053\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6520 - acc: 0.6134 - val_loss: 0.6579 - val_acc: 0.6056\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6514 - acc: 0.6135 - val_loss: 0.6581 - val_acc: 0.6069\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6510 - acc: 0.6136 - val_loss: 0.6578 - val_acc: 0.6063\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6506 - acc: 0.6154 - val_loss: 0.6576 - val_acc: 0.6075\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6503 - acc: 0.6141 - val_loss: 0.6573 - val_acc: 0.6070\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6491 - acc: 0.6173 - val_loss: 0.6572 - val_acc: 0.6074\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6492 - acc: 0.6159 - val_loss: 0.6572 - val_acc: 0.6072\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6481 - acc: 0.6178 - val_loss: 0.6571 - val_acc: 0.6082\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6474 - acc: 0.6183 - val_loss: 0.6567 - val_acc: 0.6078\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6470 - acc: 0.6199 - val_loss: 0.6569 - val_acc: 0.6069\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6464 - acc: 0.6203 - val_loss: 0.6567 - val_acc: 0.6074\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6459 - acc: 0.6215 - val_loss: 0.6572 - val_acc: 0.6078\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6461 - acc: 0.6213 - val_loss: 0.6567 - val_acc: 0.6081\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6447 - acc: 0.6223 - val_loss: 0.6569 - val_acc: 0.6085\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe627aa080>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 3-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6915 - acc: 0.5485 - val_loss: 0.6756 - val_acc: 0.5770\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6793 - acc: 0.5688 - val_loss: 0.6753 - val_acc: 0.5762\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6775 - acc: 0.5722 - val_loss: 0.6746 - val_acc: 0.5777\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6759 - acc: 0.5745 - val_loss: 0.6733 - val_acc: 0.5795\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6746 - acc: 0.5776 - val_loss: 0.6718 - val_acc: 0.5820\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6728 - acc: 0.5819 - val_loss: 0.6699 - val_acc: 0.5869\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6710 - acc: 0.5851 - val_loss: 0.6682 - val_acc: 0.5882\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6696 - acc: 0.5877 - val_loss: 0.6674 - val_acc: 0.5900\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6679 - acc: 0.5904 - val_loss: 0.6659 - val_acc: 0.5926\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6670 - acc: 0.5917 - val_loss: 0.6648 - val_acc: 0.5937\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6657 - acc: 0.5942 - val_loss: 0.6633 - val_acc: 0.5975\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6638 - acc: 0.5977 - val_loss: 0.6617 - val_acc: 0.6000\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6623 - acc: 0.5995 - val_loss: 0.6601 - val_acc: 0.6043\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6604 - acc: 0.6025 - val_loss: 0.6590 - val_acc: 0.6063\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6589 - acc: 0.6064 - val_loss: 0.6571 - val_acc: 0.6090\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6578 - acc: 0.6075 - val_loss: 0.6566 - val_acc: 0.6109\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6565 - acc: 0.6098 - val_loss: 0.6562 - val_acc: 0.6124\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6556 - acc: 0.6110 - val_loss: 0.6546 - val_acc: 0.6131\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6546 - acc: 0.6134 - val_loss: 0.6542 - val_acc: 0.6137\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6535 - acc: 0.6147 - val_loss: 0.6536 - val_acc: 0.6155\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6530 - acc: 0.6156 - val_loss: 0.6534 - val_acc: 0.6167\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6524 - acc: 0.6159 - val_loss: 0.6530 - val_acc: 0.6146\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6516 - acc: 0.6170 - val_loss: 0.6529 - val_acc: 0.6168\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6508 - acc: 0.6176 - val_loss: 0.6524 - val_acc: 0.6179\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6502 - acc: 0.6182 - val_loss: 0.6519 - val_acc: 0.6169\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6497 - acc: 0.6190 - val_loss: 0.6518 - val_acc: 0.6183\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6490 - acc: 0.6197 - val_loss: 0.6511 - val_acc: 0.6184\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6483 - acc: 0.6212 - val_loss: 0.6510 - val_acc: 0.6173\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6480 - acc: 0.6210 - val_loss: 0.6509 - val_acc: 0.6184\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6476 - acc: 0.6212 - val_loss: 0.6511 - val_acc: 0.6183\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6470 - acc: 0.6218 - val_loss: 0.6505 - val_acc: 0.6180\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6463 - acc: 0.6227 - val_loss: 0.6505 - val_acc: 0.6186\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6455 - acc: 0.6241 - val_loss: 0.6501 - val_acc: 0.6196\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6450 - acc: 0.6243 - val_loss: 0.6506 - val_acc: 0.6186\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6448 - acc: 0.6246 - val_loss: 0.6498 - val_acc: 0.6198\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6444 - acc: 0.6255 - val_loss: 0.6504 - val_acc: 0.6209\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6433 - acc: 0.6266 - val_loss: 0.6499 - val_acc: 0.6194\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6434 - acc: 0.6259 - val_loss: 0.6498 - val_acc: 0.6200\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6426 - acc: 0.6276 - val_loss: 0.6494 - val_acc: 0.6203\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6426 - acc: 0.6268 - val_loss: 0.6495 - val_acc: 0.6206\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6415 - acc: 0.6290 - val_loss: 0.6493 - val_acc: 0.6202\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6410 - acc: 0.6295 - val_loss: 0.6496 - val_acc: 0.6202\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6407 - acc: 0.6287 - val_loss: 0.6498 - val_acc: 0.6195\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.6301 - val_loss: 0.6498 - val_acc: 0.6206\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6397 - acc: 0.6300 - val_loss: 0.6495 - val_acc: 0.6198\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6388 - acc: 0.6316 - val_loss: 0.6496 - val_acc: 0.6214\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe60124a90>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 4-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6946 - acc: 0.5337 - val_loss: 0.6830 - val_acc: 0.5570\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6852 - acc: 0.5517 - val_loss: 0.6827 - val_acc: 0.5582\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6833 - acc: 0.5563 - val_loss: 0.6819 - val_acc: 0.5598\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6824 - acc: 0.5577 - val_loss: 0.6817 - val_acc: 0.5594\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6814 - acc: 0.5596 - val_loss: 0.6803 - val_acc: 0.5627\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6800 - acc: 0.5643 - val_loss: 0.6784 - val_acc: 0.5709\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6782 - acc: 0.5697 - val_loss: 0.6757 - val_acc: 0.5780\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6762 - acc: 0.5753 - val_loss: 0.6739 - val_acc: 0.5819\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6748 - acc: 0.5783 - val_loss: 0.6730 - val_acc: 0.5829\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6738 - acc: 0.5811 - val_loss: 0.6718 - val_acc: 0.5859\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6720 - acc: 0.5834 - val_loss: 0.6710 - val_acc: 0.5870\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6714 - acc: 0.5848 - val_loss: 0.6709 - val_acc: 0.5877\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6706 - acc: 0.5868 - val_loss: 0.6698 - val_acc: 0.5895\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6692 - acc: 0.5899 - val_loss: 0.6686 - val_acc: 0.5903\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6684 - acc: 0.5909 - val_loss: 0.6683 - val_acc: 0.5916\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6678 - acc: 0.5907 - val_loss: 0.6673 - val_acc: 0.5932\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6664 - acc: 0.5939 - val_loss: 0.6672 - val_acc: 0.5933\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6658 - acc: 0.5945 - val_loss: 0.6669 - val_acc: 0.5940\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6655 - acc: 0.5951 - val_loss: 0.6664 - val_acc: 0.5949\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6646 - acc: 0.5957 - val_loss: 0.6658 - val_acc: 0.5971\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6636 - acc: 0.5969 - val_loss: 0.6662 - val_acc: 0.5962\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6629 - acc: 0.5995 - val_loss: 0.6647 - val_acc: 0.5977\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6622 - acc: 0.5996 - val_loss: 0.6648 - val_acc: 0.5967\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6618 - acc: 0.6007 - val_loss: 0.6647 - val_acc: 0.5990\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6615 - acc: 0.6024 - val_loss: 0.6640 - val_acc: 0.5993\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6607 - acc: 0.6026 - val_loss: 0.6633 - val_acc: 0.6001\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6602 - acc: 0.6036 - val_loss: 0.6641 - val_acc: 0.6007\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6594 - acc: 0.6042 - val_loss: 0.6630 - val_acc: 0.6016\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6586 - acc: 0.6047 - val_loss: 0.6630 - val_acc: 0.6001\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6578 - acc: 0.6066 - val_loss: 0.6625 - val_acc: 0.6024\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6573 - acc: 0.6061 - val_loss: 0.6637 - val_acc: 0.5987\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6569 - acc: 0.6079 - val_loss: 0.6629 - val_acc: 0.6003\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6557 - acc: 0.6093 - val_loss: 0.6622 - val_acc: 0.6028\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6556 - acc: 0.6089 - val_loss: 0.6622 - val_acc: 0.6028\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6548 - acc: 0.6095 - val_loss: 0.6622 - val_acc: 0.6023\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6543 - acc: 0.6098 - val_loss: 0.6618 - val_acc: 0.6040\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6538 - acc: 0.6113 - val_loss: 0.6614 - val_acc: 0.6035\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6536 - acc: 0.6110 - val_loss: 0.6623 - val_acc: 0.6009\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6527 - acc: 0.6129 - val_loss: 0.6614 - val_acc: 0.6042\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6517 - acc: 0.6131 - val_loss: 0.6613 - val_acc: 0.6036\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6516 - acc: 0.6152 - val_loss: 0.6615 - val_acc: 0.6037\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6509 - acc: 0.6144 - val_loss: 0.6615 - val_acc: 0.6040\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6507 - acc: 0.6141 - val_loss: 0.6608 - val_acc: 0.6058\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6493 - acc: 0.6161 - val_loss: 0.6610 - val_acc: 0.6052\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6490 - acc: 0.6175 - val_loss: 0.6610 - val_acc: 0.6054\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6486 - acc: 0.6183 - val_loss: 0.6605 - val_acc: 0.6062\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6482 - acc: 0.6190 - val_loss: 0.6609 - val_acc: 0.6051\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6474 - acc: 0.6188 - val_loss: 0.6606 - val_acc: 0.6054\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6467 - acc: 0.6197 - val_loss: 0.6605 - val_acc: 0.6060\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6464 - acc: 0.6210 - val_loss: 0.6604 - val_acc: 0.6057\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6459 - acc: 0.6209 - val_loss: 0.6605 - val_acc: 0.6060\n",
            "Epoch 52/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6455 - acc: 0.6212 - val_loss: 0.6606 - val_acc: 0.6051\n",
            "Epoch 53/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6446 - acc: 0.6227 - val_loss: 0.6603 - val_acc: 0.6049\n",
            "Epoch 54/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6441 - acc: 0.6238 - val_loss: 0.6606 - val_acc: 0.6046\n",
            "Epoch 55/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6438 - acc: 0.6243 - val_loss: 0.6601 - val_acc: 0.6063\n",
            "Epoch 56/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6434 - acc: 0.6242 - val_loss: 0.6606 - val_acc: 0.6051\n",
            "Epoch 57/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6423 - acc: 0.6257 - val_loss: 0.6601 - val_acc: 0.6050\n",
            "Epoch 58/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6421 - acc: 0.6261 - val_loss: 0.6605 - val_acc: 0.6061\n",
            "Epoch 59/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6414 - acc: 0.6262 - val_loss: 0.6603 - val_acc: 0.6054\n",
            "Epoch 60/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6410 - acc: 0.6271 - val_loss: 0.6606 - val_acc: 0.6063\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe4caad080>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 5-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6946 - acc: 0.5359 - val_loss: 0.6821 - val_acc: 0.5606\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6847 - acc: 0.5523 - val_loss: 0.6817 - val_acc: 0.5618\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6834 - acc: 0.5555 - val_loss: 0.6811 - val_acc: 0.5610\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6824 - acc: 0.5578 - val_loss: 0.6804 - val_acc: 0.5620\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6818 - acc: 0.5591 - val_loss: 0.6799 - val_acc: 0.5621\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6813 - acc: 0.5600 - val_loss: 0.6796 - val_acc: 0.5632\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6809 - acc: 0.5607 - val_loss: 0.6795 - val_acc: 0.5634\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6805 - acc: 0.5612 - val_loss: 0.6793 - val_acc: 0.5643\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6799 - acc: 0.5632 - val_loss: 0.6789 - val_acc: 0.5646\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6796 - acc: 0.5647 - val_loss: 0.6780 - val_acc: 0.5683\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6783 - acc: 0.5661 - val_loss: 0.6765 - val_acc: 0.5722\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6774 - acc: 0.5684 - val_loss: 0.6754 - val_acc: 0.5755\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6759 - acc: 0.5741 - val_loss: 0.6748 - val_acc: 0.5804\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6751 - acc: 0.5755 - val_loss: 0.6743 - val_acc: 0.5798\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6747 - acc: 0.5768 - val_loss: 0.6729 - val_acc: 0.5826\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6737 - acc: 0.5799 - val_loss: 0.6726 - val_acc: 0.5830\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6730 - acc: 0.5809 - val_loss: 0.6721 - val_acc: 0.5841\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6725 - acc: 0.5815 - val_loss: 0.6715 - val_acc: 0.5848\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6720 - acc: 0.5826 - val_loss: 0.6715 - val_acc: 0.5855\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6711 - acc: 0.5842 - val_loss: 0.6704 - val_acc: 0.5870\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6705 - acc: 0.5850 - val_loss: 0.6709 - val_acc: 0.5850\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6699 - acc: 0.5855 - val_loss: 0.6702 - val_acc: 0.5883\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6692 - acc: 0.5878 - val_loss: 0.6697 - val_acc: 0.5874\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6689 - acc: 0.5875 - val_loss: 0.6692 - val_acc: 0.5899\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6682 - acc: 0.5887 - val_loss: 0.6687 - val_acc: 0.5899\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6678 - acc: 0.5894 - val_loss: 0.6687 - val_acc: 0.5915\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6666 - acc: 0.5911 - val_loss: 0.6685 - val_acc: 0.5917\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6661 - acc: 0.5915 - val_loss: 0.6681 - val_acc: 0.5913\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6657 - acc: 0.5924 - val_loss: 0.6676 - val_acc: 0.5926\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6651 - acc: 0.5945 - val_loss: 0.6678 - val_acc: 0.5937\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6643 - acc: 0.5945 - val_loss: 0.6672 - val_acc: 0.5946\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6639 - acc: 0.5953 - val_loss: 0.6673 - val_acc: 0.5929\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6633 - acc: 0.5957 - val_loss: 0.6662 - val_acc: 0.5950\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6623 - acc: 0.5980 - val_loss: 0.6664 - val_acc: 0.5950\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6621 - acc: 0.5992 - val_loss: 0.6670 - val_acc: 0.5932\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6609 - acc: 0.6014 - val_loss: 0.6655 - val_acc: 0.5959\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6606 - acc: 0.6001 - val_loss: 0.6652 - val_acc: 0.5958\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6597 - acc: 0.6011 - val_loss: 0.6650 - val_acc: 0.5968\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6592 - acc: 0.6019 - val_loss: 0.6648 - val_acc: 0.5970\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6587 - acc: 0.6042 - val_loss: 0.6650 - val_acc: 0.5968\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6582 - acc: 0.6043 - val_loss: 0.6649 - val_acc: 0.5969\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6572 - acc: 0.6054 - val_loss: 0.6645 - val_acc: 0.5986\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6573 - acc: 0.6057 - val_loss: 0.6646 - val_acc: 0.5964\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6562 - acc: 0.6067 - val_loss: 0.6640 - val_acc: 0.5988\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6560 - acc: 0.6070 - val_loss: 0.6642 - val_acc: 0.5984\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6549 - acc: 0.6075 - val_loss: 0.6638 - val_acc: 0.5987\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6538 - acc: 0.6102 - val_loss: 0.6639 - val_acc: 0.5987\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6541 - acc: 0.6088 - val_loss: 0.6639 - val_acc: 0.5987\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6534 - acc: 0.6095 - val_loss: 0.6639 - val_acc: 0.5987\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6528 - acc: 0.6110 - val_loss: 0.6643 - val_acc: 0.5984\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6517 - acc: 0.6135 - val_loss: 0.6634 - val_acc: 0.5996\n",
            "Epoch 52/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6515 - acc: 0.6133 - val_loss: 0.6637 - val_acc: 0.5996\n",
            "Epoch 53/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6509 - acc: 0.6131 - val_loss: 0.6635 - val_acc: 0.5985\n",
            "Epoch 54/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6503 - acc: 0.6136 - val_loss: 0.6639 - val_acc: 0.5991\n",
            "Epoch 55/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6497 - acc: 0.6147 - val_loss: 0.6634 - val_acc: 0.5996\n",
            "Epoch 56/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6491 - acc: 0.6153 - val_loss: 0.6636 - val_acc: 0.6004\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe49c2ab38>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 6-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.7056 - acc: 0.5247 - val_loss: 0.6852 - val_acc: 0.5493\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6893 - acc: 0.5392 - val_loss: 0.6842 - val_acc: 0.5543\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6862 - acc: 0.5454 - val_loss: 0.6840 - val_acc: 0.5570\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6839 - acc: 0.5505 - val_loss: 0.6810 - val_acc: 0.5598\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6820 - acc: 0.5562 - val_loss: 0.6799 - val_acc: 0.5640\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6810 - acc: 0.5584 - val_loss: 0.6783 - val_acc: 0.5676\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6793 - acc: 0.5637 - val_loss: 0.6769 - val_acc: 0.5750\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6778 - acc: 0.5678 - val_loss: 0.6756 - val_acc: 0.5795\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6766 - acc: 0.5710 - val_loss: 0.6744 - val_acc: 0.5810\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6755 - acc: 0.5736 - val_loss: 0.6746 - val_acc: 0.5785\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6743 - acc: 0.5771 - val_loss: 0.6721 - val_acc: 0.5840\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6734 - acc: 0.5800 - val_loss: 0.6723 - val_acc: 0.5838\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6725 - acc: 0.5813 - val_loss: 0.6704 - val_acc: 0.5874\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6715 - acc: 0.5839 - val_loss: 0.6695 - val_acc: 0.5884\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6707 - acc: 0.5836 - val_loss: 0.6694 - val_acc: 0.5889\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6698 - acc: 0.5855 - val_loss: 0.6686 - val_acc: 0.5909\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6689 - acc: 0.5884 - val_loss: 0.6675 - val_acc: 0.5925\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6681 - acc: 0.5894 - val_loss: 0.6677 - val_acc: 0.5937\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6676 - acc: 0.5899 - val_loss: 0.6667 - val_acc: 0.5946\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6665 - acc: 0.5913 - val_loss: 0.6666 - val_acc: 0.5948\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6659 - acc: 0.5933 - val_loss: 0.6655 - val_acc: 0.5951\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6648 - acc: 0.5942 - val_loss: 0.6646 - val_acc: 0.5969\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6642 - acc: 0.5956 - val_loss: 0.6646 - val_acc: 0.5980\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6634 - acc: 0.5980 - val_loss: 0.6642 - val_acc: 0.5984\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6626 - acc: 0.5977 - val_loss: 0.6638 - val_acc: 0.5981\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6619 - acc: 0.5995 - val_loss: 0.6633 - val_acc: 0.6001\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6614 - acc: 0.6000 - val_loss: 0.6634 - val_acc: 0.6002\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6609 - acc: 0.6005 - val_loss: 0.6635 - val_acc: 0.5999\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6603 - acc: 0.6012 - val_loss: 0.6626 - val_acc: 0.6019\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6596 - acc: 0.6027 - val_loss: 0.6627 - val_acc: 0.5997\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6586 - acc: 0.6040 - val_loss: 0.6622 - val_acc: 0.6020\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6584 - acc: 0.6053 - val_loss: 0.6619 - val_acc: 0.6026\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6580 - acc: 0.6055 - val_loss: 0.6623 - val_acc: 0.6026\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6572 - acc: 0.6068 - val_loss: 0.6611 - val_acc: 0.6026\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6566 - acc: 0.6058 - val_loss: 0.6609 - val_acc: 0.6045\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6555 - acc: 0.6088 - val_loss: 0.6615 - val_acc: 0.6040\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6553 - acc: 0.6089 - val_loss: 0.6612 - val_acc: 0.6038\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6551 - acc: 0.6093 - val_loss: 0.6603 - val_acc: 0.6050\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6540 - acc: 0.6098 - val_loss: 0.6600 - val_acc: 0.6053\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6534 - acc: 0.6106 - val_loss: 0.6609 - val_acc: 0.6046\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6531 - acc: 0.6110 - val_loss: 0.6597 - val_acc: 0.6060\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6526 - acc: 0.6127 - val_loss: 0.6598 - val_acc: 0.6063\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6518 - acc: 0.6130 - val_loss: 0.6597 - val_acc: 0.6053\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6514 - acc: 0.6143 - val_loss: 0.6605 - val_acc: 0.6042\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6510 - acc: 0.6140 - val_loss: 0.6595 - val_acc: 0.6064\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6500 - acc: 0.6159 - val_loss: 0.6594 - val_acc: 0.6057\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6493 - acc: 0.6174 - val_loss: 0.6589 - val_acc: 0.6073\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6492 - acc: 0.6167 - val_loss: 0.6591 - val_acc: 0.6072\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6488 - acc: 0.6175 - val_loss: 0.6589 - val_acc: 0.6064\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6477 - acc: 0.6185 - val_loss: 0.6588 - val_acc: 0.6067\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6473 - acc: 0.6189 - val_loss: 0.6593 - val_acc: 0.6072\n",
            "Epoch 52/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6471 - acc: 0.6197 - val_loss: 0.6585 - val_acc: 0.6071\n",
            "Epoch 53/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6461 - acc: 0.6214 - val_loss: 0.6587 - val_acc: 0.6081\n",
            "Epoch 54/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6453 - acc: 0.6218 - val_loss: 0.6585 - val_acc: 0.6075\n",
            "Epoch 55/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6450 - acc: 0.6220 - val_loss: 0.6585 - val_acc: 0.6078\n",
            "Epoch 56/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6439 - acc: 0.6235 - val_loss: 0.6584 - val_acc: 0.6080\n",
            "Epoch 57/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6437 - acc: 0.6234 - val_loss: 0.6583 - val_acc: 0.6088\n",
            "Epoch 58/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6432 - acc: 0.6244 - val_loss: 0.6583 - val_acc: 0.6089\n",
            "Epoch 59/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6429 - acc: 0.6246 - val_loss: 0.6587 - val_acc: 0.6087\n",
            "Epoch 60/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6418 - acc: 0.6265 - val_loss: 0.6585 - val_acc: 0.6087\n",
            "Epoch 61/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6415 - acc: 0.6259 - val_loss: 0.6590 - val_acc: 0.6067\n",
            "Epoch 62/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6411 - acc: 0.6267 - val_loss: 0.6584 - val_acc: 0.6088\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe46d47048>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 7-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6798 - acc: 0.5763 - val_loss: 0.6618 - val_acc: 0.6045\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6655 - acc: 0.5970 - val_loss: 0.6613 - val_acc: 0.6048\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6627 - acc: 0.6025 - val_loss: 0.6607 - val_acc: 0.6052\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6615 - acc: 0.6040 - val_loss: 0.6604 - val_acc: 0.6038\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6605 - acc: 0.6041 - val_loss: 0.6593 - val_acc: 0.6054\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6599 - acc: 0.6047 - val_loss: 0.6587 - val_acc: 0.6063\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6594 - acc: 0.6045 - val_loss: 0.6586 - val_acc: 0.6065\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6584 - acc: 0.6063 - val_loss: 0.6579 - val_acc: 0.6073\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6574 - acc: 0.6072 - val_loss: 0.6571 - val_acc: 0.6080\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6568 - acc: 0.6076 - val_loss: 0.6560 - val_acc: 0.6088\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6558 - acc: 0.6100 - val_loss: 0.6555 - val_acc: 0.6088\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6549 - acc: 0.6103 - val_loss: 0.6547 - val_acc: 0.6105\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6539 - acc: 0.6120 - val_loss: 0.6542 - val_acc: 0.6123\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6531 - acc: 0.6126 - val_loss: 0.6531 - val_acc: 0.6132\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6522 - acc: 0.6152 - val_loss: 0.6531 - val_acc: 0.6137\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6518 - acc: 0.6150 - val_loss: 0.6520 - val_acc: 0.6144\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6510 - acc: 0.6159 - val_loss: 0.6512 - val_acc: 0.6160\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6502 - acc: 0.6171 - val_loss: 0.6510 - val_acc: 0.6171\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6493 - acc: 0.6185 - val_loss: 0.6506 - val_acc: 0.6184\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6486 - acc: 0.6204 - val_loss: 0.6502 - val_acc: 0.6183\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6482 - acc: 0.6196 - val_loss: 0.6499 - val_acc: 0.6189\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6481 - acc: 0.6199 - val_loss: 0.6495 - val_acc: 0.6195\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6471 - acc: 0.6216 - val_loss: 0.6492 - val_acc: 0.6203\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6467 - acc: 0.6217 - val_loss: 0.6489 - val_acc: 0.6200\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6460 - acc: 0.6232 - val_loss: 0.6484 - val_acc: 0.6211\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6455 - acc: 0.6236 - val_loss: 0.6483 - val_acc: 0.6213\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6451 - acc: 0.6239 - val_loss: 0.6478 - val_acc: 0.6207\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6441 - acc: 0.6251 - val_loss: 0.6478 - val_acc: 0.6211\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6439 - acc: 0.6248 - val_loss: 0.6475 - val_acc: 0.6226\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6433 - acc: 0.6250 - val_loss: 0.6472 - val_acc: 0.6229\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6431 - acc: 0.6264 - val_loss: 0.6471 - val_acc: 0.6242\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6425 - acc: 0.6274 - val_loss: 0.6471 - val_acc: 0.6233\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6419 - acc: 0.6276 - val_loss: 0.6469 - val_acc: 0.6247\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6414 - acc: 0.6269 - val_loss: 0.6469 - val_acc: 0.6240\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6412 - acc: 0.6272 - val_loss: 0.6466 - val_acc: 0.6242\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6406 - acc: 0.6286 - val_loss: 0.6466 - val_acc: 0.6242\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6402 - acc: 0.6296 - val_loss: 0.6465 - val_acc: 0.6244\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6394 - acc: 0.6307 - val_loss: 0.6466 - val_acc: 0.6252\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6393 - acc: 0.6309 - val_loss: 0.6462 - val_acc: 0.6253\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6384 - acc: 0.6310 - val_loss: 0.6469 - val_acc: 0.6258\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6381 - acc: 0.6316 - val_loss: 0.6463 - val_acc: 0.6251\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6378 - acc: 0.6323 - val_loss: 0.6461 - val_acc: 0.6242\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6372 - acc: 0.6337 - val_loss: 0.6463 - val_acc: 0.6257\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6368 - acc: 0.6332 - val_loss: 0.6465 - val_acc: 0.6259\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6365 - acc: 0.6336 - val_loss: 0.6468 - val_acc: 0.6258\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6357 - acc: 0.6344 - val_loss: 0.6462 - val_acc: 0.6257\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6355 - acc: 0.6341 - val_loss: 0.6462 - val_acc: 0.6250\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe43ee9128>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 8-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 12us/step - loss: 0.6877 - acc: 0.5569 - val_loss: 0.6709 - val_acc: 0.5840\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6729 - acc: 0.5806 - val_loss: 0.6681 - val_acc: 0.5886\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6677 - acc: 0.5885 - val_loss: 0.6648 - val_acc: 0.5929\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6637 - acc: 0.5948 - val_loss: 0.6614 - val_acc: 0.5972\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6612 - acc: 0.5991 - val_loss: 0.6598 - val_acc: 0.6022\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6597 - acc: 0.6010 - val_loss: 0.6585 - val_acc: 0.6053\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6581 - acc: 0.6040 - val_loss: 0.6577 - val_acc: 0.6070\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6574 - acc: 0.6060 - val_loss: 0.6568 - val_acc: 0.6088\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6559 - acc: 0.6079 - val_loss: 0.6563 - val_acc: 0.6094\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6549 - acc: 0.6096 - val_loss: 0.6556 - val_acc: 0.6098\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6542 - acc: 0.6104 - val_loss: 0.6545 - val_acc: 0.6092\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6532 - acc: 0.6120 - val_loss: 0.6537 - val_acc: 0.6113\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6525 - acc: 0.6130 - val_loss: 0.6536 - val_acc: 0.6111\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6519 - acc: 0.6135 - val_loss: 0.6532 - val_acc: 0.6112\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6513 - acc: 0.6143 - val_loss: 0.6530 - val_acc: 0.6132\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6505 - acc: 0.6156 - val_loss: 0.6523 - val_acc: 0.6133\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6504 - acc: 0.6162 - val_loss: 0.6518 - val_acc: 0.6137\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6493 - acc: 0.6168 - val_loss: 0.6519 - val_acc: 0.6142\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6490 - acc: 0.6175 - val_loss: 0.6514 - val_acc: 0.6143\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6482 - acc: 0.6183 - val_loss: 0.6510 - val_acc: 0.6153\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6480 - acc: 0.6188 - val_loss: 0.6511 - val_acc: 0.6161\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6475 - acc: 0.6202 - val_loss: 0.6505 - val_acc: 0.6165\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6471 - acc: 0.6200 - val_loss: 0.6504 - val_acc: 0.6162\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6466 - acc: 0.6202 - val_loss: 0.6503 - val_acc: 0.6162\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6462 - acc: 0.6213 - val_loss: 0.6497 - val_acc: 0.6183\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6452 - acc: 0.6221 - val_loss: 0.6495 - val_acc: 0.6176\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6454 - acc: 0.6224 - val_loss: 0.6493 - val_acc: 0.6175\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6446 - acc: 0.6224 - val_loss: 0.6492 - val_acc: 0.6180\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6441 - acc: 0.6233 - val_loss: 0.6489 - val_acc: 0.6176\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6436 - acc: 0.6238 - val_loss: 0.6488 - val_acc: 0.6181\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6430 - acc: 0.6243 - val_loss: 0.6489 - val_acc: 0.6183\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6427 - acc: 0.6252 - val_loss: 0.6493 - val_acc: 0.6180\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6419 - acc: 0.6261 - val_loss: 0.6483 - val_acc: 0.6195\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6418 - acc: 0.6266 - val_loss: 0.6484 - val_acc: 0.6201\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6412 - acc: 0.6276 - val_loss: 0.6483 - val_acc: 0.6208\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6408 - acc: 0.6266 - val_loss: 0.6483 - val_acc: 0.6198\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6405 - acc: 0.6277 - val_loss: 0.6489 - val_acc: 0.6193\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.6275 - val_loss: 0.6482 - val_acc: 0.6202\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6395 - acc: 0.6297 - val_loss: 0.6482 - val_acc: 0.6192\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6385 - acc: 0.6296 - val_loss: 0.6478 - val_acc: 0.6213\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6384 - acc: 0.6299 - val_loss: 0.6487 - val_acc: 0.6199\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6381 - acc: 0.6293 - val_loss: 0.6478 - val_acc: 0.6205\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6377 - acc: 0.6307 - val_loss: 0.6479 - val_acc: 0.6204\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6370 - acc: 0.6308 - val_loss: 0.6476 - val_acc: 0.6212\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6367 - acc: 0.6311 - val_loss: 0.6478 - val_acc: 0.6214\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6363 - acc: 0.6327 - val_loss: 0.6473 - val_acc: 0.6223\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6361 - acc: 0.6326 - val_loss: 0.6478 - val_acc: 0.6221\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6351 - acc: 0.6330 - val_loss: 0.6477 - val_acc: 0.6212\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6342 - acc: 0.6336 - val_loss: 0.6475 - val_acc: 0.6219\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6342 - acc: 0.6327 - val_loss: 0.6476 - val_acc: 0.6212\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6337 - acc: 0.6333 - val_loss: 0.6479 - val_acc: 0.6213\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe41063080>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 9-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 12us/step - loss: 0.6840 - acc: 0.5668 - val_loss: 0.6683 - val_acc: 0.5955\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6709 - acc: 0.5883 - val_loss: 0.6672 - val_acc: 0.5970\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6682 - acc: 0.5923 - val_loss: 0.6658 - val_acc: 0.5978\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6670 - acc: 0.5936 - val_loss: 0.6653 - val_acc: 0.5973\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6653 - acc: 0.5967 - val_loss: 0.6630 - val_acc: 0.6022\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6631 - acc: 0.6016 - val_loss: 0.6616 - val_acc: 0.6041\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6624 - acc: 0.6031 - val_loss: 0.6610 - val_acc: 0.6048\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6614 - acc: 0.6045 - val_loss: 0.6605 - val_acc: 0.6060\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6609 - acc: 0.6055 - val_loss: 0.6603 - val_acc: 0.6055\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6603 - acc: 0.6061 - val_loss: 0.6596 - val_acc: 0.6074\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6597 - acc: 0.6063 - val_loss: 0.6591 - val_acc: 0.6082\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6593 - acc: 0.6069 - val_loss: 0.6592 - val_acc: 0.6087\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6584 - acc: 0.6071 - val_loss: 0.6583 - val_acc: 0.6101\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6579 - acc: 0.6086 - val_loss: 0.6579 - val_acc: 0.6104\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6572 - acc: 0.6086 - val_loss: 0.6578 - val_acc: 0.6105\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6569 - acc: 0.6098 - val_loss: 0.6572 - val_acc: 0.6106\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6561 - acc: 0.6110 - val_loss: 0.6572 - val_acc: 0.6113\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6554 - acc: 0.6112 - val_loss: 0.6570 - val_acc: 0.6117\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6550 - acc: 0.6127 - val_loss: 0.6567 - val_acc: 0.6118\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6546 - acc: 0.6129 - val_loss: 0.6564 - val_acc: 0.6127\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6540 - acc: 0.6134 - val_loss: 0.6563 - val_acc: 0.6132\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6535 - acc: 0.6145 - val_loss: 0.6559 - val_acc: 0.6131\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6530 - acc: 0.6149 - val_loss: 0.6556 - val_acc: 0.6134\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6523 - acc: 0.6151 - val_loss: 0.6552 - val_acc: 0.6145\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6517 - acc: 0.6165 - val_loss: 0.6557 - val_acc: 0.6132\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6514 - acc: 0.6165 - val_loss: 0.6550 - val_acc: 0.6147\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6509 - acc: 0.6173 - val_loss: 0.6550 - val_acc: 0.6135\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6504 - acc: 0.6177 - val_loss: 0.6551 - val_acc: 0.6147\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6498 - acc: 0.6185 - val_loss: 0.6555 - val_acc: 0.6146\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6495 - acc: 0.6189 - val_loss: 0.6545 - val_acc: 0.6149\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6490 - acc: 0.6189 - val_loss: 0.6543 - val_acc: 0.6159\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6482 - acc: 0.6192 - val_loss: 0.6550 - val_acc: 0.6158\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6478 - acc: 0.6208 - val_loss: 0.6543 - val_acc: 0.6162\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6480 - acc: 0.6212 - val_loss: 0.6559 - val_acc: 0.6149\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6474 - acc: 0.6207 - val_loss: 0.6538 - val_acc: 0.6169\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6465 - acc: 0.6224 - val_loss: 0.6543 - val_acc: 0.6156\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6460 - acc: 0.6227 - val_loss: 0.6540 - val_acc: 0.6164\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6457 - acc: 0.6220 - val_loss: 0.6539 - val_acc: 0.6167\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6449 - acc: 0.6236 - val_loss: 0.6538 - val_acc: 0.6167\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6445 - acc: 0.6238 - val_loss: 0.6543 - val_acc: 0.6162\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe3e17e198>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 10-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 12us/step - loss: 0.6995 - acc: 0.5214 - val_loss: 0.6880 - val_acc: 0.5410\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6897 - acc: 0.5357 - val_loss: 0.6870 - val_acc: 0.5451\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6873 - acc: 0.5427 - val_loss: 0.6860 - val_acc: 0.5476\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6860 - acc: 0.5468 - val_loss: 0.6844 - val_acc: 0.5506\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6834 - acc: 0.5548 - val_loss: 0.6810 - val_acc: 0.5616\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6807 - acc: 0.5610 - val_loss: 0.6788 - val_acc: 0.5656\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6785 - acc: 0.5669 - val_loss: 0.6767 - val_acc: 0.5699\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6766 - acc: 0.5710 - val_loss: 0.6747 - val_acc: 0.5744\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6751 - acc: 0.5753 - val_loss: 0.6728 - val_acc: 0.5774\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6731 - acc: 0.5784 - val_loss: 0.6707 - val_acc: 0.5820\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6713 - acc: 0.5821 - val_loss: 0.6688 - val_acc: 0.5865\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6700 - acc: 0.5848 - val_loss: 0.6687 - val_acc: 0.5877\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6684 - acc: 0.5886 - val_loss: 0.6662 - val_acc: 0.5934\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6672 - acc: 0.5903 - val_loss: 0.6647 - val_acc: 0.5951\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6659 - acc: 0.5932 - val_loss: 0.6646 - val_acc: 0.5980\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6648 - acc: 0.5961 - val_loss: 0.6630 - val_acc: 0.6009\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6637 - acc: 0.5974 - val_loss: 0.6630 - val_acc: 0.6006\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6631 - acc: 0.5989 - val_loss: 0.6626 - val_acc: 0.5997\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6623 - acc: 0.6004 - val_loss: 0.6618 - val_acc: 0.6033\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6610 - acc: 0.6018 - val_loss: 0.6610 - val_acc: 0.6043\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6608 - acc: 0.6022 - val_loss: 0.6612 - val_acc: 0.6023\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6598 - acc: 0.6046 - val_loss: 0.6601 - val_acc: 0.6054\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6593 - acc: 0.6044 - val_loss: 0.6599 - val_acc: 0.6051\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6583 - acc: 0.6062 - val_loss: 0.6594 - val_acc: 0.6065\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6580 - acc: 0.6066 - val_loss: 0.6592 - val_acc: 0.6061\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6570 - acc: 0.6087 - val_loss: 0.6586 - val_acc: 0.6079\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6567 - acc: 0.6090 - val_loss: 0.6587 - val_acc: 0.6074\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6563 - acc: 0.6086 - val_loss: 0.6588 - val_acc: 0.6078\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6554 - acc: 0.6103 - val_loss: 0.6582 - val_acc: 0.6068\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6552 - acc: 0.6112 - val_loss: 0.6581 - val_acc: 0.6089\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6547 - acc: 0.6109 - val_loss: 0.6583 - val_acc: 0.6083\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6540 - acc: 0.6129 - val_loss: 0.6573 - val_acc: 0.6084\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6531 - acc: 0.6136 - val_loss: 0.6573 - val_acc: 0.6094\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6527 - acc: 0.6151 - val_loss: 0.6576 - val_acc: 0.6088\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6527 - acc: 0.6143 - val_loss: 0.6576 - val_acc: 0.6096\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6513 - acc: 0.6171 - val_loss: 0.6574 - val_acc: 0.6097\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6512 - acc: 0.6158 - val_loss: 0.6568 - val_acc: 0.6092\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6507 - acc: 0.6166 - val_loss: 0.6567 - val_acc: 0.6107\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6504 - acc: 0.6170 - val_loss: 0.6565 - val_acc: 0.6107\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6493 - acc: 0.6178 - val_loss: 0.6570 - val_acc: 0.6108\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6493 - acc: 0.6188 - val_loss: 0.6569 - val_acc: 0.6100\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6493 - acc: 0.6189 - val_loss: 0.6562 - val_acc: 0.6119\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6484 - acc: 0.6201 - val_loss: 0.6563 - val_acc: 0.6098\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6476 - acc: 0.6206 - val_loss: 0.6560 - val_acc: 0.6107\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6476 - acc: 0.6210 - val_loss: 0.6563 - val_acc: 0.6115\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6466 - acc: 0.6228 - val_loss: 0.6564 - val_acc: 0.6105\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6463 - acc: 0.6222 - val_loss: 0.6559 - val_acc: 0.6127\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6451 - acc: 0.6240 - val_loss: 0.6561 - val_acc: 0.6139\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6451 - acc: 0.6237 - val_loss: 0.6561 - val_acc: 0.6127\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6442 - acc: 0.6236 - val_loss: 0.6559 - val_acc: 0.6124\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6443 - acc: 0.6244 - val_loss: 0.6562 - val_acc: 0.6128\n",
            "Epoch 52/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6436 - acc: 0.6257 - val_loss: 0.6558 - val_acc: 0.6132\n",
            "Epoch 53/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6425 - acc: 0.6264 - val_loss: 0.6559 - val_acc: 0.6120\n",
            "Epoch 54/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6426 - acc: 0.6260 - val_loss: 0.6556 - val_acc: 0.6150\n",
            "Epoch 55/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6419 - acc: 0.6265 - val_loss: 0.6556 - val_acc: 0.6137\n",
            "Epoch 56/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6420 - acc: 0.6267 - val_loss: 0.6555 - val_acc: 0.6150\n",
            "Epoch 57/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6412 - acc: 0.6280 - val_loss: 0.6555 - val_acc: 0.6129\n",
            "Epoch 58/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6403 - acc: 0.6290 - val_loss: 0.6557 - val_acc: 0.6147\n",
            "Epoch 59/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6399 - acc: 0.6289 - val_loss: 0.6559 - val_acc: 0.6150\n",
            "Epoch 60/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6394 - acc: 0.6297 - val_loss: 0.6555 - val_acc: 0.6140\n",
            "Epoch 61/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6394 - acc: 0.6291 - val_loss: 0.6558 - val_acc: 0.6132\n",
            "Epoch 62/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6388 - acc: 0.6303 - val_loss: 0.6556 - val_acc: 0.6136\n",
            "Epoch 63/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6376 - acc: 0.6325 - val_loss: 0.6556 - val_acc: 0.6147\n",
            "Epoch 64/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6379 - acc: 0.6316 - val_loss: 0.6557 - val_acc: 0.6134\n",
            "Epoch 65/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6363 - acc: 0.6335 - val_loss: 0.6554 - val_acc: 0.6138\n",
            "Epoch 66/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6364 - acc: 0.6334 - val_loss: 0.6556 - val_acc: 0.6149\n",
            "Epoch 67/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6358 - acc: 0.6331 - val_loss: 0.6556 - val_acc: 0.6138\n",
            "Epoch 68/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6353 - acc: 0.6348 - val_loss: 0.6563 - val_acc: 0.6128\n",
            "Epoch 69/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6347 - acc: 0.6347 - val_loss: 0.6557 - val_acc: 0.6141\n",
            "Epoch 70/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6348 - acc: 0.6352 - val_loss: 0.6558 - val_acc: 0.6132\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe3b2f3208>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 11-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 12us/step - loss: 0.6890 - acc: 0.5565 - val_loss: 0.6726 - val_acc: 0.5845\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6749 - acc: 0.5797 - val_loss: 0.6719 - val_acc: 0.5868\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6722 - acc: 0.5843 - val_loss: 0.6711 - val_acc: 0.5853\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6711 - acc: 0.5862 - val_loss: 0.6697 - val_acc: 0.5876\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6697 - acc: 0.5884 - val_loss: 0.6697 - val_acc: 0.5896\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6686 - acc: 0.5903 - val_loss: 0.6678 - val_acc: 0.5925\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6673 - acc: 0.5940 - val_loss: 0.6671 - val_acc: 0.5927\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6658 - acc: 0.5966 - val_loss: 0.6666 - val_acc: 0.5955\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6644 - acc: 0.5989 - val_loss: 0.6633 - val_acc: 0.6023\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6623 - acc: 0.6040 - val_loss: 0.6617 - val_acc: 0.6034\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6609 - acc: 0.6040 - val_loss: 0.6608 - val_acc: 0.6042\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6602 - acc: 0.6055 - val_loss: 0.6599 - val_acc: 0.6066\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6593 - acc: 0.6068 - val_loss: 0.6592 - val_acc: 0.6058\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6587 - acc: 0.6087 - val_loss: 0.6587 - val_acc: 0.6087\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6578 - acc: 0.6090 - val_loss: 0.6583 - val_acc: 0.6094\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6571 - acc: 0.6107 - val_loss: 0.6573 - val_acc: 0.6106\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6563 - acc: 0.6117 - val_loss: 0.6565 - val_acc: 0.6114\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6557 - acc: 0.6117 - val_loss: 0.6562 - val_acc: 0.6126\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6555 - acc: 0.6133 - val_loss: 0.6561 - val_acc: 0.6121\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6546 - acc: 0.6123 - val_loss: 0.6559 - val_acc: 0.6130\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6540 - acc: 0.6142 - val_loss: 0.6552 - val_acc: 0.6143\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6534 - acc: 0.6155 - val_loss: 0.6546 - val_acc: 0.6147\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6530 - acc: 0.6154 - val_loss: 0.6543 - val_acc: 0.6150\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6524 - acc: 0.6159 - val_loss: 0.6539 - val_acc: 0.6157\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6521 - acc: 0.6163 - val_loss: 0.6532 - val_acc: 0.6172\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6514 - acc: 0.6174 - val_loss: 0.6530 - val_acc: 0.6172\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6505 - acc: 0.6188 - val_loss: 0.6529 - val_acc: 0.6177\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6504 - acc: 0.6189 - val_loss: 0.6529 - val_acc: 0.6175\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6497 - acc: 0.6199 - val_loss: 0.6523 - val_acc: 0.6168\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6494 - acc: 0.6202 - val_loss: 0.6532 - val_acc: 0.6151\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6489 - acc: 0.6211 - val_loss: 0.6533 - val_acc: 0.6157\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6487 - acc: 0.6209 - val_loss: 0.6522 - val_acc: 0.6181\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6480 - acc: 0.6219 - val_loss: 0.6518 - val_acc: 0.6181\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6478 - acc: 0.6225 - val_loss: 0.6516 - val_acc: 0.6185\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6470 - acc: 0.6222 - val_loss: 0.6513 - val_acc: 0.6181\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6469 - acc: 0.6227 - val_loss: 0.6516 - val_acc: 0.6172\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6459 - acc: 0.6239 - val_loss: 0.6509 - val_acc: 0.6190\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6459 - acc: 0.6237 - val_loss: 0.6512 - val_acc: 0.6177\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6453 - acc: 0.6248 - val_loss: 0.6511 - val_acc: 0.6190\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6444 - acc: 0.6264 - val_loss: 0.6514 - val_acc: 0.6178\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6441 - acc: 0.6260 - val_loss: 0.6506 - val_acc: 0.6194\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6437 - acc: 0.6259 - val_loss: 0.6510 - val_acc: 0.6192\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6429 - acc: 0.6277 - val_loss: 0.6506 - val_acc: 0.6196\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6425 - acc: 0.6274 - val_loss: 0.6505 - val_acc: 0.6192\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6420 - acc: 0.6281 - val_loss: 0.6506 - val_acc: 0.6192\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6420 - acc: 0.6288 - val_loss: 0.6504 - val_acc: 0.6191\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6412 - acc: 0.6291 - val_loss: 0.6504 - val_acc: 0.6192\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6407 - acc: 0.6302 - val_loss: 0.6503 - val_acc: 0.6199\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6403 - acc: 0.6301 - val_loss: 0.6503 - val_acc: 0.6203\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6398 - acc: 0.6305 - val_loss: 0.6501 - val_acc: 0.6202\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6394 - acc: 0.6310 - val_loss: 0.6502 - val_acc: 0.6204\n",
            "Epoch 52/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6389 - acc: 0.6314 - val_loss: 0.6502 - val_acc: 0.6202\n",
            "Epoch 53/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6381 - acc: 0.6319 - val_loss: 0.6500 - val_acc: 0.6207\n",
            "Epoch 54/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6377 - acc: 0.6333 - val_loss: 0.6502 - val_acc: 0.6204\n",
            "Epoch 55/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6375 - acc: 0.6337 - val_loss: 0.6500 - val_acc: 0.6209\n",
            "Epoch 56/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6374 - acc: 0.6328 - val_loss: 0.6502 - val_acc: 0.6204\n",
            "Epoch 57/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6363 - acc: 0.6341 - val_loss: 0.6506 - val_acc: 0.6203\n",
            "Epoch 58/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6362 - acc: 0.6343 - val_loss: 0.6499 - val_acc: 0.6202\n",
            "Epoch 59/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6352 - acc: 0.6361 - val_loss: 0.6500 - val_acc: 0.6202\n",
            "Epoch 60/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6350 - acc: 0.6358 - val_loss: 0.6501 - val_acc: 0.6198\n",
            "Epoch 61/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6341 - acc: 0.6368 - val_loss: 0.6500 - val_acc: 0.6200\n",
            "Epoch 62/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6338 - acc: 0.6359 - val_loss: 0.6499 - val_acc: 0.6209\n",
            "Epoch 63/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6331 - acc: 0.6383 - val_loss: 0.6504 - val_acc: 0.6204\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe384942b0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 12-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6907 - acc: 0.5475 - val_loss: 0.6751 - val_acc: 0.5760\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6779 - acc: 0.5712 - val_loss: 0.6749 - val_acc: 0.5748\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6754 - acc: 0.5757 - val_loss: 0.6729 - val_acc: 0.5815\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6740 - acc: 0.5786 - val_loss: 0.6724 - val_acc: 0.5831\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6733 - acc: 0.5801 - val_loss: 0.6719 - val_acc: 0.5840\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6728 - acc: 0.5812 - val_loss: 0.6712 - val_acc: 0.5866\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6719 - acc: 0.5841 - val_loss: 0.6701 - val_acc: 0.5886\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6707 - acc: 0.5868 - val_loss: 0.6693 - val_acc: 0.5896\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6699 - acc: 0.5883 - val_loss: 0.6683 - val_acc: 0.5913\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6689 - acc: 0.5906 - val_loss: 0.6678 - val_acc: 0.5920\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6684 - acc: 0.5914 - val_loss: 0.6674 - val_acc: 0.5927\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6678 - acc: 0.5923 - val_loss: 0.6666 - val_acc: 0.5947\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6672 - acc: 0.5928 - val_loss: 0.6662 - val_acc: 0.5950\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6665 - acc: 0.5950 - val_loss: 0.6657 - val_acc: 0.5963\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6659 - acc: 0.5959 - val_loss: 0.6655 - val_acc: 0.5975\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6654 - acc: 0.5967 - val_loss: 0.6648 - val_acc: 0.5977\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6647 - acc: 0.5979 - val_loss: 0.6643 - val_acc: 0.5982\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6640 - acc: 0.5993 - val_loss: 0.6637 - val_acc: 0.5993\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6633 - acc: 0.5994 - val_loss: 0.6632 - val_acc: 0.6004\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6624 - acc: 0.6012 - val_loss: 0.6628 - val_acc: 0.6013\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6615 - acc: 0.6022 - val_loss: 0.6623 - val_acc: 0.6014\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6611 - acc: 0.6032 - val_loss: 0.6616 - val_acc: 0.6022\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6603 - acc: 0.6035 - val_loss: 0.6612 - val_acc: 0.6036\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6597 - acc: 0.6051 - val_loss: 0.6607 - val_acc: 0.6041\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6589 - acc: 0.6062 - val_loss: 0.6602 - val_acc: 0.6052\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6581 - acc: 0.6071 - val_loss: 0.6602 - val_acc: 0.6045\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6576 - acc: 0.6075 - val_loss: 0.6591 - val_acc: 0.6065\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6569 - acc: 0.6090 - val_loss: 0.6590 - val_acc: 0.6072\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6560 - acc: 0.6086 - val_loss: 0.6593 - val_acc: 0.6064\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6562 - acc: 0.6106 - val_loss: 0.6587 - val_acc: 0.6072\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6551 - acc: 0.6118 - val_loss: 0.6582 - val_acc: 0.6088\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6548 - acc: 0.6115 - val_loss: 0.6588 - val_acc: 0.6073\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6542 - acc: 0.6122 - val_loss: 0.6582 - val_acc: 0.6083\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6532 - acc: 0.6135 - val_loss: 0.6575 - val_acc: 0.6089\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6529 - acc: 0.6140 - val_loss: 0.6574 - val_acc: 0.6099\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6518 - acc: 0.6153 - val_loss: 0.6572 - val_acc: 0.6091\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6513 - acc: 0.6158 - val_loss: 0.6569 - val_acc: 0.6103\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6508 - acc: 0.6171 - val_loss: 0.6569 - val_acc: 0.6096\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6503 - acc: 0.6182 - val_loss: 0.6567 - val_acc: 0.6106\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6497 - acc: 0.6192 - val_loss: 0.6571 - val_acc: 0.6096\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6488 - acc: 0.6195 - val_loss: 0.6565 - val_acc: 0.6115\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6486 - acc: 0.6194 - val_loss: 0.6560 - val_acc: 0.6116\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6480 - acc: 0.6208 - val_loss: 0.6562 - val_acc: 0.6114\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6473 - acc: 0.6207 - val_loss: 0.6562 - val_acc: 0.6105\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6470 - acc: 0.6213 - val_loss: 0.6560 - val_acc: 0.6121\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6464 - acc: 0.6209 - val_loss: 0.6554 - val_acc: 0.6118\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6457 - acc: 0.6231 - val_loss: 0.6556 - val_acc: 0.6118\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6454 - acc: 0.6216 - val_loss: 0.6556 - val_acc: 0.6114\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6447 - acc: 0.6235 - val_loss: 0.6560 - val_acc: 0.6114\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6443 - acc: 0.6248 - val_loss: 0.6555 - val_acc: 0.6124\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6430 - acc: 0.6261 - val_loss: 0.6555 - val_acc: 0.6125\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe3560c320>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 13-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6808 - acc: 0.5728 - val_loss: 0.6634 - val_acc: 0.5988\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6644 - acc: 0.5962 - val_loss: 0.6587 - val_acc: 0.6059\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6590 - acc: 0.6032 - val_loss: 0.6561 - val_acc: 0.6107\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6552 - acc: 0.6077 - val_loss: 0.6533 - val_acc: 0.6134\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6532 - acc: 0.6097 - val_loss: 0.6521 - val_acc: 0.6143\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6517 - acc: 0.6130 - val_loss: 0.6504 - val_acc: 0.6177\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6504 - acc: 0.6144 - val_loss: 0.6502 - val_acc: 0.6164\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6493 - acc: 0.6162 - val_loss: 0.6491 - val_acc: 0.6206\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6482 - acc: 0.6180 - val_loss: 0.6484 - val_acc: 0.6203\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6473 - acc: 0.6183 - val_loss: 0.6475 - val_acc: 0.6219\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6465 - acc: 0.6200 - val_loss: 0.6471 - val_acc: 0.6232\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6460 - acc: 0.6214 - val_loss: 0.6473 - val_acc: 0.6232\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6451 - acc: 0.6223 - val_loss: 0.6464 - val_acc: 0.6238\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6450 - acc: 0.6227 - val_loss: 0.6462 - val_acc: 0.6253\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6443 - acc: 0.6245 - val_loss: 0.6457 - val_acc: 0.6258\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6436 - acc: 0.6249 - val_loss: 0.6458 - val_acc: 0.6256\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6431 - acc: 0.6250 - val_loss: 0.6450 - val_acc: 0.6266\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6426 - acc: 0.6256 - val_loss: 0.6451 - val_acc: 0.6262\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6421 - acc: 0.6266 - val_loss: 0.6447 - val_acc: 0.6270\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6419 - acc: 0.6267 - val_loss: 0.6444 - val_acc: 0.6281\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6412 - acc: 0.6277 - val_loss: 0.6442 - val_acc: 0.6277\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6407 - acc: 0.6291 - val_loss: 0.6446 - val_acc: 0.6273\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6401 - acc: 0.6285 - val_loss: 0.6437 - val_acc: 0.6289\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6397 - acc: 0.6291 - val_loss: 0.6436 - val_acc: 0.6289\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6395 - acc: 0.6295 - val_loss: 0.6444 - val_acc: 0.6286\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6389 - acc: 0.6303 - val_loss: 0.6435 - val_acc: 0.6289\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6386 - acc: 0.6298 - val_loss: 0.6431 - val_acc: 0.6302\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6384 - acc: 0.6322 - val_loss: 0.6431 - val_acc: 0.6302\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6375 - acc: 0.6312 - val_loss: 0.6426 - val_acc: 0.6309\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6370 - acc: 0.6331 - val_loss: 0.6427 - val_acc: 0.6311\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6365 - acc: 0.6335 - val_loss: 0.6429 - val_acc: 0.6302\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6358 - acc: 0.6336 - val_loss: 0.6422 - val_acc: 0.6306\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6358 - acc: 0.6329 - val_loss: 0.6426 - val_acc: 0.6303\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6355 - acc: 0.6348 - val_loss: 0.6426 - val_acc: 0.6308\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6347 - acc: 0.6345 - val_loss: 0.6420 - val_acc: 0.6315\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6342 - acc: 0.6352 - val_loss: 0.6422 - val_acc: 0.6313\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6336 - acc: 0.6353 - val_loss: 0.6429 - val_acc: 0.6312\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6336 - acc: 0.6355 - val_loss: 0.6423 - val_acc: 0.6306\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6330 - acc: 0.6357 - val_loss: 0.6419 - val_acc: 0.6320\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6325 - acc: 0.6368 - val_loss: 0.6418 - val_acc: 0.6322\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6324 - acc: 0.6362 - val_loss: 0.6420 - val_acc: 0.6316\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6315 - acc: 0.6382 - val_loss: 0.6419 - val_acc: 0.6314\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6314 - acc: 0.6366 - val_loss: 0.6423 - val_acc: 0.6317\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6310 - acc: 0.6376 - val_loss: 0.6420 - val_acc: 0.6318\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6306 - acc: 0.6381 - val_loss: 0.6422 - val_acc: 0.6321\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe3272f390>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 14-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 12us/step - loss: 0.6866 - acc: 0.5589 - val_loss: 0.6710 - val_acc: 0.5874\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6736 - acc: 0.5816 - val_loss: 0.6713 - val_acc: 0.5847\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6711 - acc: 0.5854 - val_loss: 0.6690 - val_acc: 0.5904\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6698 - acc: 0.5882 - val_loss: 0.6679 - val_acc: 0.5915\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6681 - acc: 0.5901 - val_loss: 0.6672 - val_acc: 0.5934\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6668 - acc: 0.5930 - val_loss: 0.6651 - val_acc: 0.5961\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6655 - acc: 0.5950 - val_loss: 0.6641 - val_acc: 0.5992\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6643 - acc: 0.5984 - val_loss: 0.6631 - val_acc: 0.6008\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6633 - acc: 0.6000 - val_loss: 0.6626 - val_acc: 0.6024\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6623 - acc: 0.6024 - val_loss: 0.6617 - val_acc: 0.6032\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6618 - acc: 0.6033 - val_loss: 0.6616 - val_acc: 0.6031\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6607 - acc: 0.6043 - val_loss: 0.6607 - val_acc: 0.6040\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6602 - acc: 0.6044 - val_loss: 0.6599 - val_acc: 0.6064\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6590 - acc: 0.6057 - val_loss: 0.6590 - val_acc: 0.6080\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6582 - acc: 0.6066 - val_loss: 0.6595 - val_acc: 0.6083\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6579 - acc: 0.6075 - val_loss: 0.6585 - val_acc: 0.6088\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6569 - acc: 0.6090 - val_loss: 0.6580 - val_acc: 0.6095\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6563 - acc: 0.6103 - val_loss: 0.6576 - val_acc: 0.6094\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6556 - acc: 0.6108 - val_loss: 0.6572 - val_acc: 0.6100\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6552 - acc: 0.6112 - val_loss: 0.6570 - val_acc: 0.6116\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6546 - acc: 0.6136 - val_loss: 0.6568 - val_acc: 0.6119\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6540 - acc: 0.6132 - val_loss: 0.6563 - val_acc: 0.6113\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6531 - acc: 0.6137 - val_loss: 0.6562 - val_acc: 0.6116\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6529 - acc: 0.6146 - val_loss: 0.6574 - val_acc: 0.6114\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6521 - acc: 0.6154 - val_loss: 0.6555 - val_acc: 0.6131\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6520 - acc: 0.6153 - val_loss: 0.6556 - val_acc: 0.6130\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6512 - acc: 0.6170 - val_loss: 0.6558 - val_acc: 0.6123\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6509 - acc: 0.6172 - val_loss: 0.6551 - val_acc: 0.6128\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6502 - acc: 0.6175 - val_loss: 0.6548 - val_acc: 0.6141\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6496 - acc: 0.6181 - val_loss: 0.6547 - val_acc: 0.6143\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6492 - acc: 0.6182 - val_loss: 0.6548 - val_acc: 0.6136\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6488 - acc: 0.6194 - val_loss: 0.6544 - val_acc: 0.6146\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6483 - acc: 0.6194 - val_loss: 0.6546 - val_acc: 0.6145\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6477 - acc: 0.6202 - val_loss: 0.6540 - val_acc: 0.6149\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6473 - acc: 0.6209 - val_loss: 0.6543 - val_acc: 0.6143\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6466 - acc: 0.6220 - val_loss: 0.6540 - val_acc: 0.6156\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6457 - acc: 0.6227 - val_loss: 0.6542 - val_acc: 0.6155\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6458 - acc: 0.6238 - val_loss: 0.6542 - val_acc: 0.6157\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6450 - acc: 0.6241 - val_loss: 0.6542 - val_acc: 0.6144\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6449 - acc: 0.6240 - val_loss: 0.6542 - val_acc: 0.6151\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6440 - acc: 0.6247 - val_loss: 0.6536 - val_acc: 0.6155\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6437 - acc: 0.6253 - val_loss: 0.6536 - val_acc: 0.6155\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6432 - acc: 0.6260 - val_loss: 0.6539 - val_acc: 0.6160\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6420 - acc: 0.6273 - val_loss: 0.6536 - val_acc: 0.6158\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6421 - acc: 0.6283 - val_loss: 0.6538 - val_acc: 0.6170\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6418 - acc: 0.6271 - val_loss: 0.6534 - val_acc: 0.6166\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6412 - acc: 0.6298 - val_loss: 0.6532 - val_acc: 0.6164\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6407 - acc: 0.6291 - val_loss: 0.6531 - val_acc: 0.6168\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6403 - acc: 0.6289 - val_loss: 0.6534 - val_acc: 0.6182\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6393 - acc: 0.6311 - val_loss: 0.6532 - val_acc: 0.6167\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6389 - acc: 0.6301 - val_loss: 0.6534 - val_acc: 0.6164\n",
            "Epoch 52/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6387 - acc: 0.6308 - val_loss: 0.6532 - val_acc: 0.6171\n",
            "Epoch 53/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6378 - acc: 0.6322 - val_loss: 0.6536 - val_acc: 0.6175\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe2f8ab400>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 15-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6877 - acc: 0.5548 - val_loss: 0.6719 - val_acc: 0.5831\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6735 - acc: 0.5787 - val_loss: 0.6702 - val_acc: 0.5837\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6707 - acc: 0.5819 - val_loss: 0.6686 - val_acc: 0.5873\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6683 - acc: 0.5854 - val_loss: 0.6667 - val_acc: 0.5899\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6666 - acc: 0.5886 - val_loss: 0.6652 - val_acc: 0.5928\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6654 - acc: 0.5919 - val_loss: 0.6646 - val_acc: 0.5947\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6643 - acc: 0.5927 - val_loss: 0.6638 - val_acc: 0.5962\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6633 - acc: 0.5943 - val_loss: 0.6631 - val_acc: 0.5962\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6623 - acc: 0.5977 - val_loss: 0.6627 - val_acc: 0.5979\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6612 - acc: 0.5989 - val_loss: 0.6612 - val_acc: 0.6009\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6601 - acc: 0.6010 - val_loss: 0.6602 - val_acc: 0.6022\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6591 - acc: 0.6022 - val_loss: 0.6591 - val_acc: 0.6041\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6581 - acc: 0.6042 - val_loss: 0.6584 - val_acc: 0.6057\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6571 - acc: 0.6066 - val_loss: 0.6578 - val_acc: 0.6070\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6565 - acc: 0.6066 - val_loss: 0.6570 - val_acc: 0.6085\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6555 - acc: 0.6085 - val_loss: 0.6576 - val_acc: 0.6074\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6544 - acc: 0.6107 - val_loss: 0.6555 - val_acc: 0.6102\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6537 - acc: 0.6120 - val_loss: 0.6553 - val_acc: 0.6109\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6529 - acc: 0.6126 - val_loss: 0.6547 - val_acc: 0.6107\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6521 - acc: 0.6132 - val_loss: 0.6542 - val_acc: 0.6128\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6520 - acc: 0.6134 - val_loss: 0.6538 - val_acc: 0.6134\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6509 - acc: 0.6150 - val_loss: 0.6533 - val_acc: 0.6140\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6501 - acc: 0.6161 - val_loss: 0.6532 - val_acc: 0.6133\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6494 - acc: 0.6172 - val_loss: 0.6528 - val_acc: 0.6145\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6490 - acc: 0.6178 - val_loss: 0.6523 - val_acc: 0.6149\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6486 - acc: 0.6184 - val_loss: 0.6519 - val_acc: 0.6153\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6479 - acc: 0.6197 - val_loss: 0.6516 - val_acc: 0.6160\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6471 - acc: 0.6213 - val_loss: 0.6512 - val_acc: 0.6172\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6469 - acc: 0.6213 - val_loss: 0.6509 - val_acc: 0.6185\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6458 - acc: 0.6218 - val_loss: 0.6507 - val_acc: 0.6184\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6459 - acc: 0.6225 - val_loss: 0.6508 - val_acc: 0.6174\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6447 - acc: 0.6237 - val_loss: 0.6504 - val_acc: 0.6184\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6439 - acc: 0.6247 - val_loss: 0.6501 - val_acc: 0.6186\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6433 - acc: 0.6250 - val_loss: 0.6497 - val_acc: 0.6196\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6431 - acc: 0.6256 - val_loss: 0.6495 - val_acc: 0.6204\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6423 - acc: 0.6259 - val_loss: 0.6491 - val_acc: 0.6207\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6418 - acc: 0.6272 - val_loss: 0.6490 - val_acc: 0.6205\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6412 - acc: 0.6280 - val_loss: 0.6488 - val_acc: 0.6214\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6406 - acc: 0.6282 - val_loss: 0.6489 - val_acc: 0.6210\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.6298 - val_loss: 0.6486 - val_acc: 0.6210\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6396 - acc: 0.6293 - val_loss: 0.6487 - val_acc: 0.6215\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6388 - acc: 0.6312 - val_loss: 0.6483 - val_acc: 0.6218\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6382 - acc: 0.6314 - val_loss: 0.6483 - val_acc: 0.6216\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6374 - acc: 0.6321 - val_loss: 0.6482 - val_acc: 0.6220\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6371 - acc: 0.6320 - val_loss: 0.6481 - val_acc: 0.6224\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6361 - acc: 0.6315 - val_loss: 0.6485 - val_acc: 0.6217\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6362 - acc: 0.6319 - val_loss: 0.6483 - val_acc: 0.6225\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6354 - acc: 0.6347 - val_loss: 0.6479 - val_acc: 0.6231\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6340 - acc: 0.6352 - val_loss: 0.6479 - val_acc: 0.6224\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6342 - acc: 0.6349 - val_loss: 0.6479 - val_acc: 0.6230\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6335 - acc: 0.6353 - val_loss: 0.6481 - val_acc: 0.6232\n",
            "Epoch 52/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6333 - acc: 0.6357 - val_loss: 0.6481 - val_acc: 0.6227\n",
            "Epoch 53/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6325 - acc: 0.6366 - val_loss: 0.6481 - val_acc: 0.6235\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe2c9c9400>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 16-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 12us/step - loss: 0.6928 - acc: 0.5454 - val_loss: 0.6812 - val_acc: 0.5592\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6817 - acc: 0.5637 - val_loss: 0.6802 - val_acc: 0.5642\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6789 - acc: 0.5687 - val_loss: 0.6777 - val_acc: 0.5707\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6774 - acc: 0.5718 - val_loss: 0.6766 - val_acc: 0.5732\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6757 - acc: 0.5761 - val_loss: 0.6754 - val_acc: 0.5753\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6740 - acc: 0.5796 - val_loss: 0.6732 - val_acc: 0.5839\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6724 - acc: 0.5832 - val_loss: 0.6709 - val_acc: 0.5857\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6708 - acc: 0.5862 - val_loss: 0.6701 - val_acc: 0.5886\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6696 - acc: 0.5875 - val_loss: 0.6690 - val_acc: 0.5900\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6682 - acc: 0.5906 - val_loss: 0.6673 - val_acc: 0.5953\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6676 - acc: 0.5920 - val_loss: 0.6665 - val_acc: 0.5960\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6662 - acc: 0.5947 - val_loss: 0.6658 - val_acc: 0.5968\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6654 - acc: 0.5955 - val_loss: 0.6655 - val_acc: 0.5958\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6648 - acc: 0.5979 - val_loss: 0.6651 - val_acc: 0.5974\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6641 - acc: 0.5992 - val_loss: 0.6640 - val_acc: 0.5994\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6632 - acc: 0.5994 - val_loss: 0.6637 - val_acc: 0.6005\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6624 - acc: 0.6016 - val_loss: 0.6631 - val_acc: 0.6013\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6620 - acc: 0.6025 - val_loss: 0.6624 - val_acc: 0.6022\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6611 - acc: 0.6028 - val_loss: 0.6619 - val_acc: 0.6022\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6607 - acc: 0.6042 - val_loss: 0.6617 - val_acc: 0.6034\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6600 - acc: 0.6038 - val_loss: 0.6617 - val_acc: 0.6036\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6597 - acc: 0.6055 - val_loss: 0.6613 - val_acc: 0.6037\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6588 - acc: 0.6074 - val_loss: 0.6609 - val_acc: 0.6036\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6581 - acc: 0.6074 - val_loss: 0.6606 - val_acc: 0.6042\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6581 - acc: 0.6071 - val_loss: 0.6605 - val_acc: 0.6052\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6572 - acc: 0.6091 - val_loss: 0.6604 - val_acc: 0.6041\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6564 - acc: 0.6099 - val_loss: 0.6603 - val_acc: 0.6059\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6563 - acc: 0.6108 - val_loss: 0.6600 - val_acc: 0.6062\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6558 - acc: 0.6100 - val_loss: 0.6595 - val_acc: 0.6071\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6548 - acc: 0.6121 - val_loss: 0.6593 - val_acc: 0.6075\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6546 - acc: 0.6122 - val_loss: 0.6595 - val_acc: 0.6057\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6539 - acc: 0.6132 - val_loss: 0.6597 - val_acc: 0.6068\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6537 - acc: 0.6129 - val_loss: 0.6586 - val_acc: 0.6085\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6531 - acc: 0.6134 - val_loss: 0.6586 - val_acc: 0.6073\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6526 - acc: 0.6145 - val_loss: 0.6585 - val_acc: 0.6084\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6517 - acc: 0.6161 - val_loss: 0.6582 - val_acc: 0.6083\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6516 - acc: 0.6149 - val_loss: 0.6580 - val_acc: 0.6097\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6505 - acc: 0.6175 - val_loss: 0.6600 - val_acc: 0.6056\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6500 - acc: 0.6181 - val_loss: 0.6581 - val_acc: 0.6089\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6496 - acc: 0.6176 - val_loss: 0.6587 - val_acc: 0.6078\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6492 - acc: 0.6184 - val_loss: 0.6577 - val_acc: 0.6111\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6489 - acc: 0.6189 - val_loss: 0.6578 - val_acc: 0.6088\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6481 - acc: 0.6197 - val_loss: 0.6575 - val_acc: 0.6097\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6474 - acc: 0.6221 - val_loss: 0.6577 - val_acc: 0.6102\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6468 - acc: 0.6222 - val_loss: 0.6574 - val_acc: 0.6095\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6464 - acc: 0.6213 - val_loss: 0.6574 - val_acc: 0.6104\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6460 - acc: 0.6227 - val_loss: 0.6571 - val_acc: 0.6096\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6458 - acc: 0.6231 - val_loss: 0.6571 - val_acc: 0.6109\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6450 - acc: 0.6244 - val_loss: 0.6581 - val_acc: 0.6104\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6453 - acc: 0.6231 - val_loss: 0.6573 - val_acc: 0.6102\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6438 - acc: 0.6238 - val_loss: 0.6574 - val_acc: 0.6109\n",
            "Epoch 52/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6438 - acc: 0.6248 - val_loss: 0.6573 - val_acc: 0.6110\n",
            "Epoch 53/10000\n",
            "204079/204079 [==============================] - 2s 9us/step - loss: 0.6432 - acc: 0.6249 - val_loss: 0.6573 - val_acc: 0.6111\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe3e22cda0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 17-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 12us/step - loss: 0.6812 - acc: 0.5721 - val_loss: 0.6652 - val_acc: 0.5991\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6676 - acc: 0.5945 - val_loss: 0.6632 - val_acc: 0.6027\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6634 - acc: 0.6000 - val_loss: 0.6597 - val_acc: 0.6079\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6601 - acc: 0.6050 - val_loss: 0.6562 - val_acc: 0.6105\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6571 - acc: 0.6096 - val_loss: 0.6550 - val_acc: 0.6100\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6557 - acc: 0.6107 - val_loss: 0.6535 - val_acc: 0.6155\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6547 - acc: 0.6124 - val_loss: 0.6524 - val_acc: 0.6169\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6537 - acc: 0.6141 - val_loss: 0.6517 - val_acc: 0.6166\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 11us/step - loss: 0.6525 - acc: 0.6150 - val_loss: 0.6512 - val_acc: 0.6181\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6518 - acc: 0.6167 - val_loss: 0.6508 - val_acc: 0.6186\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6508 - acc: 0.6181 - val_loss: 0.6495 - val_acc: 0.6193\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6499 - acc: 0.6182 - val_loss: 0.6494 - val_acc: 0.6201\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6491 - acc: 0.6192 - val_loss: 0.6487 - val_acc: 0.6212\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6480 - acc: 0.6199 - val_loss: 0.6478 - val_acc: 0.6220\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6474 - acc: 0.6212 - val_loss: 0.6477 - val_acc: 0.6218\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6466 - acc: 0.6236 - val_loss: 0.6472 - val_acc: 0.6224\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6461 - acc: 0.6233 - val_loss: 0.6467 - val_acc: 0.6230\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6453 - acc: 0.6244 - val_loss: 0.6464 - val_acc: 0.6235\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6448 - acc: 0.6251 - val_loss: 0.6462 - val_acc: 0.6240\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6443 - acc: 0.6254 - val_loss: 0.6459 - val_acc: 0.6248\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6440 - acc: 0.6253 - val_loss: 0.6456 - val_acc: 0.6245\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6432 - acc: 0.6257 - val_loss: 0.6454 - val_acc: 0.6246\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6430 - acc: 0.6265 - val_loss: 0.6455 - val_acc: 0.6255\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6421 - acc: 0.6278 - val_loss: 0.6450 - val_acc: 0.6251\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6421 - acc: 0.6273 - val_loss: 0.6449 - val_acc: 0.6260\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6413 - acc: 0.6286 - val_loss: 0.6447 - val_acc: 0.6264\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6415 - acc: 0.6282 - val_loss: 0.6444 - val_acc: 0.6266\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6406 - acc: 0.6283 - val_loss: 0.6446 - val_acc: 0.6265\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6399 - acc: 0.6292 - val_loss: 0.6448 - val_acc: 0.6268\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6396 - acc: 0.6300 - val_loss: 0.6446 - val_acc: 0.6271\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6395 - acc: 0.6301 - val_loss: 0.6443 - val_acc: 0.6269\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6392 - acc: 0.6304 - val_loss: 0.6447 - val_acc: 0.6273\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6383 - acc: 0.6321 - val_loss: 0.6445 - val_acc: 0.6278\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6381 - acc: 0.6309 - val_loss: 0.6445 - val_acc: 0.6275\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6377 - acc: 0.6311 - val_loss: 0.6440 - val_acc: 0.6276\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6373 - acc: 0.6332 - val_loss: 0.6440 - val_acc: 0.6286\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6366 - acc: 0.6324 - val_loss: 0.6445 - val_acc: 0.6273\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6362 - acc: 0.6333 - val_loss: 0.6440 - val_acc: 0.6285\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6358 - acc: 0.6338 - val_loss: 0.6440 - val_acc: 0.6280\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6356 - acc: 0.6337 - val_loss: 0.6439 - val_acc: 0.6284\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6348 - acc: 0.6347 - val_loss: 0.6438 - val_acc: 0.6281\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6347 - acc: 0.6341 - val_loss: 0.6435 - val_acc: 0.6278\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6339 - acc: 0.6354 - val_loss: 0.6441 - val_acc: 0.6289\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6336 - acc: 0.6355 - val_loss: 0.6442 - val_acc: 0.6287\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6328 - acc: 0.6361 - val_loss: 0.6443 - val_acc: 0.6275\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6332 - acc: 0.6359 - val_loss: 0.6439 - val_acc: 0.6283\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6322 - acc: 0.6368 - val_loss: 0.6446 - val_acc: 0.6284\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe26ce32e8>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 18-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 12us/step - loss: 0.6850 - acc: 0.5649 - val_loss: 0.6679 - val_acc: 0.5975\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6702 - acc: 0.5897 - val_loss: 0.6646 - val_acc: 0.6001\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6652 - acc: 0.5971 - val_loss: 0.6610 - val_acc: 0.6049\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6615 - acc: 0.6032 - val_loss: 0.6578 - val_acc: 0.6096\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6590 - acc: 0.6072 - val_loss: 0.6564 - val_acc: 0.6115\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6575 - acc: 0.6094 - val_loss: 0.6555 - val_acc: 0.6129\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6563 - acc: 0.6109 - val_loss: 0.6549 - val_acc: 0.6128\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6553 - acc: 0.6128 - val_loss: 0.6543 - val_acc: 0.6145\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6547 - acc: 0.6144 - val_loss: 0.6529 - val_acc: 0.6163\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6541 - acc: 0.6148 - val_loss: 0.6527 - val_acc: 0.6170\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6530 - acc: 0.6162 - val_loss: 0.6519 - val_acc: 0.6189\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6525 - acc: 0.6163 - val_loss: 0.6519 - val_acc: 0.6184\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6518 - acc: 0.6166 - val_loss: 0.6511 - val_acc: 0.6200\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6514 - acc: 0.6180 - val_loss: 0.6510 - val_acc: 0.6200\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6508 - acc: 0.6185 - val_loss: 0.6508 - val_acc: 0.6202\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6500 - acc: 0.6198 - val_loss: 0.6503 - val_acc: 0.6213\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6494 - acc: 0.6203 - val_loss: 0.6499 - val_acc: 0.6219\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6491 - acc: 0.6203 - val_loss: 0.6502 - val_acc: 0.6204\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6483 - acc: 0.6214 - val_loss: 0.6499 - val_acc: 0.6211\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6482 - acc: 0.6209 - val_loss: 0.6494 - val_acc: 0.6219\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6475 - acc: 0.6232 - val_loss: 0.6495 - val_acc: 0.6215\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6469 - acc: 0.6238 - val_loss: 0.6491 - val_acc: 0.6225\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6464 - acc: 0.6227 - val_loss: 0.6488 - val_acc: 0.6224\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6460 - acc: 0.6232 - val_loss: 0.6485 - val_acc: 0.6230\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6453 - acc: 0.6248 - val_loss: 0.6484 - val_acc: 0.6229\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6449 - acc: 0.6258 - val_loss: 0.6484 - val_acc: 0.6238\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6447 - acc: 0.6248 - val_loss: 0.6482 - val_acc: 0.6230\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6441 - acc: 0.6254 - val_loss: 0.6480 - val_acc: 0.6238\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6436 - acc: 0.6248 - val_loss: 0.6485 - val_acc: 0.6238\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6430 - acc: 0.6270 - val_loss: 0.6480 - val_acc: 0.6235\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6428 - acc: 0.6280 - val_loss: 0.6477 - val_acc: 0.6241\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6422 - acc: 0.6283 - val_loss: 0.6475 - val_acc: 0.6241\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6415 - acc: 0.6288 - val_loss: 0.6483 - val_acc: 0.6237\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6408 - acc: 0.6290 - val_loss: 0.6472 - val_acc: 0.6246\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6407 - acc: 0.6292 - val_loss: 0.6469 - val_acc: 0.6245\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.6297 - val_loss: 0.6472 - val_acc: 0.6238\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6394 - acc: 0.6310 - val_loss: 0.6467 - val_acc: 0.6250\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6391 - acc: 0.6305 - val_loss: 0.6464 - val_acc: 0.6262\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6389 - acc: 0.6306 - val_loss: 0.6470 - val_acc: 0.6242\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6385 - acc: 0.6319 - val_loss: 0.6465 - val_acc: 0.6257\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6376 - acc: 0.6315 - val_loss: 0.6464 - val_acc: 0.6264\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6373 - acc: 0.6317 - val_loss: 0.6464 - val_acc: 0.6265\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6365 - acc: 0.6329 - val_loss: 0.6463 - val_acc: 0.6255\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6363 - acc: 0.6332 - val_loss: 0.6463 - val_acc: 0.6260\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6359 - acc: 0.6332 - val_loss: 0.6467 - val_acc: 0.6253\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6350 - acc: 0.6343 - val_loss: 0.6471 - val_acc: 0.6257\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6342 - acc: 0.6348 - val_loss: 0.6462 - val_acc: 0.6266\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6343 - acc: 0.6353 - val_loss: 0.6463 - val_acc: 0.6260\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6336 - acc: 0.6360 - val_loss: 0.6463 - val_acc: 0.6265\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6331 - acc: 0.6373 - val_loss: 0.6464 - val_acc: 0.6263\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6325 - acc: 0.6369 - val_loss: 0.6459 - val_acc: 0.6267\n",
            "Epoch 52/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6320 - acc: 0.6372 - val_loss: 0.6463 - val_acc: 0.6253\n",
            "Epoch 53/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6315 - acc: 0.6379 - val_loss: 0.6468 - val_acc: 0.6276\n",
            "Epoch 54/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6312 - acc: 0.6388 - val_loss: 0.6465 - val_acc: 0.6261\n",
            "Epoch 55/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6308 - acc: 0.6385 - val_loss: 0.6460 - val_acc: 0.6264\n",
            "Epoch 56/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6296 - acc: 0.6406 - val_loss: 0.6460 - val_acc: 0.6264\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe23e5af28>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 19-----------------\n",
            "\n",
            "Train on 204079 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204079/204079 [==============================] - 2s 12us/step - loss: 0.6921 - acc: 0.5434 - val_loss: 0.6784 - val_acc: 0.5693\n",
            "Epoch 2/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6810 - acc: 0.5608 - val_loss: 0.6778 - val_acc: 0.5691\n",
            "Epoch 3/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6790 - acc: 0.5654 - val_loss: 0.6773 - val_acc: 0.5694\n",
            "Epoch 4/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6778 - acc: 0.5677 - val_loss: 0.6763 - val_acc: 0.5702\n",
            "Epoch 5/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6772 - acc: 0.5683 - val_loss: 0.6760 - val_acc: 0.5717\n",
            "Epoch 6/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6766 - acc: 0.5686 - val_loss: 0.6753 - val_acc: 0.5722\n",
            "Epoch 7/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6758 - acc: 0.5706 - val_loss: 0.6745 - val_acc: 0.5750\n",
            "Epoch 8/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6745 - acc: 0.5747 - val_loss: 0.6727 - val_acc: 0.5799\n",
            "Epoch 9/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6724 - acc: 0.5792 - val_loss: 0.6711 - val_acc: 0.5841\n",
            "Epoch 10/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6708 - acc: 0.5839 - val_loss: 0.6685 - val_acc: 0.5922\n",
            "Epoch 11/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6685 - acc: 0.5888 - val_loss: 0.6659 - val_acc: 0.5962\n",
            "Epoch 12/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6673 - acc: 0.5922 - val_loss: 0.6651 - val_acc: 0.6000\n",
            "Epoch 13/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6657 - acc: 0.5959 - val_loss: 0.6636 - val_acc: 0.5994\n",
            "Epoch 14/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6647 - acc: 0.5958 - val_loss: 0.6624 - val_acc: 0.6026\n",
            "Epoch 15/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6630 - acc: 0.5994 - val_loss: 0.6612 - val_acc: 0.6032\n",
            "Epoch 16/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6623 - acc: 0.6013 - val_loss: 0.6608 - val_acc: 0.6052\n",
            "Epoch 17/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6616 - acc: 0.6011 - val_loss: 0.6604 - val_acc: 0.6045\n",
            "Epoch 18/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6607 - acc: 0.6034 - val_loss: 0.6596 - val_acc: 0.6056\n",
            "Epoch 19/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6598 - acc: 0.6034 - val_loss: 0.6586 - val_acc: 0.6076\n",
            "Epoch 20/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6587 - acc: 0.6062 - val_loss: 0.6581 - val_acc: 0.6082\n",
            "Epoch 21/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6583 - acc: 0.6079 - val_loss: 0.6580 - val_acc: 0.6091\n",
            "Epoch 22/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6575 - acc: 0.6083 - val_loss: 0.6572 - val_acc: 0.6104\n",
            "Epoch 23/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6566 - acc: 0.6091 - val_loss: 0.6564 - val_acc: 0.6108\n",
            "Epoch 24/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6557 - acc: 0.6100 - val_loss: 0.6568 - val_acc: 0.6111\n",
            "Epoch 25/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6553 - acc: 0.6110 - val_loss: 0.6562 - val_acc: 0.6111\n",
            "Epoch 26/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6542 - acc: 0.6123 - val_loss: 0.6559 - val_acc: 0.6118\n",
            "Epoch 27/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6535 - acc: 0.6146 - val_loss: 0.6552 - val_acc: 0.6135\n",
            "Epoch 28/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6527 - acc: 0.6142 - val_loss: 0.6549 - val_acc: 0.6131\n",
            "Epoch 29/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6525 - acc: 0.6156 - val_loss: 0.6542 - val_acc: 0.6137\n",
            "Epoch 30/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6512 - acc: 0.6165 - val_loss: 0.6547 - val_acc: 0.6126\n",
            "Epoch 31/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6507 - acc: 0.6165 - val_loss: 0.6542 - val_acc: 0.6146\n",
            "Epoch 32/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6500 - acc: 0.6183 - val_loss: 0.6539 - val_acc: 0.6146\n",
            "Epoch 33/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6499 - acc: 0.6177 - val_loss: 0.6537 - val_acc: 0.6150\n",
            "Epoch 34/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6487 - acc: 0.6197 - val_loss: 0.6536 - val_acc: 0.6150\n",
            "Epoch 35/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6480 - acc: 0.6202 - val_loss: 0.6536 - val_acc: 0.6153\n",
            "Epoch 36/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6479 - acc: 0.6215 - val_loss: 0.6538 - val_acc: 0.6164\n",
            "Epoch 37/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6472 - acc: 0.6217 - val_loss: 0.6529 - val_acc: 0.6152\n",
            "Epoch 38/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6463 - acc: 0.6221 - val_loss: 0.6525 - val_acc: 0.6166\n",
            "Epoch 39/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6456 - acc: 0.6230 - val_loss: 0.6529 - val_acc: 0.6173\n",
            "Epoch 40/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6456 - acc: 0.6249 - val_loss: 0.6526 - val_acc: 0.6164\n",
            "Epoch 41/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6449 - acc: 0.6247 - val_loss: 0.6524 - val_acc: 0.6173\n",
            "Epoch 42/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6439 - acc: 0.6259 - val_loss: 0.6523 - val_acc: 0.6179\n",
            "Epoch 43/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6435 - acc: 0.6262 - val_loss: 0.6525 - val_acc: 0.6169\n",
            "Epoch 44/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6427 - acc: 0.6267 - val_loss: 0.6519 - val_acc: 0.6188\n",
            "Epoch 45/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6424 - acc: 0.6278 - val_loss: 0.6517 - val_acc: 0.6180\n",
            "Epoch 46/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6415 - acc: 0.6277 - val_loss: 0.6521 - val_acc: 0.6179\n",
            "Epoch 47/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6413 - acc: 0.6284 - val_loss: 0.6517 - val_acc: 0.6183\n",
            "Epoch 48/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6409 - acc: 0.6285 - val_loss: 0.6520 - val_acc: 0.6177\n",
            "Epoch 49/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6404 - acc: 0.6297 - val_loss: 0.6518 - val_acc: 0.6183\n",
            "Epoch 50/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6391 - acc: 0.6312 - val_loss: 0.6520 - val_acc: 0.6184\n",
            "Epoch 51/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6395 - acc: 0.6303 - val_loss: 0.6517 - val_acc: 0.6187\n",
            "Epoch 52/10000\n",
            "204079/204079 [==============================] - 2s 10us/step - loss: 0.6382 - acc: 0.6308 - val_loss: 0.6521 - val_acc: 0.6191\n",
            "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7efe20f7c208>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rhjtCfbUAVeF",
        "colab_type": "code",
        "outputId": "0cb72613-9432-4957-f5a3-af7b16b5becf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        }
      },
      "cell_type": "code",
      "source": [
        "evaluate_model(features, params)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEVCAYAAAALsCk2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng4IMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7efdfae9b898>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy:         0.6352\n",
            "Recall:           0.5851\n",
            "Precision:        0.6502\n",
            "F1:               0.6160\n",
            "AUROC:            0.6868\n",
            "AUPR:             0.6848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BynnaZnV_suQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}