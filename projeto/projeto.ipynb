{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projeto.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "22FDBji8AVbb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# IF702 - Redes Neurais\n",
        "\n",
        "#### Equipe\n",
        "* Bruno César - bcgs\n",
        "* Franclin Cabral - fcmo\n",
        "* Ítalo Rodrigo Barbosa Paulino - irbp\n",
        "* José Nilton de Oliveira Lima Júnior - jnolj\n",
        "\n",
        "Este notebook contém os scripts executados em cada passo do projeto da disciplina de Redes Neurais."
      ]
    },
    {
      "metadata": {
        "id": "_V1_Nx7F942d",
        "colab_type": "code",
        "outputId": "2afde630-4f1f-4f71-cd55-330f74fc360d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://cin.ufpe.br/~gcv/web_lci/TRN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-21 15:56:57--  http://cin.ufpe.br/~gcv/web_lci/TRN\n",
            "Resolving cin.ufpe.br (cin.ufpe.br)... 150.161.2.9\n",
            "Connecting to cin.ufpe.br (cin.ufpe.br)|150.161.2.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 766723352 (731M) [application/vnd.rim.cod]\n",
            "Saving to: ‘TRN’\n",
            "\n",
            "TRN                  33%[=====>              ] 245.10M  12.5MB/s    eta 41s    ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GwaC6c6zAVbh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Leitura e limpeza dos dados\n",
        "A leitura dos dados é feita utilizando a biblioteca pandas. Os trechos de código que seguem abaixo fazem a leitura do dataset de crédito \"TRN\" e a remoção de features que não nos interessam, mas estão presentes no dataset."
      ]
    },
    {
      "metadata": {
        "id": "ooX48t-jAVbk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/gdrive/My Drive/Acadêmico/10º Período/Redes Neurais/Projeto/dataset/\"\n",
        "MODELS_PATH = \"/content/gdrive/My Drive/Acadêmico/10º Período/Redes Neurais/Projeto/mlp_models/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLJewUO4AVbw",
        "colab_type": "code",
        "outputId": "4c17f4c4-a7e4-43bf-ae5a-0716c530cd8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Only for google colab\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "!pip3 install git+https://github.com/irbp/neuro-evolution.git\n",
        "!pip3 install -U imbalanced-learn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ip2hZJECO95",
        "colab_type": "code",
        "outputId": "7b4bc751-43db-4ee3-eeb6-4e84516a896d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Only for google colab\"\"\"\n",
        "!wget http://cin.ufpe.br/~gcv/web_lci/TRN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-19 23:37:32--  http://cin.ufpe.br/~gcv/web_lci/TRN\n",
            "Resolving cin.ufpe.br (cin.ufpe.br)... 150.161.2.9\n",
            "Connecting to cin.ufpe.br (cin.ufpe.br)|150.161.2.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 766723352 (731M) [application/vnd.rim.cod]\n",
            "Saving to: ‘TRN’\n",
            "\n",
            "TRN                 100%[===================>] 731.20M  9.48MB/s    in 2m 32s  \n",
            "\n",
            "2018-11-19 23:40:06 (4.80 MB/s) - ‘TRN’ saved [766723352/766723352]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SsHHhVSgAVb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JGPtAuAiAVb_",
        "colab_type": "code",
        "outputId": "3dfd2820-f27d-4fcf-fde2-0fc8d0c89525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_table(\"TRN\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>INDEX</th>\n",
              "      <th>UF_1</th>\n",
              "      <th>UF_2</th>\n",
              "      <th>UF_3</th>\n",
              "      <th>UF_4</th>\n",
              "      <th>UF_5</th>\n",
              "      <th>UF_6</th>\n",
              "      <th>UF_7</th>\n",
              "      <th>IDADE</th>\n",
              "      <th>SEXO_1</th>\n",
              "      <th>...</th>\n",
              "      <th>CEP4_7</th>\n",
              "      <th>CEP4_8</th>\n",
              "      <th>CEP4_9</th>\n",
              "      <th>CEP4_10</th>\n",
              "      <th>CEP4_11</th>\n",
              "      <th>CEP4_12</th>\n",
              "      <th>CEP4_13</th>\n",
              "      <th>CEP4_14</th>\n",
              "      <th>IND_BOM_1_1</th>\n",
              "      <th>IND_BOM_1_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.135098</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.273504</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.281910</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.225741</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.480403</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 246 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   INDEX  UF_1  UF_2  UF_3  UF_4  UF_5  UF_6  UF_7     IDADE  SEXO_1  \\\n",
              "0      0     1     1     1     0     0     0     0  0.135098       1   \n",
              "1      1     1     0     1     0     0     1     0  0.273504       1   \n",
              "2      2     1     0     1     0     0     1     0  0.281910       0   \n",
              "3      3     1     1     1     0     0     0     0  0.225741       0   \n",
              "4      4     1     1     0     0     0     1     0  0.480403       0   \n",
              "\n",
              "      ...       CEP4_7  CEP4_8  CEP4_9  CEP4_10  CEP4_11  CEP4_12  CEP4_13  \\\n",
              "0     ...            0       0       1        1        0        1        1   \n",
              "1     ...            0       1       0        1        1        0        0   \n",
              "2     ...            1       1       0        0        0        0        1   \n",
              "3     ...            1       1       0        1        1        0        1   \n",
              "4     ...            1       1       1        0        0        1        0   \n",
              "\n",
              "   CEP4_14  IND_BOM_1_1  IND_BOM_1_2  \n",
              "0        1            0            1  \n",
              "1        0            1            0  \n",
              "2        0            1            0  \n",
              "3        0            1            0  \n",
              "4        1            1            0  \n",
              "\n",
              "[5 rows x 246 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "BNsZkdCLAVcD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Divisão dos dados em treino, teste e validação"
      ]
    },
    {
      "metadata": {
        "id": "IupHaHBBAVcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 1:-2].values\n",
        "y = df.iloc[:, -2].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=1/4,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
        "                                                  y_train,\n",
        "                                                  test_size=1/3,\n",
        "                                                  random_state=42,\n",
        "                                                  stratify=y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SWGXjumHAVcK",
        "colab_type": "code",
        "outputId": "eef4fc9a-bab9-410d-f8ba-b388af7da288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "cell_type": "code",
      "source": [
        "df[\"IND_BOM_1_1\"].value_counts().plot.bar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc23b4059e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD0CAYAAAB0KjqYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fc23b405a90>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "yOfdaCpYAVcR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sampling dos dados e normalização\n",
        "\n",
        "Como podemos ver no gráfico plotado acima, há um desbalanceamento entre as duas classes presentes no dataset. Para resolver este problema vamos utilizar uma técnica de oversampling bem simples que consiste em escolher algumas instâncias aleatórias da classe minoritária e replicá-las algumas vezes. Mas antes disso vamos normalizar a nossa base de dados. É interessante fazer a normalização antes do under/oversampling dos dados pois técnicas como o SMOTE fazem uso de algoritmos como o k-NN para gerar os samples, e estes obtém um melhor desempenho com dados normalizados."
      ]
    },
    {
      "metadata": {
        "id": "rlgNnGvmAVcV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0JOOskadAVce",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora vamos aplicar a técnica de oversampling nos conjuntos de treino, teste e validação, assim como verificar se a proporção está de 1/2 para cada classe dos conjuntos."
      ]
    },
    {
      "metadata": {
        "id": "LSbrMvJOAVcg",
        "colab_type": "code",
        "outputId": "7a7e44f6-01c5-49ff-c1ad-5165434d40a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "cell_type": "code",
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "# Oversampling\n",
        "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
        "X_test, y_test = ros.fit_resample(X_test, y_test)\n",
        "X_val, y_val = ros.fit_resample(X_val, y_val)\n",
        "\n",
        "print(\"***Train***\")\n",
        "print(pd.value_counts(pd.Series(y_train), normalize=True))\n",
        "print()\n",
        "print(\"***Test***\")\n",
        "print(pd.value_counts(pd.Series(y_test), normalize=True))\n",
        "print()\n",
        "print(\"***Validation***\")\n",
        "print(pd.value_counts(pd.Series(y_val), normalize=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***Train***\n",
            "1    0.5\n",
            "0    0.5\n",
            "dtype: float64\n",
            "\n",
            "***Test***\n",
            "1    0.5\n",
            "0    0.5\n",
            "dtype: float64\n",
            "\n",
            "***Validation***\n",
            "1    0.5\n",
            "0    0.5\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uxYVix6wAVcq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Seleção de features\n",
        "Como o dataset apresenta bastante features se torna importante fazer uma análise de quais delas são realmente relevantes para representar o nosso problema. Features irrelevantes além tornar o tempo de treinamento maior também podem diminuir a acurácia do nosso modelo. Sendo assim, nos trechos abaixo vamos fazer uma análise de quais features são mais relevantes para representar o nosso problema."
      ]
    },
    {
      "metadata": {
        "id": "V2agh7e-AVcs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Fitting the PCA algorithm with our Data\n",
        "pca = PCA().fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IKe-yOt9AVcv",
        "colab_type": "code",
        "outputId": "db629620-03a7-4704-b451-e402831ad096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "# Plotting the Cumulative Summation of the Explained Variance\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Número de componentes')\n",
        "plt.ylabel('Variância (%)')\n",
        "plt.title('Variância do conjunto de treino')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEVCAYAAAACW4lMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fc23b3254a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "poJ6ozIMAVc1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "n_components = np.arange(X_train.shape[1])[cumsum >= 0.9][0] + 1\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_val = pca.transform(X_val)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "np.save(DATASET_PATH + \"X_train.npy\", X_train)\n",
        "np.save(DATASET_PATH + \"y_train.npy\", y_train)\n",
        "np.save(DATASET_PATH + \"X_test.npy\", X_test)\n",
        "np.save(DATASET_PATH + \"y_test.npy\", y_test)\n",
        "np.save(DATASET_PATH + \"X_val.npy\", X_val)\n",
        "np.save(DATASET_PATH + \"y_val.npy\", y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Yq9a1QgAVc8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Definição dos parâmetros da MLP\n",
        "\n",
        "Pensando em obter os melhores parâmetros possíveis e ao mesmo tempo visando o desempenho, nós optamos por utilizar algoritmo genético, pois este apresenta resultado mais rápido do que o grid search. Ao invés de testar todas as possíveis combinações de hiperparâmetros, o algoritmo genético irá selecionar os melhores indivíduos onde cada indivíduo representa uma combinação de hiperparâmetros. A métrica de avaliação do indivíduo é a acurácia do conjunto de validação, ou seja, para cada indivíduo uma rede neural é criada e treinada utilizando os parâmetros que este indíviduo está codificando. Após treinada ela é avaliada e o resultado dessa avaliação é utilizado para saber quais indivíduos irão representar aquela geração e assim criar indivíduos ainda melhores.\n",
        "\n",
        "Para isso nós estamos fazendo uso da bilioteca neuro-evolution (https://github.com/irbp/neuro-evolution), pois ela já apresenta tudo o que é necessário para realizarmos o que foi descrito acima."
      ]
    },
    {
      "metadata": {
        "id": "_Cx1Lr0IAVc-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from neuro_evolution.evolution import NeuroEvolution\n",
        "\n",
        "params = {\n",
        "    \"batch_size\": [1024],\n",
        "    \"n_layers\": [1, 2, 3],\n",
        "    \"n_neurons\": [256, 512, 1024],\n",
        "    \"dropout\": [0.1, 0.25, 0.5],\n",
        "    \"optimizers\": [\"rmsprop\", \"adam\"],\n",
        "    \"activations\": [\"relu\", \"sigmoid\", \"tanh\"],\n",
        "    \"last_layer_activations\": [\"sigmoid\"],\n",
        "    \"losses\": [\"binary_crossentropy\"],\n",
        "    \"metrics\": [\"accuracy\"],\n",
        "    \"verbose\": [0]\n",
        "}\n",
        "search = NeuroEvolution(generations=10, population=10, params=params)\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfcrnwBsAVdE",
        "colab_type": "code",
        "outputId": "52dd050b-f606-489d-e871-f513a920cbc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3337
        }
      },
      "cell_type": "code",
      "source": [
        "search.evolve(X_train, y_train, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:19<02:51, 19.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.636583874416429\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:26<04:28, 33.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6364741116583649\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [02:14<04:25, 37.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6405823690465751\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:51<03:44, 37.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6335810832056579\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [03:33<03:14, 38.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6344043028004348\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [04:11<02:34, 38.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6357528145275804\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [04:40<01:47, 35.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6332204347701783\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [05:00<01:02, 31.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6281791951767973\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [05:34<00:31, 31.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6380264685191122\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [06:08<00:00, 32.68s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.5904287014432411\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:41<06:10, 41.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6399394737181384\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:01<04:39, 34.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6383165552294834\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [01:19<03:28, 29.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.635917458480556\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:22<03:58, 39.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6409351773001376\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [02:48<02:58, 35.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6164345971693918\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [03:10<02:06, 31.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6359880202523956\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [03:24<01:19, 26.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6334164394153067\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [03:41<00:46, 23.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6272070123723359\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [04:15<00:26, 26.64s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6168030858484737\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [05:51<00:00, 47.61s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6383243954730855\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [01:15<11:17, 75.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6382381535355518\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [02:07<09:07, 68.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6408018942449317\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [03:44<08:58, 76.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6369680433509289\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [04:14<06:16, 62.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6357920156539983\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [04:36<04:12, 50.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6356116911783026\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [05:25<03:19, 50.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6407626933203923\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [06:16<02:30, 50.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6407626933708619\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [06:34<01:21, 40.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6358155360670332\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [06:56<00:35, 35.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6336908457973592\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [07:42<00:00, 38.43s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6408175745620348\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:55<08:23, 55.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.639876752182424\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:44<07:09, 53.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6411625426262033\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [02:30<06:00, 51.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6403785241148517\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [03:25<05:15, 52.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6417740773166953\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [04:35<04:48, 57.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6391397748279988\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [05:06<03:18, 49.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6384027973763726\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [05:43<02:17, 45.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6379715871148453\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [06:11<01:21, 40.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6373443723670753\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [06:47<00:39, 39.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6405980494758329\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [07:25<00:00, 38.79s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6399786747006245\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:44<06:39, 44.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.640684291514306\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:37<06:15, 46.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6408018942486703\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [02:19<05:17, 45.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6408410951227401\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [03:07<04:37, 46.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6416486342957362\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [04:06<04:10, 50.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6414761501608434\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [04:58<03:22, 50.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6419857621705105\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [05:48<02:31, 50.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6420092828938402\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [06:36<01:39, 49.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6414134287260683\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [07:37<00:53, 53.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6408332548249298\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [08:19<00:00, 49.72s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6402452411175925\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:40<06:01, 40.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6392260168739488\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:32<05:51, 43.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6401197980891564\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [02:13<05:01, 43.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6383792767689362\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:58<04:20, 43.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6407940539508599\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [03:45<03:42, 44.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6414212688575157\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [04:37<03:06, 46.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6422444884597697\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [05:24<02:21, 47.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.640041396290547\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [06:08<01:31, 45.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6405039671395495\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [06:51<00:45, 45.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6408724559158321\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [07:43<00:00, 47.22s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6417897574823895\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:54<08:06, 54.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6400492364219945\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:49<07:16, 54.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.640315802788493\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [02:37<06:06, 52.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6398689120434995\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [03:30<05:16, 52.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6408097344343259\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [04:20<04:18, 51.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6410684606114302\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [05:13<03:29, 52.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6402295605873954\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [06:05<02:36, 52.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6388418478310327\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [06:53<01:41, 50.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6398218710342437\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [07:53<00:53, 53.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6413350269237205\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [08:43<00:00, 52.47s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6404490858362218\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:45<06:49, 45.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6409508578303347\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:34<06:13, 46.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.641201743659159\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [02:15<05:13, 44.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6405039670853413\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [03:06<04:40, 46.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6406372502414864\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [04:03<04:09, 49.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6413507072987701\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [04:48<03:13, 48.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6406607709573392\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [05:26<02:16, 45.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6399002728440686\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [06:18<01:34, 47.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6418367986467927\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [07:08<00:48, 48.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6412487846684148\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [08:02<00:00, 49.73s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.641217423983739\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:44<06:41, 44.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6400492364219945\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:36<06:15, 46.95s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6410449398413693\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [02:33<05:49, 49.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6402687616240897\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [03:14<04:42, 47.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6386301628762782\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [03:56<03:48, 45.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.640778373695442\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [04:45<03:06, 46.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6406450905318198\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [05:39<02:26, 48.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6397669896299768\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [06:28<01:37, 48.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6401354784679446\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [07:23<00:50, 50.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6402295606958117\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10/10 [08:11<00:00, 50.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ Accuracy: 0.6410135792613714\n",
            "[20/Nov/2018 00:50:53] INFO - best accuracy: 0.6410449398413693, best params: {'batch_size': 1024, 'n_layers': 2, 'n_neurons': 256, 'dropout': 0.25, 'optimizers': 'adam', 'activations': 'sigmoid', 'last_layer_activations': 'sigmoid', 'losses': 'binary_crossentropy', 'metrics': 'accuracy', 'verbose': 0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "g7f_HOrjZBko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Treino da MLP\n",
        "\n",
        "Logo abaixo a arquitetura da nossa rede será definida utilizando os parâmetros escolhidos pelo algoritmo genético executado acima. Após definida a arquitetura, a rede será treinada utilizando o conjunto de treino e para ao early stopping será utilizado o conjunto de validação. Logo após o conjunto de teste será avaliado utilizando as métricas exigidas pelo projeto."
      ]
    },
    {
      "metadata": {
        "id": "HSwvsb0wAVdI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip3 install scikit-plot\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "import scikitplot as skplt\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-2ntgaHAVdL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    X_train = np.load(DATASET_PATH + \"X_train.npy\")\n",
        "    y_train = np.load(DATASET_PATH + \"y_train.npy\")\n",
        "    X_val = np.load(DATASET_PATH + \"X_val.npy\")\n",
        "    y_val = np.load(DATASET_PATH + \"y_val.npy\")\n",
        "    X_test = np.load(DATASET_PATH + \"X_test.npy\")\n",
        "    y_test = np.load(DATASET_PATH + \"y_test.npy\")\n",
        "    \n",
        "    train_perm = np.random.permutation(X_train.shape[0])\n",
        "    X_train, y_train = X_train[train_perm], y_train[train_perm]\n",
        "    val_perm = np.random.permutation(X_val.shape[0])\n",
        "    X_val, y_val = X_val[val_perm], y_val[val_perm]\n",
        "    test_perm = np.random.permutation(X_test.shape[0])\n",
        "    X_test, y_test = X_test[test_perm], y_test[test_perm]\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9pxeymIhAVdU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(input_dim, output_dim, params):\n",
        "    n_layers = params[\"n_layers\"]\n",
        "    n_neurons = params[\"n_neurons\"]\n",
        "    dropout = params[\"dropout\"]\n",
        "    optimizer = params[\"optimizer\"]\n",
        "    activation = params[\"activation\"]\n",
        "    last_layer_activation = params[\"last_layer_activation\"]\n",
        "    loss = params[\"loss\"]\n",
        "    metrics = params[\"metrics\"]\n",
        "    \n",
        "    model = Sequential()\n",
        "    # Hidden layers\n",
        "    for n in range(n_layers):\n",
        "        if n == 0:\n",
        "            model.add(Dense(n_neurons[n], activation=activation, input_dim=input_dim))\n",
        "        else:\n",
        "            model.add(Dense(n_neurons[n], activation=activation))\n",
        "        if dropout != 0.0:\n",
        "            model.add(Dropout(dropout))\n",
        "    # Output layer\n",
        "    model.add(Dense(output_dim, activation=last_layer_activation))\n",
        "    \n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MQDYdtXBp40N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"n_layers\": 2,\n",
        "    \"n_neurons\": [256, 256],\n",
        "    \"dropout\": 0.25,\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"activation\": \"sigmoid\",\n",
        "    \"last_layer_activation\": \"sigmoid\",\n",
        "    \"loss\": \"binary_crossentropy\",\n",
        "    \"metrics\": [\"accuracy\"],\n",
        "}\n",
        "model = create_model(X_train.shape[1], 1, params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v7dGGRc7qNB4",
        "colab_type": "code",
        "outputId": "e33893d4-039e-461c-e5e2-15e4905b6e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=1024,\n",
        "                    epochs=10000,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[EarlyStopping(patience=5)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 255098 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "255098/255098 [==============================] - 3s 10us/step - loss: 0.6663 - acc: 0.5980 - val_loss: 0.6447 - val_acc: 0.6287\n",
            "Epoch 2/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6476 - acc: 0.6237 - val_loss: 0.6435 - val_acc: 0.6294\n",
            "Epoch 3/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6443 - acc: 0.6264 - val_loss: 0.6425 - val_acc: 0.6297\n",
            "Epoch 4/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6423 - acc: 0.6276 - val_loss: 0.6418 - val_acc: 0.6310\n",
            "Epoch 5/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6412 - acc: 0.6293 - val_loss: 0.6403 - val_acc: 0.6320\n",
            "Epoch 6/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6397 - acc: 0.6306 - val_loss: 0.6381 - val_acc: 0.6345\n",
            "Epoch 7/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6370 - acc: 0.6339 - val_loss: 0.6356 - val_acc: 0.6373\n",
            "Epoch 8/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6352 - acc: 0.6362 - val_loss: 0.6348 - val_acc: 0.6394\n",
            "Epoch 9/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6342 - acc: 0.6371 - val_loss: 0.6350 - val_acc: 0.6378\n",
            "Epoch 10/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6334 - acc: 0.6378 - val_loss: 0.6342 - val_acc: 0.6391\n",
            "Epoch 11/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6324 - acc: 0.6383 - val_loss: 0.6340 - val_acc: 0.6389\n",
            "Epoch 12/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6318 - acc: 0.6389 - val_loss: 0.6349 - val_acc: 0.6384\n",
            "Epoch 13/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6309 - acc: 0.6400 - val_loss: 0.6333 - val_acc: 0.6405\n",
            "Epoch 14/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6299 - acc: 0.6405 - val_loss: 0.6344 - val_acc: 0.6375\n",
            "Epoch 15/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6296 - acc: 0.6409 - val_loss: 0.6333 - val_acc: 0.6403\n",
            "Epoch 16/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6287 - acc: 0.6413 - val_loss: 0.6332 - val_acc: 0.6387\n",
            "Epoch 17/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6275 - acc: 0.6418 - val_loss: 0.6339 - val_acc: 0.6395\n",
            "Epoch 18/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6268 - acc: 0.6426 - val_loss: 0.6334 - val_acc: 0.6402\n",
            "Epoch 19/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6260 - acc: 0.6436 - val_loss: 0.6332 - val_acc: 0.6407\n",
            "Epoch 20/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6252 - acc: 0.6442 - val_loss: 0.6332 - val_acc: 0.6409\n",
            "Epoch 21/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6243 - acc: 0.6456 - val_loss: 0.6331 - val_acc: 0.6411\n",
            "Epoch 22/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6235 - acc: 0.6462 - val_loss: 0.6330 - val_acc: 0.6408\n",
            "Epoch 23/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6222 - acc: 0.6465 - val_loss: 0.6346 - val_acc: 0.6400\n",
            "Epoch 24/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6216 - acc: 0.6476 - val_loss: 0.6334 - val_acc: 0.6410\n",
            "Epoch 25/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6205 - acc: 0.6487 - val_loss: 0.6344 - val_acc: 0.6396\n",
            "Epoch 26/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6197 - acc: 0.6487 - val_loss: 0.6334 - val_acc: 0.6409\n",
            "Epoch 27/10000\n",
            "255098/255098 [==============================] - 2s 9us/step - loss: 0.6190 - acc: 0.6496 - val_loss: 0.6343 - val_acc: 0.6400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "arEeSG3msvo5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Predições no conjunto de teste"
      ]
    },
    {
      "metadata": {
        "id": "rdJvcUk9rJbo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_performance_metrics(y, y_pred_class, y_pred_scores=None):\n",
        "    accuracy = accuracy_score(y, y_pred_class)\n",
        "    recall = recall_score(y, y_pred_class)\n",
        "    precision = precision_score(y, y_pred_class)\n",
        "    f1 = f1_score(y, y_pred_class)\n",
        "    performance_metrics = (accuracy, recall, precision, f1)\n",
        "    if y_pred_scores is not None:\n",
        "        skplt.metrics.plot_ks_statistic(y, y_pred_scores)\n",
        "        plt.show()\n",
        "        y_pred_scores = y_pred_scores[:, 1]\n",
        "        auroc = roc_auc_score(y, y_pred_scores)\n",
        "        aupr = average_precision_score(y, y_pred_scores)\n",
        "        performance_metrics = performance_metrics + (auroc, aupr)\n",
        "    return performance_metrics\n",
        "\n",
        "def print_metrics_summary(accuracy, recall, precision, f1, auroc=None, aupr=None):\n",
        "    print()\n",
        "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Accuracy:\", value=accuracy))\n",
        "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Recall:\", value=recall))\n",
        "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Precision:\", value=precision))\n",
        "    print(\"{metric:<18}{value:.4f}\".format(metric=\"F1:\", value=f1))\n",
        "    if auroc is not None:\n",
        "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUROC:\", value=auroc))\n",
        "    if aupr is not None:\n",
        "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUPR:\", value=aupr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dr5kzA27r4xB",
        "colab_type": "code",
        "outputId": "c04ace86-54ea-4f96-b2d1-6314aba53481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "cell_type": "code",
      "source": [
        "# Fazer predições no conjunto de teste\n",
        "y_pred_scores = model.predict(X_test)\n",
        "y_pred_class = model.predict_classes(X_test, verbose=0)\n",
        "y_pred_scores_0 = 1 - y_pred_scores\n",
        "y_pred_scores = np.concatenate([y_pred_scores_0, y_pred_scores], axis=1)\n",
        "\n",
        "accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_test, y_pred_class, y_pred_scores)\n",
        "print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEVCAYAAAALsCk2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYTw8P8W1TmXfHrlTQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fc23b345668>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy:         0.6406\n",
            "Recall:           0.5846\n",
            "Precision:        0.6584\n",
            "F1:               0.6193\n",
            "AUROC:            0.6942\n",
            "AUPR:             0.6923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WD_gLWZFaPtn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ensemble de MLP's\n",
        "Nosso próximo passo será criar um ensemble de MLP's para avaliarmos, para isso utilizaremos o método bagging. Várias redes serão treinadas de forma independentes utilizando subconjuntos do conjunto original de treino,  onde esse subconjuntos podem se sobrepôr. Além disso, cada subconjunto contém suas próprias features, ou seja, cada rede é treinada utilizando um conjuno de features definido de forma aleatória. Isso permite que cada rede consiga se \"especializar\" em características diferentes do dataset. No momento de avaliação cada rede neural é submetida ao conjunto de teste e suas predições são obtidas para compor a predição final do ensemble, onde esta predição final é definida pela maioria dos votos das redes."
      ]
    },
    {
      "metadata": {
        "id": "8cCq4siwAVda",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_emsemble(n_models, instances_ratio, features_ratio, params):\n",
        "    models = []\n",
        "    features_idx = []\n",
        "    for i in range(n_models):\n",
        "        features = np.random.choice(range(X_train.shape[1]),\n",
        "                                    round(X_train.shape[1] * features_ratio))\n",
        "        \n",
        "        _, xi_train, _, yi_train = train_test_split(X_train,\n",
        "                                                    y_train,\n",
        "                                                    test_size=instances_ratio,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_train)\n",
        "        \n",
        "        xi_train = xi_train[:, features]\n",
        "        xi_val = X_val[:, features]\n",
        "    \n",
        "        model = create_model(xi_train.shape[1], 1, params)\n",
        "        print()\n",
        "        print(\"-----------------Training MLP {}-----------------\".format(i))\n",
        "        print()\n",
        "        model.fit(xi_train, yi_train,\n",
        "                  batch_size=1024,\n",
        "                  epochs=10000,\n",
        "                  validation_data=(xi_val, y_val),\n",
        "                  callbacks=[EarlyStopping(patience=5)],\n",
        "                  verbose=1)\n",
        "        model.save_weights(MODELS_PATH + \"model-{}.hdf5\".format(i), overwrite=True)\n",
        "        \n",
        "        models.append(model)\n",
        "        features_idx.append(features)\n",
        "        \n",
        "        # Avoiding memory issues\n",
        "        K.clear_session()\n",
        "        \n",
        "    np.save(MODELS_PATH + \"features.npy\", np.array(features_idx))\n",
        "        \n",
        "    return models, features_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orEfjs9EAVdg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_model(features_idx, params):\n",
        "    predictions = []\n",
        "    for i, f in enumerate(features_idx):\n",
        "        xi_test = X_test[:, f]\n",
        "        model = create_model(xi_test.shape[1], 1, params)\n",
        "        model.load_weights(MODELS_PATH + \"model-{}.hdf5\".format(i))\n",
        "        pred = model.predict(xi_test, batch_size=1024, verbose=1)\n",
        "        predictions.append(pred)\n",
        "        \n",
        "        K.clear_session()\n",
        "        \n",
        "    pred_mean = np.round(np.array(predictions)[:,:,0].mean(axis=0))\n",
        "    accuracy = pred_mean[pred_mean == y_test].shape[0] / X_test.shape[0]\n",
        "    \n",
        "    print(\"Accuray: {}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VeSggobSAVd4",
        "colab_type": "code",
        "outputId": "5e3db3b7-d827-494c-db81-3f06a2a4dd2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31123
        }
      },
      "cell_type": "code",
      "source": [
        "models, features = create_emsemble(20, 0.8, 0.5, params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------------Training MLP 0-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6912 - acc: 0.5504 - val_loss: 0.6748 - val_acc: 0.5807\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6761 - acc: 0.5739 - val_loss: 0.6718 - val_acc: 0.5822\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6720 - acc: 0.5792 - val_loss: 0.6705 - val_acc: 0.5827\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6705 - acc: 0.5800 - val_loss: 0.6692 - val_acc: 0.5844\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6693 - acc: 0.5823 - val_loss: 0.6685 - val_acc: 0.5846\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6685 - acc: 0.5840 - val_loss: 0.6679 - val_acc: 0.5859\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6670 - acc: 0.5868 - val_loss: 0.6667 - val_acc: 0.5895\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6657 - acc: 0.5892 - val_loss: 0.6656 - val_acc: 0.5931\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6643 - acc: 0.5944 - val_loss: 0.6643 - val_acc: 0.5974\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6621 - acc: 0.5993 - val_loss: 0.6631 - val_acc: 0.5989\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6609 - acc: 0.6003 - val_loss: 0.6610 - val_acc: 0.6044\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6594 - acc: 0.6050 - val_loss: 0.6599 - val_acc: 0.6063\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6583 - acc: 0.6070 - val_loss: 0.6593 - val_acc: 0.6066\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6572 - acc: 0.6087 - val_loss: 0.6589 - val_acc: 0.6064\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6563 - acc: 0.6102 - val_loss: 0.6584 - val_acc: 0.6082\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6558 - acc: 0.6101 - val_loss: 0.6582 - val_acc: 0.6089\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6549 - acc: 0.6116 - val_loss: 0.6574 - val_acc: 0.6097\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6539 - acc: 0.6128 - val_loss: 0.6574 - val_acc: 0.6085\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6530 - acc: 0.6146 - val_loss: 0.6568 - val_acc: 0.6096\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6523 - acc: 0.6158 - val_loss: 0.6570 - val_acc: 0.6105\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6518 - acc: 0.6160 - val_loss: 0.6564 - val_acc: 0.6112\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6509 - acc: 0.6183 - val_loss: 0.6565 - val_acc: 0.6104\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6505 - acc: 0.6177 - val_loss: 0.6560 - val_acc: 0.6119\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6498 - acc: 0.6189 - val_loss: 0.6561 - val_acc: 0.6105\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6491 - acc: 0.6194 - val_loss: 0.6560 - val_acc: 0.6121\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6484 - acc: 0.6200 - val_loss: 0.6558 - val_acc: 0.6111\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6475 - acc: 0.6211 - val_loss: 0.6558 - val_acc: 0.6125\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6475 - acc: 0.6211 - val_loss: 0.6561 - val_acc: 0.6129\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6462 - acc: 0.6228 - val_loss: 0.6554 - val_acc: 0.6140\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6455 - acc: 0.6231 - val_loss: 0.6551 - val_acc: 0.6134\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6449 - acc: 0.6248 - val_loss: 0.6558 - val_acc: 0.6135\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6443 - acc: 0.6250 - val_loss: 0.6557 - val_acc: 0.6124\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6439 - acc: 0.6259 - val_loss: 0.6551 - val_acc: 0.6132\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6432 - acc: 0.6266 - val_loss: 0.6555 - val_acc: 0.6128\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6423 - acc: 0.6272 - val_loss: 0.6555 - val_acc: 0.6153\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6417 - acc: 0.6285 - val_loss: 0.6551 - val_acc: 0.6129\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6411 - acc: 0.6283 - val_loss: 0.6550 - val_acc: 0.6145\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6403 - acc: 0.6289 - val_loss: 0.6553 - val_acc: 0.6135\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6390 - acc: 0.6311 - val_loss: 0.6558 - val_acc: 0.6132\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6385 - acc: 0.6319 - val_loss: 0.6556 - val_acc: 0.6147\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6375 - acc: 0.6332 - val_loss: 0.6553 - val_acc: 0.6145\n",
            "Epoch 42/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6376 - acc: 0.6328 - val_loss: 0.6558 - val_acc: 0.6137\n",
            "[20/Nov/2018 02:12:16] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc1a6c4e278>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 1-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6983 - acc: 0.5418 - val_loss: 0.6810 - val_acc: 0.5656\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6816 - acc: 0.5642 - val_loss: 0.6768 - val_acc: 0.5741\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6787 - acc: 0.5705 - val_loss: 0.6763 - val_acc: 0.5760\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6773 - acc: 0.5741 - val_loss: 0.6761 - val_acc: 0.5768\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6765 - acc: 0.5744 - val_loss: 0.6755 - val_acc: 0.5753\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6755 - acc: 0.5767 - val_loss: 0.6748 - val_acc: 0.5785\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6745 - acc: 0.5785 - val_loss: 0.6741 - val_acc: 0.5785\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6733 - acc: 0.5816 - val_loss: 0.6724 - val_acc: 0.5841\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6715 - acc: 0.5862 - val_loss: 0.6713 - val_acc: 0.5856\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6701 - acc: 0.5882 - val_loss: 0.6702 - val_acc: 0.5871\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6688 - acc: 0.5907 - val_loss: 0.6691 - val_acc: 0.5881\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6671 - acc: 0.5936 - val_loss: 0.6679 - val_acc: 0.5911\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6656 - acc: 0.5963 - val_loss: 0.6667 - val_acc: 0.5935\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6640 - acc: 0.5984 - val_loss: 0.6655 - val_acc: 0.5946\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6632 - acc: 0.6001 - val_loss: 0.6654 - val_acc: 0.5958\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6618 - acc: 0.6016 - val_loss: 0.6644 - val_acc: 0.5974\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6610 - acc: 0.6033 - val_loss: 0.6633 - val_acc: 0.5979\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6601 - acc: 0.6048 - val_loss: 0.6629 - val_acc: 0.5995\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6591 - acc: 0.6059 - val_loss: 0.6623 - val_acc: 0.5985\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6581 - acc: 0.6072 - val_loss: 0.6618 - val_acc: 0.6017\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6569 - acc: 0.6081 - val_loss: 0.6610 - val_acc: 0.6023\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6559 - acc: 0.6100 - val_loss: 0.6607 - val_acc: 0.6035\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6553 - acc: 0.6104 - val_loss: 0.6608 - val_acc: 0.6029\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6545 - acc: 0.6126 - val_loss: 0.6596 - val_acc: 0.6051\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6538 - acc: 0.6127 - val_loss: 0.6597 - val_acc: 0.6046\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6526 - acc: 0.6145 - val_loss: 0.6592 - val_acc: 0.6057\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6521 - acc: 0.6145 - val_loss: 0.6597 - val_acc: 0.6051\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6509 - acc: 0.6160 - val_loss: 0.6591 - val_acc: 0.6063\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6500 - acc: 0.6167 - val_loss: 0.6586 - val_acc: 0.6068\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6496 - acc: 0.6180 - val_loss: 0.6587 - val_acc: 0.6066\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6486 - acc: 0.6200 - val_loss: 0.6581 - val_acc: 0.6087\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6477 - acc: 0.6198 - val_loss: 0.6581 - val_acc: 0.6078\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6463 - acc: 0.6228 - val_loss: 0.6583 - val_acc: 0.6075\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6460 - acc: 0.6242 - val_loss: 0.6581 - val_acc: 0.6078\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6450 - acc: 0.6233 - val_loss: 0.6577 - val_acc: 0.6080\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6441 - acc: 0.6248 - val_loss: 0.6577 - val_acc: 0.6085\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6432 - acc: 0.6258 - val_loss: 0.6576 - val_acc: 0.6098\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6421 - acc: 0.6261 - val_loss: 0.6585 - val_acc: 0.6086\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6415 - acc: 0.6278 - val_loss: 0.6577 - val_acc: 0.6093\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6405 - acc: 0.6278 - val_loss: 0.6577 - val_acc: 0.6098\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6395 - acc: 0.6292 - val_loss: 0.6583 - val_acc: 0.6093\n",
            "Epoch 42/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6389 - acc: 0.6311 - val_loss: 0.6572 - val_acc: 0.6098\n",
            "Epoch 43/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6378 - acc: 0.6317 - val_loss: 0.6577 - val_acc: 0.6109\n",
            "Epoch 44/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6367 - acc: 0.6339 - val_loss: 0.6578 - val_acc: 0.6109\n",
            "Epoch 45/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6365 - acc: 0.6330 - val_loss: 0.6584 - val_acc: 0.6092\n",
            "Epoch 46/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6351 - acc: 0.6352 - val_loss: 0.6580 - val_acc: 0.6111\n",
            "Epoch 47/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6345 - acc: 0.6365 - val_loss: 0.6586 - val_acc: 0.6098\n",
            "[20/Nov/2018 02:13:52] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc1a3d662e8>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 2-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6824 - acc: 0.5676 - val_loss: 0.6677 - val_acc: 0.5961\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6688 - acc: 0.5922 - val_loss: 0.6668 - val_acc: 0.5963\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6661 - acc: 0.5953 - val_loss: 0.6652 - val_acc: 0.5969\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6646 - acc: 0.5970 - val_loss: 0.6645 - val_acc: 0.5981\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6634 - acc: 0.5989 - val_loss: 0.6631 - val_acc: 0.6005\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6623 - acc: 0.6011 - val_loss: 0.6621 - val_acc: 0.6020\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6613 - acc: 0.6022 - val_loss: 0.6617 - val_acc: 0.6027\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6604 - acc: 0.6040 - val_loss: 0.6606 - val_acc: 0.6043\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6593 - acc: 0.6061 - val_loss: 0.6604 - val_acc: 0.6046\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6586 - acc: 0.6070 - val_loss: 0.6591 - val_acc: 0.6077\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6578 - acc: 0.6087 - val_loss: 0.6589 - val_acc: 0.6079\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6569 - acc: 0.6101 - val_loss: 0.6581 - val_acc: 0.6100\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6563 - acc: 0.6102 - val_loss: 0.6578 - val_acc: 0.6104\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6556 - acc: 0.6117 - val_loss: 0.6573 - val_acc: 0.6107\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6548 - acc: 0.6124 - val_loss: 0.6573 - val_acc: 0.6111\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6540 - acc: 0.6133 - val_loss: 0.6572 - val_acc: 0.6118\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6535 - acc: 0.6146 - val_loss: 0.6572 - val_acc: 0.6117\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6523 - acc: 0.6164 - val_loss: 0.6560 - val_acc: 0.6133\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6518 - acc: 0.6160 - val_loss: 0.6559 - val_acc: 0.6136\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6508 - acc: 0.6174 - val_loss: 0.6556 - val_acc: 0.6147\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6504 - acc: 0.6178 - val_loss: 0.6548 - val_acc: 0.6156\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6496 - acc: 0.6190 - val_loss: 0.6552 - val_acc: 0.6151\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6487 - acc: 0.6196 - val_loss: 0.6546 - val_acc: 0.6155\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6479 - acc: 0.6208 - val_loss: 0.6543 - val_acc: 0.6167\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6474 - acc: 0.6228 - val_loss: 0.6544 - val_acc: 0.6155\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6468 - acc: 0.6226 - val_loss: 0.6543 - val_acc: 0.6159\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6456 - acc: 0.6244 - val_loss: 0.6540 - val_acc: 0.6172\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6449 - acc: 0.6238 - val_loss: 0.6536 - val_acc: 0.6179\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6443 - acc: 0.6246 - val_loss: 0.6536 - val_acc: 0.6178\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6436 - acc: 0.6250 - val_loss: 0.6535 - val_acc: 0.6163\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6428 - acc: 0.6262 - val_loss: 0.6535 - val_acc: 0.6187\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6417 - acc: 0.6279 - val_loss: 0.6538 - val_acc: 0.6187\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6413 - acc: 0.6288 - val_loss: 0.6533 - val_acc: 0.6192\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6406 - acc: 0.6297 - val_loss: 0.6528 - val_acc: 0.6196\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6395 - acc: 0.6302 - val_loss: 0.6531 - val_acc: 0.6185\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6392 - acc: 0.6306 - val_loss: 0.6531 - val_acc: 0.6181\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6379 - acc: 0.6325 - val_loss: 0.6531 - val_acc: 0.6187\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6368 - acc: 0.6333 - val_loss: 0.6529 - val_acc: 0.6190\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6357 - acc: 0.6335 - val_loss: 0.6526 - val_acc: 0.6200\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6354 - acc: 0.6345 - val_loss: 0.6532 - val_acc: 0.6191\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6346 - acc: 0.6361 - val_loss: 0.6526 - val_acc: 0.6195\n",
            "Epoch 42/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6340 - acc: 0.6355 - val_loss: 0.6529 - val_acc: 0.6192\n",
            "Epoch 43/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6326 - acc: 0.6370 - val_loss: 0.6526 - val_acc: 0.6194\n",
            "Epoch 44/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6323 - acc: 0.6379 - val_loss: 0.6530 - val_acc: 0.6198\n",
            "[20/Nov/2018 02:15:24] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc1a0edf438>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 3-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6932 - acc: 0.5541 - val_loss: 0.6724 - val_acc: 0.5851\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6761 - acc: 0.5774 - val_loss: 0.6719 - val_acc: 0.5853\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6733 - acc: 0.5829 - val_loss: 0.6714 - val_acc: 0.5867\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6723 - acc: 0.5830 - val_loss: 0.6710 - val_acc: 0.5860\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6713 - acc: 0.5850 - val_loss: 0.6707 - val_acc: 0.5868\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6702 - acc: 0.5860 - val_loss: 0.6700 - val_acc: 0.5873\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6693 - acc: 0.5861 - val_loss: 0.6689 - val_acc: 0.5891\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6680 - acc: 0.5898 - val_loss: 0.6669 - val_acc: 0.5926\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6659 - acc: 0.5935 - val_loss: 0.6651 - val_acc: 0.5959\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6636 - acc: 0.5985 - val_loss: 0.6626 - val_acc: 0.6010\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6619 - acc: 0.6016 - val_loss: 0.6640 - val_acc: 0.6008\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6605 - acc: 0.6032 - val_loss: 0.6598 - val_acc: 0.6074\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6596 - acc: 0.6049 - val_loss: 0.6597 - val_acc: 0.6074\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6581 - acc: 0.6067 - val_loss: 0.6580 - val_acc: 0.6109\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6572 - acc: 0.6089 - val_loss: 0.6568 - val_acc: 0.6127\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6557 - acc: 0.6107 - val_loss: 0.6560 - val_acc: 0.6139\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6550 - acc: 0.6128 - val_loss: 0.6549 - val_acc: 0.6155\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6539 - acc: 0.6146 - val_loss: 0.6554 - val_acc: 0.6139\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6525 - acc: 0.6159 - val_loss: 0.6538 - val_acc: 0.6167\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6522 - acc: 0.6162 - val_loss: 0.6531 - val_acc: 0.6176\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6510 - acc: 0.6175 - val_loss: 0.6527 - val_acc: 0.6197\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6500 - acc: 0.6191 - val_loss: 0.6521 - val_acc: 0.6195\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6491 - acc: 0.6211 - val_loss: 0.6523 - val_acc: 0.6204\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6481 - acc: 0.6213 - val_loss: 0.6519 - val_acc: 0.6175\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6471 - acc: 0.6228 - val_loss: 0.6516 - val_acc: 0.6209\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6468 - acc: 0.6225 - val_loss: 0.6513 - val_acc: 0.6203\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6457 - acc: 0.6249 - val_loss: 0.6511 - val_acc: 0.6197\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6453 - acc: 0.6250 - val_loss: 0.6508 - val_acc: 0.6213\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6436 - acc: 0.6263 - val_loss: 0.6508 - val_acc: 0.6204\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6432 - acc: 0.6276 - val_loss: 0.6505 - val_acc: 0.6212\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6424 - acc: 0.6280 - val_loss: 0.6509 - val_acc: 0.6202\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6421 - acc: 0.6288 - val_loss: 0.6508 - val_acc: 0.6201\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6411 - acc: 0.6290 - val_loss: 0.6506 - val_acc: 0.6212\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6402 - acc: 0.6312 - val_loss: 0.6504 - val_acc: 0.6222\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6394 - acc: 0.6315 - val_loss: 0.6509 - val_acc: 0.6215\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6382 - acc: 0.6326 - val_loss: 0.6512 - val_acc: 0.6205\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6372 - acc: 0.6339 - val_loss: 0.6506 - val_acc: 0.6216\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6367 - acc: 0.6346 - val_loss: 0.6504 - val_acc: 0.6220\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6354 - acc: 0.6359 - val_loss: 0.6506 - val_acc: 0.6211\n",
            "[20/Nov/2018 02:16:44] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc19e07f2b0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 4-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 12us/step - loss: 0.6840 - acc: 0.5661 - val_loss: 0.6681 - val_acc: 0.5936\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6703 - acc: 0.5893 - val_loss: 0.6670 - val_acc: 0.5953\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6676 - acc: 0.5944 - val_loss: 0.6679 - val_acc: 0.5909\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6664 - acc: 0.5963 - val_loss: 0.6662 - val_acc: 0.5950\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6659 - acc: 0.5972 - val_loss: 0.6662 - val_acc: 0.5955\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6656 - acc: 0.5982 - val_loss: 0.6662 - val_acc: 0.5958\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6649 - acc: 0.5987 - val_loss: 0.6653 - val_acc: 0.5954\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6643 - acc: 0.5993 - val_loss: 0.6654 - val_acc: 0.5983\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6634 - acc: 0.6010 - val_loss: 0.6635 - val_acc: 0.6005\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6623 - acc: 0.6024 - val_loss: 0.6623 - val_acc: 0.6019\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6613 - acc: 0.6040 - val_loss: 0.6617 - val_acc: 0.6033\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6602 - acc: 0.6050 - val_loss: 0.6611 - val_acc: 0.6054\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6597 - acc: 0.6063 - val_loss: 0.6607 - val_acc: 0.6061\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6589 - acc: 0.6075 - val_loss: 0.6601 - val_acc: 0.6062\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6580 - acc: 0.6082 - val_loss: 0.6596 - val_acc: 0.6063\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6574 - acc: 0.6099 - val_loss: 0.6594 - val_acc: 0.6090\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6564 - acc: 0.6107 - val_loss: 0.6588 - val_acc: 0.6087\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6562 - acc: 0.6116 - val_loss: 0.6586 - val_acc: 0.6091\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6555 - acc: 0.6120 - val_loss: 0.6582 - val_acc: 0.6093\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6550 - acc: 0.6122 - val_loss: 0.6582 - val_acc: 0.6090\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6543 - acc: 0.6132 - val_loss: 0.6578 - val_acc: 0.6091\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6537 - acc: 0.6146 - val_loss: 0.6581 - val_acc: 0.6100\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6532 - acc: 0.6148 - val_loss: 0.6575 - val_acc: 0.6103\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6526 - acc: 0.6160 - val_loss: 0.6575 - val_acc: 0.6102\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6523 - acc: 0.6157 - val_loss: 0.6572 - val_acc: 0.6104\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6515 - acc: 0.6174 - val_loss: 0.6571 - val_acc: 0.6104\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6510 - acc: 0.6173 - val_loss: 0.6569 - val_acc: 0.6105\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6503 - acc: 0.6178 - val_loss: 0.6572 - val_acc: 0.6120\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6498 - acc: 0.6187 - val_loss: 0.6566 - val_acc: 0.6110\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6494 - acc: 0.6181 - val_loss: 0.6564 - val_acc: 0.6129\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6487 - acc: 0.6194 - val_loss: 0.6568 - val_acc: 0.6112\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6481 - acc: 0.6197 - val_loss: 0.6563 - val_acc: 0.6115\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6479 - acc: 0.6211 - val_loss: 0.6560 - val_acc: 0.6125\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6468 - acc: 0.6217 - val_loss: 0.6563 - val_acc: 0.6116\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6463 - acc: 0.6225 - val_loss: 0.6564 - val_acc: 0.6113\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6456 - acc: 0.6231 - val_loss: 0.6567 - val_acc: 0.6122\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6450 - acc: 0.6238 - val_loss: 0.6564 - val_acc: 0.6124\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6442 - acc: 0.6252 - val_loss: 0.6557 - val_acc: 0.6122\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6434 - acc: 0.6255 - val_loss: 0.6566 - val_acc: 0.6113\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6430 - acc: 0.6254 - val_loss: 0.6559 - val_acc: 0.6128\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6422 - acc: 0.6272 - val_loss: 0.6559 - val_acc: 0.6139\n",
            "Epoch 42/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6412 - acc: 0.6279 - val_loss: 0.6564 - val_acc: 0.6136\n",
            "Epoch 43/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6406 - acc: 0.6287 - val_loss: 0.6567 - val_acc: 0.6121\n",
            "[20/Nov/2018 02:18:15] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc1a9b63400>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 5-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6876 - acc: 0.5550 - val_loss: 0.6741 - val_acc: 0.5801\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6764 - acc: 0.5737 - val_loss: 0.6722 - val_acc: 0.5819\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6731 - acc: 0.5772 - val_loss: 0.6706 - val_acc: 0.5848\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6711 - acc: 0.5799 - val_loss: 0.6693 - val_acc: 0.5881\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6690 - acc: 0.5843 - val_loss: 0.6671 - val_acc: 0.5910\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6669 - acc: 0.5885 - val_loss: 0.6654 - val_acc: 0.5954\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6654 - acc: 0.5932 - val_loss: 0.6637 - val_acc: 0.5982\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6636 - acc: 0.5963 - val_loss: 0.6625 - val_acc: 0.6008\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6625 - acc: 0.5982 - val_loss: 0.6619 - val_acc: 0.6026\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6604 - acc: 0.6029 - val_loss: 0.6603 - val_acc: 0.6050\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6594 - acc: 0.6045 - val_loss: 0.6592 - val_acc: 0.6067\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6574 - acc: 0.6080 - val_loss: 0.6575 - val_acc: 0.6093\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6564 - acc: 0.6094 - val_loss: 0.6567 - val_acc: 0.6118\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6550 - acc: 0.6113 - val_loss: 0.6562 - val_acc: 0.6122\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6542 - acc: 0.6125 - val_loss: 0.6562 - val_acc: 0.6113\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6530 - acc: 0.6145 - val_loss: 0.6554 - val_acc: 0.6122\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6520 - acc: 0.6153 - val_loss: 0.6548 - val_acc: 0.6131\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6513 - acc: 0.6169 - val_loss: 0.6547 - val_acc: 0.6124\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6506 - acc: 0.6180 - val_loss: 0.6543 - val_acc: 0.6132\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6500 - acc: 0.6184 - val_loss: 0.6545 - val_acc: 0.6136\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6489 - acc: 0.6199 - val_loss: 0.6542 - val_acc: 0.6136\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6482 - acc: 0.6208 - val_loss: 0.6537 - val_acc: 0.6128\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6471 - acc: 0.6230 - val_loss: 0.6536 - val_acc: 0.6154\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6464 - acc: 0.6235 - val_loss: 0.6539 - val_acc: 0.6141\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6455 - acc: 0.6252 - val_loss: 0.6538 - val_acc: 0.6143\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6453 - acc: 0.6241 - val_loss: 0.6534 - val_acc: 0.6140\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6440 - acc: 0.6268 - val_loss: 0.6536 - val_acc: 0.6141\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6436 - acc: 0.6274 - val_loss: 0.6532 - val_acc: 0.6149\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6427 - acc: 0.6283 - val_loss: 0.6533 - val_acc: 0.6149\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6416 - acc: 0.6294 - val_loss: 0.6537 - val_acc: 0.6144\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6416 - acc: 0.6289 - val_loss: 0.6535 - val_acc: 0.6154\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6402 - acc: 0.6302 - val_loss: 0.6535 - val_acc: 0.6152\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.6307 - val_loss: 0.6533 - val_acc: 0.6144\n",
            "[20/Nov/2018 02:19:23] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc19837a080>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 6-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6800 - acc: 0.5713 - val_loss: 0.6661 - val_acc: 0.5968\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6656 - acc: 0.5948 - val_loss: 0.6636 - val_acc: 0.5984\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6616 - acc: 0.6001 - val_loss: 0.6619 - val_acc: 0.5993\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6602 - acc: 0.6017 - val_loss: 0.6609 - val_acc: 0.6013\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6588 - acc: 0.6034 - val_loss: 0.6595 - val_acc: 0.6030\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6574 - acc: 0.6056 - val_loss: 0.6586 - val_acc: 0.6059\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6561 - acc: 0.6081 - val_loss: 0.6572 - val_acc: 0.6094\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6545 - acc: 0.6114 - val_loss: 0.6560 - val_acc: 0.6118\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6533 - acc: 0.6130 - val_loss: 0.6551 - val_acc: 0.6121\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6524 - acc: 0.6146 - val_loss: 0.6543 - val_acc: 0.6137\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6512 - acc: 0.6158 - val_loss: 0.6538 - val_acc: 0.6125\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6504 - acc: 0.6171 - val_loss: 0.6535 - val_acc: 0.6136\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6499 - acc: 0.6183 - val_loss: 0.6529 - val_acc: 0.6146\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6485 - acc: 0.6189 - val_loss: 0.6524 - val_acc: 0.6138\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6481 - acc: 0.6205 - val_loss: 0.6522 - val_acc: 0.6159\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6477 - acc: 0.6209 - val_loss: 0.6517 - val_acc: 0.6163\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6469 - acc: 0.6218 - val_loss: 0.6512 - val_acc: 0.6174\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6458 - acc: 0.6235 - val_loss: 0.6510 - val_acc: 0.6177\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6452 - acc: 0.6241 - val_loss: 0.6508 - val_acc: 0.6173\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6446 - acc: 0.6246 - val_loss: 0.6508 - val_acc: 0.6176\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6438 - acc: 0.6252 - val_loss: 0.6508 - val_acc: 0.6172\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6425 - acc: 0.6268 - val_loss: 0.6505 - val_acc: 0.6178\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6424 - acc: 0.6270 - val_loss: 0.6500 - val_acc: 0.6176\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6414 - acc: 0.6279 - val_loss: 0.6505 - val_acc: 0.6179\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6408 - acc: 0.6284 - val_loss: 0.6503 - val_acc: 0.6175\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6403 - acc: 0.6289 - val_loss: 0.6501 - val_acc: 0.6181\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6398 - acc: 0.6294 - val_loss: 0.6498 - val_acc: 0.6185\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6391 - acc: 0.6302 - val_loss: 0.6496 - val_acc: 0.6196\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6382 - acc: 0.6324 - val_loss: 0.6498 - val_acc: 0.6190\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6376 - acc: 0.6320 - val_loss: 0.6493 - val_acc: 0.6205\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6367 - acc: 0.6324 - val_loss: 0.6492 - val_acc: 0.6201\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6366 - acc: 0.6331 - val_loss: 0.6494 - val_acc: 0.6203\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6355 - acc: 0.6344 - val_loss: 0.6493 - val_acc: 0.6205\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6345 - acc: 0.6352 - val_loss: 0.6494 - val_acc: 0.6205\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6338 - acc: 0.6357 - val_loss: 0.6493 - val_acc: 0.6211\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6332 - acc: 0.6362 - val_loss: 0.6492 - val_acc: 0.6213\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6324 - acc: 0.6375 - val_loss: 0.6493 - val_acc: 0.6205\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6318 - acc: 0.6377 - val_loss: 0.6496 - val_acc: 0.6208\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6311 - acc: 0.6383 - val_loss: 0.6492 - val_acc: 0.6217\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6307 - acc: 0.6393 - val_loss: 0.6494 - val_acc: 0.6224\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6298 - acc: 0.6396 - val_loss: 0.6496 - val_acc: 0.6220\n",
            "[20/Nov/2018 02:20:48] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc1954f2f98>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 7-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 12us/step - loss: 0.6912 - acc: 0.5480 - val_loss: 0.6800 - val_acc: 0.5701\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6797 - acc: 0.5684 - val_loss: 0.6784 - val_acc: 0.5740\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6777 - acc: 0.5735 - val_loss: 0.6777 - val_acc: 0.5744\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6769 - acc: 0.5729 - val_loss: 0.6777 - val_acc: 0.5735\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6762 - acc: 0.5750 - val_loss: 0.6772 - val_acc: 0.5745\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6760 - acc: 0.5754 - val_loss: 0.6769 - val_acc: 0.5756\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6756 - acc: 0.5768 - val_loss: 0.6772 - val_acc: 0.5735\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6749 - acc: 0.5778 - val_loss: 0.6759 - val_acc: 0.5775\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6741 - acc: 0.5787 - val_loss: 0.6752 - val_acc: 0.5777\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6732 - acc: 0.5819 - val_loss: 0.6737 - val_acc: 0.5817\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6721 - acc: 0.5831 - val_loss: 0.6730 - val_acc: 0.5834\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6712 - acc: 0.5842 - val_loss: 0.6720 - val_acc: 0.5852\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6699 - acc: 0.5873 - val_loss: 0.6712 - val_acc: 0.5874\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6692 - acc: 0.5895 - val_loss: 0.6704 - val_acc: 0.5887\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6684 - acc: 0.5897 - val_loss: 0.6698 - val_acc: 0.5895\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6674 - acc: 0.5922 - val_loss: 0.6695 - val_acc: 0.5904\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6667 - acc: 0.5926 - val_loss: 0.6689 - val_acc: 0.5914\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6656 - acc: 0.5942 - val_loss: 0.6681 - val_acc: 0.5929\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6649 - acc: 0.5962 - val_loss: 0.6684 - val_acc: 0.5917\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6639 - acc: 0.5968 - val_loss: 0.6682 - val_acc: 0.5932\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6630 - acc: 0.5982 - val_loss: 0.6673 - val_acc: 0.5956\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6624 - acc: 0.6002 - val_loss: 0.6672 - val_acc: 0.5953\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6614 - acc: 0.6011 - val_loss: 0.6668 - val_acc: 0.5957\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6608 - acc: 0.6018 - val_loss: 0.6662 - val_acc: 0.5964\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6600 - acc: 0.6032 - val_loss: 0.6661 - val_acc: 0.5970\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6590 - acc: 0.6044 - val_loss: 0.6660 - val_acc: 0.5965\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6584 - acc: 0.6057 - val_loss: 0.6659 - val_acc: 0.5973\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6578 - acc: 0.6060 - val_loss: 0.6657 - val_acc: 0.5980\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6570 - acc: 0.6070 - val_loss: 0.6655 - val_acc: 0.5978\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6559 - acc: 0.6093 - val_loss: 0.6651 - val_acc: 0.5990\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6551 - acc: 0.6093 - val_loss: 0.6653 - val_acc: 0.5990\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6544 - acc: 0.6102 - val_loss: 0.6652 - val_acc: 0.5990\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6530 - acc: 0.6122 - val_loss: 0.6655 - val_acc: 0.5983\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6529 - acc: 0.6120 - val_loss: 0.6651 - val_acc: 0.5994\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6524 - acc: 0.6127 - val_loss: 0.6651 - val_acc: 0.5994\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6512 - acc: 0.6141 - val_loss: 0.6648 - val_acc: 0.6003\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6501 - acc: 0.6156 - val_loss: 0.6651 - val_acc: 0.6000\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6497 - acc: 0.6159 - val_loss: 0.6658 - val_acc: 0.5992\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6483 - acc: 0.6173 - val_loss: 0.6652 - val_acc: 0.5999\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6479 - acc: 0.6191 - val_loss: 0.6649 - val_acc: 0.6006\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6467 - acc: 0.6195 - val_loss: 0.6650 - val_acc: 0.6004\n",
            "[20/Nov/2018 02:22:12] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc192604048>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 8-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 12us/step - loss: 0.6870 - acc: 0.5614 - val_loss: 0.6711 - val_acc: 0.5855\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6725 - acc: 0.5872 - val_loss: 0.6684 - val_acc: 0.5946\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6699 - acc: 0.5910 - val_loss: 0.6679 - val_acc: 0.5937\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6685 - acc: 0.5933 - val_loss: 0.6665 - val_acc: 0.5969\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6675 - acc: 0.5945 - val_loss: 0.6660 - val_acc: 0.5984\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6665 - acc: 0.5957 - val_loss: 0.6655 - val_acc: 0.5984\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6655 - acc: 0.5972 - val_loss: 0.6641 - val_acc: 0.5999\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6645 - acc: 0.5988 - val_loss: 0.6636 - val_acc: 0.6003\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6630 - acc: 0.6017 - val_loss: 0.6617 - val_acc: 0.6030\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6620 - acc: 0.6027 - val_loss: 0.6606 - val_acc: 0.6052\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6606 - acc: 0.6045 - val_loss: 0.6600 - val_acc: 0.6061\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6598 - acc: 0.6069 - val_loss: 0.6598 - val_acc: 0.6050\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6589 - acc: 0.6072 - val_loss: 0.6584 - val_acc: 0.6071\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6580 - acc: 0.6091 - val_loss: 0.6581 - val_acc: 0.6083\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6569 - acc: 0.6101 - val_loss: 0.6578 - val_acc: 0.6079\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6564 - acc: 0.6107 - val_loss: 0.6572 - val_acc: 0.6101\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6554 - acc: 0.6114 - val_loss: 0.6566 - val_acc: 0.6099\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6552 - acc: 0.6121 - val_loss: 0.6567 - val_acc: 0.6112\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6540 - acc: 0.6141 - val_loss: 0.6581 - val_acc: 0.6084\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6534 - acc: 0.6148 - val_loss: 0.6561 - val_acc: 0.6117\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6528 - acc: 0.6146 - val_loss: 0.6560 - val_acc: 0.6120\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6517 - acc: 0.6173 - val_loss: 0.6558 - val_acc: 0.6125\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6509 - acc: 0.6167 - val_loss: 0.6556 - val_acc: 0.6119\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6505 - acc: 0.6180 - val_loss: 0.6558 - val_acc: 0.6123\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6496 - acc: 0.6186 - val_loss: 0.6544 - val_acc: 0.6137\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6488 - acc: 0.6204 - val_loss: 0.6546 - val_acc: 0.6136\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6481 - acc: 0.6208 - val_loss: 0.6546 - val_acc: 0.6145\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6472 - acc: 0.6216 - val_loss: 0.6546 - val_acc: 0.6153\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6463 - acc: 0.6236 - val_loss: 0.6539 - val_acc: 0.6143\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6463 - acc: 0.6234 - val_loss: 0.6538 - val_acc: 0.6154\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6451 - acc: 0.6246 - val_loss: 0.6540 - val_acc: 0.6160\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6443 - acc: 0.6252 - val_loss: 0.6536 - val_acc: 0.6161\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6437 - acc: 0.6257 - val_loss: 0.6546 - val_acc: 0.6160\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6425 - acc: 0.6281 - val_loss: 0.6540 - val_acc: 0.6158\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6421 - acc: 0.6284 - val_loss: 0.6541 - val_acc: 0.6166\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6410 - acc: 0.6291 - val_loss: 0.6542 - val_acc: 0.6149\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.6299 - val_loss: 0.6535 - val_acc: 0.6175\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6388 - acc: 0.6318 - val_loss: 0.6533 - val_acc: 0.6170\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6387 - acc: 0.6323 - val_loss: 0.6542 - val_acc: 0.6159\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6378 - acc: 0.6329 - val_loss: 0.6543 - val_acc: 0.6167\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6368 - acc: 0.6341 - val_loss: 0.6534 - val_acc: 0.6169\n",
            "Epoch 42/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6365 - acc: 0.6337 - val_loss: 0.6535 - val_acc: 0.6173\n",
            "Epoch 43/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6350 - acc: 0.6361 - val_loss: 0.6534 - val_acc: 0.6177\n",
            "[20/Nov/2018 02:23:41] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc18f78bb38>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 9-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6901 - acc: 0.5484 - val_loss: 0.6759 - val_acc: 0.5811\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6783 - acc: 0.5720 - val_loss: 0.6748 - val_acc: 0.5827\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6764 - acc: 0.5758 - val_loss: 0.6743 - val_acc: 0.5810\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6752 - acc: 0.5783 - val_loss: 0.6733 - val_acc: 0.5837\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6742 - acc: 0.5788 - val_loss: 0.6721 - val_acc: 0.5847\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6734 - acc: 0.5814 - val_loss: 0.6713 - val_acc: 0.5849\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6721 - acc: 0.5829 - val_loss: 0.6696 - val_acc: 0.5882\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6710 - acc: 0.5843 - val_loss: 0.6690 - val_acc: 0.5902\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6700 - acc: 0.5863 - val_loss: 0.6681 - val_acc: 0.5910\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6692 - acc: 0.5873 - val_loss: 0.6677 - val_acc: 0.5924\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6684 - acc: 0.5891 - val_loss: 0.6670 - val_acc: 0.5929\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6677 - acc: 0.5905 - val_loss: 0.6670 - val_acc: 0.5945\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6668 - acc: 0.5933 - val_loss: 0.6661 - val_acc: 0.5959\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6660 - acc: 0.5948 - val_loss: 0.6657 - val_acc: 0.5954\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6647 - acc: 0.5960 - val_loss: 0.6647 - val_acc: 0.5979\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6635 - acc: 0.5988 - val_loss: 0.6640 - val_acc: 0.5988\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6629 - acc: 0.5992 - val_loss: 0.6634 - val_acc: 0.6003\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6621 - acc: 0.6006 - val_loss: 0.6631 - val_acc: 0.6013\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6610 - acc: 0.6020 - val_loss: 0.6630 - val_acc: 0.6020\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6601 - acc: 0.6034 - val_loss: 0.6632 - val_acc: 0.6017\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6595 - acc: 0.6042 - val_loss: 0.6619 - val_acc: 0.6038\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6585 - acc: 0.6055 - val_loss: 0.6616 - val_acc: 0.6046\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6578 - acc: 0.6077 - val_loss: 0.6614 - val_acc: 0.6048\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6568 - acc: 0.6079 - val_loss: 0.6610 - val_acc: 0.6054\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6561 - acc: 0.6092 - val_loss: 0.6607 - val_acc: 0.6056\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6547 - acc: 0.6114 - val_loss: 0.6608 - val_acc: 0.6049\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6542 - acc: 0.6133 - val_loss: 0.6604 - val_acc: 0.6063\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6532 - acc: 0.6130 - val_loss: 0.6605 - val_acc: 0.6067\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6528 - acc: 0.6148 - val_loss: 0.6603 - val_acc: 0.6065\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6520 - acc: 0.6142 - val_loss: 0.6599 - val_acc: 0.6072\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6506 - acc: 0.6169 - val_loss: 0.6597 - val_acc: 0.6083\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6493 - acc: 0.6191 - val_loss: 0.6599 - val_acc: 0.6076\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6490 - acc: 0.6189 - val_loss: 0.6597 - val_acc: 0.6088\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6481 - acc: 0.6205 - val_loss: 0.6597 - val_acc: 0.6091\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6471 - acc: 0.6214 - val_loss: 0.6596 - val_acc: 0.6092\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6464 - acc: 0.6213 - val_loss: 0.6600 - val_acc: 0.6081\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6455 - acc: 0.6237 - val_loss: 0.6597 - val_acc: 0.6104\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6445 - acc: 0.6245 - val_loss: 0.6597 - val_acc: 0.6097\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6437 - acc: 0.6244 - val_loss: 0.6597 - val_acc: 0.6089\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6433 - acc: 0.6257 - val_loss: 0.6595 - val_acc: 0.6098\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6418 - acc: 0.6270 - val_loss: 0.6598 - val_acc: 0.6105\n",
            "Epoch 42/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6414 - acc: 0.6274 - val_loss: 0.6604 - val_acc: 0.6081\n",
            "Epoch 43/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.6290 - val_loss: 0.6596 - val_acc: 0.6105\n",
            "Epoch 44/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6390 - acc: 0.6309 - val_loss: 0.6602 - val_acc: 0.6097\n",
            "Epoch 45/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6381 - acc: 0.6306 - val_loss: 0.6597 - val_acc: 0.6113\n",
            "[20/Nov/2018 02:25:14] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc18c92b080>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 10-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6873 - acc: 0.5572 - val_loss: 0.6718 - val_acc: 0.5859\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6749 - acc: 0.5791 - val_loss: 0.6699 - val_acc: 0.5900\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6722 - acc: 0.5846 - val_loss: 0.6688 - val_acc: 0.5916\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6703 - acc: 0.5876 - val_loss: 0.6674 - val_acc: 0.5941\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6687 - acc: 0.5905 - val_loss: 0.6662 - val_acc: 0.5955\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6675 - acc: 0.5933 - val_loss: 0.6651 - val_acc: 0.5973\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6659 - acc: 0.5951 - val_loss: 0.6636 - val_acc: 0.5997\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6644 - acc: 0.5991 - val_loss: 0.6628 - val_acc: 0.6016\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6636 - acc: 0.6005 - val_loss: 0.6621 - val_acc: 0.6051\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6624 - acc: 0.6015 - val_loss: 0.6614 - val_acc: 0.6052\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6616 - acc: 0.6035 - val_loss: 0.6609 - val_acc: 0.6061\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6611 - acc: 0.6043 - val_loss: 0.6606 - val_acc: 0.6060\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6603 - acc: 0.6051 - val_loss: 0.6614 - val_acc: 0.6056\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6596 - acc: 0.6067 - val_loss: 0.6603 - val_acc: 0.6073\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6592 - acc: 0.6079 - val_loss: 0.6597 - val_acc: 0.6073\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6585 - acc: 0.6079 - val_loss: 0.6598 - val_acc: 0.6070\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6577 - acc: 0.6084 - val_loss: 0.6593 - val_acc: 0.6094\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6572 - acc: 0.6091 - val_loss: 0.6593 - val_acc: 0.6071\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6565 - acc: 0.6102 - val_loss: 0.6590 - val_acc: 0.6088\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6556 - acc: 0.6109 - val_loss: 0.6591 - val_acc: 0.6089\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6553 - acc: 0.6112 - val_loss: 0.6589 - val_acc: 0.6077\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6546 - acc: 0.6130 - val_loss: 0.6590 - val_acc: 0.6102\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6535 - acc: 0.6142 - val_loss: 0.6583 - val_acc: 0.6095\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6530 - acc: 0.6145 - val_loss: 0.6589 - val_acc: 0.6079\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6524 - acc: 0.6144 - val_loss: 0.6579 - val_acc: 0.6099\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6518 - acc: 0.6160 - val_loss: 0.6581 - val_acc: 0.6100\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6512 - acc: 0.6166 - val_loss: 0.6582 - val_acc: 0.6090\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6504 - acc: 0.6176 - val_loss: 0.6595 - val_acc: 0.6073\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6498 - acc: 0.6188 - val_loss: 0.6575 - val_acc: 0.6100\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6489 - acc: 0.6198 - val_loss: 0.6581 - val_acc: 0.6113\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6485 - acc: 0.6202 - val_loss: 0.6578 - val_acc: 0.6122\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6478 - acc: 0.6211 - val_loss: 0.6574 - val_acc: 0.6109\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6475 - acc: 0.6208 - val_loss: 0.6576 - val_acc: 0.6117\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6469 - acc: 0.6211 - val_loss: 0.6581 - val_acc: 0.6115\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6458 - acc: 0.6230 - val_loss: 0.6572 - val_acc: 0.6116\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6453 - acc: 0.6226 - val_loss: 0.6575 - val_acc: 0.6113\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6450 - acc: 0.6252 - val_loss: 0.6577 - val_acc: 0.6109\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6443 - acc: 0.6247 - val_loss: 0.6583 - val_acc: 0.6087\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6439 - acc: 0.6248 - val_loss: 0.6577 - val_acc: 0.6105\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6428 - acc: 0.6275 - val_loss: 0.6580 - val_acc: 0.6096\n",
            "[20/Nov/2018 02:26:36] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc189a4c0b8>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 11-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 12us/step - loss: 0.6864 - acc: 0.5592 - val_loss: 0.6729 - val_acc: 0.5854\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6743 - acc: 0.5804 - val_loss: 0.6723 - val_acc: 0.5851\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6715 - acc: 0.5849 - val_loss: 0.6712 - val_acc: 0.5862\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6699 - acc: 0.5863 - val_loss: 0.6710 - val_acc: 0.5858\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6690 - acc: 0.5866 - val_loss: 0.6702 - val_acc: 0.5869\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6678 - acc: 0.5885 - val_loss: 0.6692 - val_acc: 0.5877\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6666 - acc: 0.5898 - val_loss: 0.6681 - val_acc: 0.5886\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6653 - acc: 0.5921 - val_loss: 0.6677 - val_acc: 0.5878\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6645 - acc: 0.5929 - val_loss: 0.6672 - val_acc: 0.5902\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6638 - acc: 0.5941 - val_loss: 0.6671 - val_acc: 0.5900\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6631 - acc: 0.5945 - val_loss: 0.6669 - val_acc: 0.5915\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6625 - acc: 0.5956 - val_loss: 0.6661 - val_acc: 0.5926\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6618 - acc: 0.5972 - val_loss: 0.6656 - val_acc: 0.5942\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6611 - acc: 0.5980 - val_loss: 0.6662 - val_acc: 0.5927\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6603 - acc: 0.5989 - val_loss: 0.6654 - val_acc: 0.5950\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6595 - acc: 0.5980 - val_loss: 0.6648 - val_acc: 0.5962\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6586 - acc: 0.6007 - val_loss: 0.6645 - val_acc: 0.5965\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6585 - acc: 0.6009 - val_loss: 0.6637 - val_acc: 0.5978\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6571 - acc: 0.6035 - val_loss: 0.6637 - val_acc: 0.5979\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6566 - acc: 0.6042 - val_loss: 0.6630 - val_acc: 0.5983\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6557 - acc: 0.6046 - val_loss: 0.6623 - val_acc: 0.5995\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6546 - acc: 0.6064 - val_loss: 0.6622 - val_acc: 0.5998\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6540 - acc: 0.6071 - val_loss: 0.6617 - val_acc: 0.5998\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6529 - acc: 0.6088 - val_loss: 0.6617 - val_acc: 0.6002\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6522 - acc: 0.6096 - val_loss: 0.6612 - val_acc: 0.6001\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6516 - acc: 0.6109 - val_loss: 0.6610 - val_acc: 0.6012\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6504 - acc: 0.6120 - val_loss: 0.6618 - val_acc: 0.6023\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6497 - acc: 0.6123 - val_loss: 0.6606 - val_acc: 0.6022\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6490 - acc: 0.6143 - val_loss: 0.6603 - val_acc: 0.6016\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6475 - acc: 0.6164 - val_loss: 0.6601 - val_acc: 0.6042\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6469 - acc: 0.6173 - val_loss: 0.6595 - val_acc: 0.6035\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6460 - acc: 0.6190 - val_loss: 0.6596 - val_acc: 0.6060\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6450 - acc: 0.6200 - val_loss: 0.6608 - val_acc: 0.6041\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6442 - acc: 0.6203 - val_loss: 0.6591 - val_acc: 0.6061\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6435 - acc: 0.6210 - val_loss: 0.6590 - val_acc: 0.6071\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6430 - acc: 0.6216 - val_loss: 0.6588 - val_acc: 0.6064\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6416 - acc: 0.6240 - val_loss: 0.6589 - val_acc: 0.6070\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6409 - acc: 0.6238 - val_loss: 0.6586 - val_acc: 0.6079\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6403 - acc: 0.6246 - val_loss: 0.6584 - val_acc: 0.6078\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6395 - acc: 0.6266 - val_loss: 0.6588 - val_acc: 0.6080\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6384 - acc: 0.6270 - val_loss: 0.6585 - val_acc: 0.6083\n",
            "Epoch 42/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6378 - acc: 0.6288 - val_loss: 0.6583 - val_acc: 0.6094\n",
            "Epoch 43/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6371 - acc: 0.6296 - val_loss: 0.6585 - val_acc: 0.6084\n",
            "Epoch 44/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6360 - acc: 0.6305 - val_loss: 0.6582 - val_acc: 0.6096\n",
            "Epoch 45/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6352 - acc: 0.6300 - val_loss: 0.6581 - val_acc: 0.6090\n",
            "Epoch 46/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6343 - acc: 0.6323 - val_loss: 0.6589 - val_acc: 0.6087\n",
            "Epoch 47/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6340 - acc: 0.6326 - val_loss: 0.6588 - val_acc: 0.6097\n",
            "Epoch 48/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6328 - acc: 0.6352 - val_loss: 0.6583 - val_acc: 0.6099\n",
            "Epoch 49/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6322 - acc: 0.6344 - val_loss: 0.6586 - val_acc: 0.6108\n",
            "Epoch 50/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6308 - acc: 0.6366 - val_loss: 0.6587 - val_acc: 0.6095\n",
            "[20/Nov/2018 02:28:21] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc186bc60f0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 12-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 12us/step - loss: 0.6868 - acc: 0.5602 - val_loss: 0.6713 - val_acc: 0.5890\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6728 - acc: 0.5827 - val_loss: 0.6674 - val_acc: 0.5933\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6670 - acc: 0.5934 - val_loss: 0.6631 - val_acc: 0.6006\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6637 - acc: 0.5987 - val_loss: 0.6614 - val_acc: 0.6033\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6614 - acc: 0.6025 - val_loss: 0.6598 - val_acc: 0.6053\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6598 - acc: 0.6044 - val_loss: 0.6579 - val_acc: 0.6089\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6578 - acc: 0.6085 - val_loss: 0.6570 - val_acc: 0.6095\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6564 - acc: 0.6105 - val_loss: 0.6559 - val_acc: 0.6124\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6554 - acc: 0.6115 - val_loss: 0.6551 - val_acc: 0.6124\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6540 - acc: 0.6143 - val_loss: 0.6547 - val_acc: 0.6122\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6534 - acc: 0.6144 - val_loss: 0.6548 - val_acc: 0.6125\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6524 - acc: 0.6165 - val_loss: 0.6546 - val_acc: 0.6137\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6515 - acc: 0.6176 - val_loss: 0.6541 - val_acc: 0.6151\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6510 - acc: 0.6183 - val_loss: 0.6533 - val_acc: 0.6151\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6500 - acc: 0.6186 - val_loss: 0.6532 - val_acc: 0.6151\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6493 - acc: 0.6199 - val_loss: 0.6531 - val_acc: 0.6160\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6486 - acc: 0.6215 - val_loss: 0.6528 - val_acc: 0.6158\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6478 - acc: 0.6216 - val_loss: 0.6527 - val_acc: 0.6160\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6471 - acc: 0.6226 - val_loss: 0.6525 - val_acc: 0.6154\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6464 - acc: 0.6234 - val_loss: 0.6526 - val_acc: 0.6163\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6456 - acc: 0.6245 - val_loss: 0.6520 - val_acc: 0.6168\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6451 - acc: 0.6241 - val_loss: 0.6517 - val_acc: 0.6174\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6441 - acc: 0.6263 - val_loss: 0.6520 - val_acc: 0.6159\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6437 - acc: 0.6258 - val_loss: 0.6521 - val_acc: 0.6175\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6426 - acc: 0.6272 - val_loss: 0.6523 - val_acc: 0.6164\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6418 - acc: 0.6294 - val_loss: 0.6519 - val_acc: 0.6181\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6413 - acc: 0.6287 - val_loss: 0.6516 - val_acc: 0.6171\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6408 - acc: 0.6292 - val_loss: 0.6518 - val_acc: 0.6179\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.6304 - val_loss: 0.6511 - val_acc: 0.6185\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6394 - acc: 0.6305 - val_loss: 0.6530 - val_acc: 0.6180\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6382 - acc: 0.6322 - val_loss: 0.6514 - val_acc: 0.6183\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6374 - acc: 0.6328 - val_loss: 0.6513 - val_acc: 0.6189\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6366 - acc: 0.6334 - val_loss: 0.6510 - val_acc: 0.6185\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6364 - acc: 0.6339 - val_loss: 0.6524 - val_acc: 0.6172\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6356 - acc: 0.6344 - val_loss: 0.6514 - val_acc: 0.6184\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6351 - acc: 0.6347 - val_loss: 0.6521 - val_acc: 0.6194\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6341 - acc: 0.6375 - val_loss: 0.6507 - val_acc: 0.6185\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6336 - acc: 0.6366 - val_loss: 0.6517 - val_acc: 0.6184\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6323 - acc: 0.6378 - val_loss: 0.6511 - val_acc: 0.6186\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6322 - acc: 0.6390 - val_loss: 0.6512 - val_acc: 0.6199\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6317 - acc: 0.6396 - val_loss: 0.6513 - val_acc: 0.6194\n",
            "Epoch 42/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6305 - acc: 0.6393 - val_loss: 0.6509 - val_acc: 0.6189\n",
            "[20/Nov/2018 02:29:49] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc183d5c198>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 13-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 12us/step - loss: 0.6807 - acc: 0.5729 - val_loss: 0.6642 - val_acc: 0.5999\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6673 - acc: 0.5951 - val_loss: 0.6628 - val_acc: 0.6011\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6648 - acc: 0.5989 - val_loss: 0.6631 - val_acc: 0.5991\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6633 - acc: 0.6010 - val_loss: 0.6616 - val_acc: 0.6019\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6625 - acc: 0.6011 - val_loss: 0.6609 - val_acc: 0.6027\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6616 - acc: 0.6021 - val_loss: 0.6604 - val_acc: 0.6029\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6607 - acc: 0.6032 - val_loss: 0.6592 - val_acc: 0.6059\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6594 - acc: 0.6058 - val_loss: 0.6584 - val_acc: 0.6074\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6585 - acc: 0.6068 - val_loss: 0.6581 - val_acc: 0.6075\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6579 - acc: 0.6090 - val_loss: 0.6569 - val_acc: 0.6093\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6570 - acc: 0.6089 - val_loss: 0.6563 - val_acc: 0.6103\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6559 - acc: 0.6109 - val_loss: 0.6564 - val_acc: 0.6103\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6554 - acc: 0.6114 - val_loss: 0.6558 - val_acc: 0.6107\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6549 - acc: 0.6121 - val_loss: 0.6552 - val_acc: 0.6124\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6540 - acc: 0.6134 - val_loss: 0.6551 - val_acc: 0.6120\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6536 - acc: 0.6131 - val_loss: 0.6545 - val_acc: 0.6133\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6527 - acc: 0.6146 - val_loss: 0.6544 - val_acc: 0.6129\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6520 - acc: 0.6159 - val_loss: 0.6541 - val_acc: 0.6135\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6511 - acc: 0.6161 - val_loss: 0.6540 - val_acc: 0.6145\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6508 - acc: 0.6174 - val_loss: 0.6542 - val_acc: 0.6141\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6499 - acc: 0.6182 - val_loss: 0.6538 - val_acc: 0.6148\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6489 - acc: 0.6195 - val_loss: 0.6533 - val_acc: 0.6156\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6487 - acc: 0.6201 - val_loss: 0.6540 - val_acc: 0.6144\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6476 - acc: 0.6205 - val_loss: 0.6530 - val_acc: 0.6166\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6469 - acc: 0.6222 - val_loss: 0.6533 - val_acc: 0.6168\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6462 - acc: 0.6227 - val_loss: 0.6539 - val_acc: 0.6157\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6454 - acc: 0.6234 - val_loss: 0.6524 - val_acc: 0.6177\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6444 - acc: 0.6246 - val_loss: 0.6526 - val_acc: 0.6182\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6438 - acc: 0.6251 - val_loss: 0.6530 - val_acc: 0.6177\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6432 - acc: 0.6251 - val_loss: 0.6526 - val_acc: 0.6181\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6421 - acc: 0.6265 - val_loss: 0.6529 - val_acc: 0.6180\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6412 - acc: 0.6277 - val_loss: 0.6527 - val_acc: 0.6182\n",
            "[20/Nov/2018 02:30:57] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc180ed8208>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 14-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 12us/step - loss: 0.6969 - acc: 0.5464 - val_loss: 0.6753 - val_acc: 0.5801\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6798 - acc: 0.5696 - val_loss: 0.6738 - val_acc: 0.5836\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6766 - acc: 0.5761 - val_loss: 0.6738 - val_acc: 0.5825\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6750 - acc: 0.5807 - val_loss: 0.6721 - val_acc: 0.5874\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6736 - acc: 0.5817 - val_loss: 0.6713 - val_acc: 0.5879\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6728 - acc: 0.5837 - val_loss: 0.6708 - val_acc: 0.5865\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6723 - acc: 0.5842 - val_loss: 0.6708 - val_acc: 0.5873\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6717 - acc: 0.5848 - val_loss: 0.6700 - val_acc: 0.5881\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6710 - acc: 0.5860 - val_loss: 0.6700 - val_acc: 0.5890\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6704 - acc: 0.5868 - val_loss: 0.6695 - val_acc: 0.5894\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6699 - acc: 0.5880 - val_loss: 0.6695 - val_acc: 0.5895\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6693 - acc: 0.5887 - val_loss: 0.6685 - val_acc: 0.5912\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6687 - acc: 0.5899 - val_loss: 0.6685 - val_acc: 0.5914\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6679 - acc: 0.5913 - val_loss: 0.6678 - val_acc: 0.5929\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6676 - acc: 0.5914 - val_loss: 0.6678 - val_acc: 0.5924\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6668 - acc: 0.5933 - val_loss: 0.6680 - val_acc: 0.5915\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6663 - acc: 0.5938 - val_loss: 0.6675 - val_acc: 0.5927\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6656 - acc: 0.5939 - val_loss: 0.6667 - val_acc: 0.5936\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6649 - acc: 0.5953 - val_loss: 0.6671 - val_acc: 0.5932\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6645 - acc: 0.5949 - val_loss: 0.6661 - val_acc: 0.5941\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6641 - acc: 0.5965 - val_loss: 0.6659 - val_acc: 0.5948\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6630 - acc: 0.5977 - val_loss: 0.6659 - val_acc: 0.5943\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6623 - acc: 0.6001 - val_loss: 0.6655 - val_acc: 0.5947\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6618 - acc: 0.6001 - val_loss: 0.6655 - val_acc: 0.5952\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6611 - acc: 0.6007 - val_loss: 0.6654 - val_acc: 0.5954\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6604 - acc: 0.6018 - val_loss: 0.6658 - val_acc: 0.5952\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6591 - acc: 0.6041 - val_loss: 0.6647 - val_acc: 0.5969\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6585 - acc: 0.6043 - val_loss: 0.6646 - val_acc: 0.5970\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6581 - acc: 0.6039 - val_loss: 0.6653 - val_acc: 0.5947\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6574 - acc: 0.6071 - val_loss: 0.6648 - val_acc: 0.5968\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6567 - acc: 0.6062 - val_loss: 0.6655 - val_acc: 0.5958\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6558 - acc: 0.6076 - val_loss: 0.6645 - val_acc: 0.5970\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6554 - acc: 0.6085 - val_loss: 0.6646 - val_acc: 0.5975\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6544 - acc: 0.6093 - val_loss: 0.6640 - val_acc: 0.5982\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6537 - acc: 0.6105 - val_loss: 0.6641 - val_acc: 0.5979\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6535 - acc: 0.6099 - val_loss: 0.6647 - val_acc: 0.5976\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6529 - acc: 0.6120 - val_loss: 0.6645 - val_acc: 0.5979\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6517 - acc: 0.6125 - val_loss: 0.6644 - val_acc: 0.5981\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6509 - acc: 0.6140 - val_loss: 0.6646 - val_acc: 0.5986\n",
            "[20/Nov/2018 02:32:19] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc17dff9278>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 15-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 12us/step - loss: 0.6964 - acc: 0.5369 - val_loss: 0.6803 - val_acc: 0.5642\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6834 - acc: 0.5574 - val_loss: 0.6789 - val_acc: 0.5679\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6803 - acc: 0.5627 - val_loss: 0.6781 - val_acc: 0.5671\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6785 - acc: 0.5661 - val_loss: 0.6771 - val_acc: 0.5716\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6769 - acc: 0.5688 - val_loss: 0.6757 - val_acc: 0.5767\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6753 - acc: 0.5734 - val_loss: 0.6735 - val_acc: 0.5819\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6729 - acc: 0.5807 - val_loss: 0.6707 - val_acc: 0.5882\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6703 - acc: 0.5868 - val_loss: 0.6684 - val_acc: 0.5900\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6683 - acc: 0.5901 - val_loss: 0.6666 - val_acc: 0.5957\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6660 - acc: 0.5959 - val_loss: 0.6648 - val_acc: 0.5983\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6642 - acc: 0.5983 - val_loss: 0.6643 - val_acc: 0.5998\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6633 - acc: 0.6003 - val_loss: 0.6632 - val_acc: 0.6014\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6624 - acc: 0.6025 - val_loss: 0.6623 - val_acc: 0.6022\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6615 - acc: 0.6036 - val_loss: 0.6620 - val_acc: 0.6027\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6606 - acc: 0.6044 - val_loss: 0.6613 - val_acc: 0.6040\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6596 - acc: 0.6067 - val_loss: 0.6608 - val_acc: 0.6044\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6586 - acc: 0.6078 - val_loss: 0.6607 - val_acc: 0.6050\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6581 - acc: 0.6084 - val_loss: 0.6608 - val_acc: 0.6045\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6576 - acc: 0.6100 - val_loss: 0.6599 - val_acc: 0.6055\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6563 - acc: 0.6119 - val_loss: 0.6595 - val_acc: 0.6077\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6554 - acc: 0.6123 - val_loss: 0.6591 - val_acc: 0.6083\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6546 - acc: 0.6144 - val_loss: 0.6592 - val_acc: 0.6073\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6536 - acc: 0.6158 - val_loss: 0.6595 - val_acc: 0.6081\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6532 - acc: 0.6149 - val_loss: 0.6585 - val_acc: 0.6091\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6522 - acc: 0.6165 - val_loss: 0.6589 - val_acc: 0.6080\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6512 - acc: 0.6190 - val_loss: 0.6587 - val_acc: 0.6096\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6509 - acc: 0.6180 - val_loss: 0.6580 - val_acc: 0.6094\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6493 - acc: 0.6201 - val_loss: 0.6581 - val_acc: 0.6098\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6490 - acc: 0.6207 - val_loss: 0.6580 - val_acc: 0.6101\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6480 - acc: 0.6219 - val_loss: 0.6575 - val_acc: 0.6111\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6469 - acc: 0.6233 - val_loss: 0.6581 - val_acc: 0.6093\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6463 - acc: 0.6237 - val_loss: 0.6578 - val_acc: 0.6114\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6457 - acc: 0.6247 - val_loss: 0.6580 - val_acc: 0.6097\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6447 - acc: 0.6260 - val_loss: 0.6578 - val_acc: 0.6099\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6435 - acc: 0.6270 - val_loss: 0.6582 - val_acc: 0.6114\n",
            "[20/Nov/2018 02:33:33] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc17b1702b0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 16-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 12us/step - loss: 0.6919 - acc: 0.5496 - val_loss: 0.6767 - val_acc: 0.5739\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6794 - acc: 0.5686 - val_loss: 0.6766 - val_acc: 0.5740\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6776 - acc: 0.5719 - val_loss: 0.6762 - val_acc: 0.5752\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6769 - acc: 0.5724 - val_loss: 0.6758 - val_acc: 0.5747\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6761 - acc: 0.5727 - val_loss: 0.6758 - val_acc: 0.5748\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6757 - acc: 0.5734 - val_loss: 0.6758 - val_acc: 0.5746\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6752 - acc: 0.5752 - val_loss: 0.6754 - val_acc: 0.5756\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6749 - acc: 0.5750 - val_loss: 0.6749 - val_acc: 0.5759\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6739 - acc: 0.5773 - val_loss: 0.6740 - val_acc: 0.5784\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6726 - acc: 0.5802 - val_loss: 0.6723 - val_acc: 0.5803\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6717 - acc: 0.5817 - val_loss: 0.6718 - val_acc: 0.5816\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6708 - acc: 0.5835 - val_loss: 0.6708 - val_acc: 0.5843\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6697 - acc: 0.5849 - val_loss: 0.6701 - val_acc: 0.5865\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6691 - acc: 0.5862 - val_loss: 0.6691 - val_acc: 0.5882\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6678 - acc: 0.5883 - val_loss: 0.6683 - val_acc: 0.5905\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6667 - acc: 0.5910 - val_loss: 0.6677 - val_acc: 0.5918\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6659 - acc: 0.5913 - val_loss: 0.6670 - val_acc: 0.5923\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6649 - acc: 0.5944 - val_loss: 0.6663 - val_acc: 0.5937\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6640 - acc: 0.5960 - val_loss: 0.6658 - val_acc: 0.5957\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6629 - acc: 0.5986 - val_loss: 0.6652 - val_acc: 0.5960\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6620 - acc: 0.5996 - val_loss: 0.6641 - val_acc: 0.5985\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6611 - acc: 0.6015 - val_loss: 0.6636 - val_acc: 0.5998\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6602 - acc: 0.6032 - val_loss: 0.6636 - val_acc: 0.5992\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6588 - acc: 0.6043 - val_loss: 0.6625 - val_acc: 0.6008\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6583 - acc: 0.6046 - val_loss: 0.6625 - val_acc: 0.6011\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6573 - acc: 0.6057 - val_loss: 0.6623 - val_acc: 0.6023\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6564 - acc: 0.6075 - val_loss: 0.6621 - val_acc: 0.6023\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6558 - acc: 0.6079 - val_loss: 0.6618 - val_acc: 0.6032\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6548 - acc: 0.6110 - val_loss: 0.6621 - val_acc: 0.6023\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6540 - acc: 0.6106 - val_loss: 0.6616 - val_acc: 0.6031\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6534 - acc: 0.6109 - val_loss: 0.6610 - val_acc: 0.6027\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6518 - acc: 0.6129 - val_loss: 0.6612 - val_acc: 0.6038\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6511 - acc: 0.6147 - val_loss: 0.6609 - val_acc: 0.6052\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6505 - acc: 0.6152 - val_loss: 0.6616 - val_acc: 0.6027\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6496 - acc: 0.6163 - val_loss: 0.6604 - val_acc: 0.6043\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6486 - acc: 0.6176 - val_loss: 0.6602 - val_acc: 0.6054\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6478 - acc: 0.6187 - val_loss: 0.6605 - val_acc: 0.6060\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6468 - acc: 0.6198 - val_loss: 0.6604 - val_acc: 0.6053\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6457 - acc: 0.6206 - val_loss: 0.6604 - val_acc: 0.6065\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6448 - acc: 0.6214 - val_loss: 0.6607 - val_acc: 0.6056\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6439 - acc: 0.6238 - val_loss: 0.6604 - val_acc: 0.6061\n",
            "[20/Nov/2018 02:34:58] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc178292358>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 17-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 12us/step - loss: 0.7015 - acc: 0.5249 - val_loss: 0.6845 - val_acc: 0.5524\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6869 - acc: 0.5454 - val_loss: 0.6836 - val_acc: 0.5550\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6841 - acc: 0.5528 - val_loss: 0.6843 - val_acc: 0.5511\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6818 - acc: 0.5569 - val_loss: 0.6804 - val_acc: 0.5629\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6795 - acc: 0.5621 - val_loss: 0.6788 - val_acc: 0.5675\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6777 - acc: 0.5675 - val_loss: 0.6764 - val_acc: 0.5716\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6758 - acc: 0.5714 - val_loss: 0.6747 - val_acc: 0.5775\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6741 - acc: 0.5762 - val_loss: 0.6730 - val_acc: 0.5815\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6722 - acc: 0.5807 - val_loss: 0.6712 - val_acc: 0.5838\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6704 - acc: 0.5850 - val_loss: 0.6698 - val_acc: 0.5879\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6690 - acc: 0.5880 - val_loss: 0.6685 - val_acc: 0.5911\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6678 - acc: 0.5902 - val_loss: 0.6673 - val_acc: 0.5935\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6671 - acc: 0.5908 - val_loss: 0.6670 - val_acc: 0.5936\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6655 - acc: 0.5950 - val_loss: 0.6661 - val_acc: 0.5970\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6648 - acc: 0.5953 - val_loss: 0.6663 - val_acc: 0.5959\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6641 - acc: 0.5976 - val_loss: 0.6650 - val_acc: 0.5980\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6629 - acc: 0.5986 - val_loss: 0.6646 - val_acc: 0.5986\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6617 - acc: 0.6007 - val_loss: 0.6642 - val_acc: 0.6006\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6610 - acc: 0.6006 - val_loss: 0.6653 - val_acc: 0.6003\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6603 - acc: 0.6030 - val_loss: 0.6630 - val_acc: 0.6026\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6593 - acc: 0.6033 - val_loss: 0.6627 - val_acc: 0.6030\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6583 - acc: 0.6063 - val_loss: 0.6624 - val_acc: 0.6037\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6578 - acc: 0.6053 - val_loss: 0.6623 - val_acc: 0.6039\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6566 - acc: 0.6075 - val_loss: 0.6621 - val_acc: 0.6038\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6559 - acc: 0.6080 - val_loss: 0.6616 - val_acc: 0.6040\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6553 - acc: 0.6089 - val_loss: 0.6610 - val_acc: 0.6055\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6543 - acc: 0.6102 - val_loss: 0.6611 - val_acc: 0.6054\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6535 - acc: 0.6111 - val_loss: 0.6606 - val_acc: 0.6058\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6523 - acc: 0.6119 - val_loss: 0.6608 - val_acc: 0.6060\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6519 - acc: 0.6138 - val_loss: 0.6602 - val_acc: 0.6069\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6508 - acc: 0.6136 - val_loss: 0.6605 - val_acc: 0.6068\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6498 - acc: 0.6152 - val_loss: 0.6596 - val_acc: 0.6059\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6490 - acc: 0.6167 - val_loss: 0.6598 - val_acc: 0.6070\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6484 - acc: 0.6176 - val_loss: 0.6595 - val_acc: 0.6055\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6476 - acc: 0.6189 - val_loss: 0.6593 - val_acc: 0.6078\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6460 - acc: 0.6205 - val_loss: 0.6593 - val_acc: 0.6064\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6451 - acc: 0.6222 - val_loss: 0.6594 - val_acc: 0.6082\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6447 - acc: 0.6219 - val_loss: 0.6589 - val_acc: 0.6090\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6442 - acc: 0.6226 - val_loss: 0.6590 - val_acc: 0.6076\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6427 - acc: 0.6243 - val_loss: 0.6591 - val_acc: 0.6072\n",
            "Epoch 41/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6425 - acc: 0.6241 - val_loss: 0.6591 - val_acc: 0.6076\n",
            "Epoch 42/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6407 - acc: 0.6270 - val_loss: 0.6588 - val_acc: 0.6081\n",
            "Epoch 43/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6400 - acc: 0.6285 - val_loss: 0.6589 - val_acc: 0.6083\n",
            "Epoch 44/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6392 - acc: 0.6287 - val_loss: 0.6588 - val_acc: 0.6086\n",
            "Epoch 45/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6386 - acc: 0.6292 - val_loss: 0.6585 - val_acc: 0.6095\n",
            "Epoch 46/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6375 - acc: 0.6305 - val_loss: 0.6582 - val_acc: 0.6093\n",
            "Epoch 47/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6364 - acc: 0.6313 - val_loss: 0.6595 - val_acc: 0.6087\n",
            "Epoch 48/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6362 - acc: 0.6313 - val_loss: 0.6589 - val_acc: 0.6071\n",
            "Epoch 49/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6343 - acc: 0.6341 - val_loss: 0.6591 - val_acc: 0.6077\n",
            "Epoch 50/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6342 - acc: 0.6346 - val_loss: 0.6597 - val_acc: 0.6073\n",
            "Epoch 51/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6328 - acc: 0.6365 - val_loss: 0.6587 - val_acc: 0.6097\n",
            "[20/Nov/2018 02:36:43] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc1754083c8>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 18-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 11us/step - loss: 0.6861 - acc: 0.5629 - val_loss: 0.6699 - val_acc: 0.5902\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6715 - acc: 0.5860 - val_loss: 0.6690 - val_acc: 0.5903\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6691 - acc: 0.5895 - val_loss: 0.6674 - val_acc: 0.5927\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6676 - acc: 0.5916 - val_loss: 0.6669 - val_acc: 0.5931\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6666 - acc: 0.5929 - val_loss: 0.6659 - val_acc: 0.5952\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6652 - acc: 0.5948 - val_loss: 0.6650 - val_acc: 0.5971\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6636 - acc: 0.5974 - val_loss: 0.6630 - val_acc: 0.6004\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6622 - acc: 0.5994 - val_loss: 0.6617 - val_acc: 0.6036\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6612 - acc: 0.6015 - val_loss: 0.6608 - val_acc: 0.6043\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6599 - acc: 0.6045 - val_loss: 0.6595 - val_acc: 0.6071\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6589 - acc: 0.6055 - val_loss: 0.6584 - val_acc: 0.6095\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6572 - acc: 0.6084 - val_loss: 0.6576 - val_acc: 0.6108\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6563 - acc: 0.6102 - val_loss: 0.6579 - val_acc: 0.6086\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6560 - acc: 0.6106 - val_loss: 0.6568 - val_acc: 0.6110\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6547 - acc: 0.6128 - val_loss: 0.6566 - val_acc: 0.6116\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6541 - acc: 0.6127 - val_loss: 0.6570 - val_acc: 0.6120\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6534 - acc: 0.6136 - val_loss: 0.6557 - val_acc: 0.6137\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6524 - acc: 0.6151 - val_loss: 0.6551 - val_acc: 0.6136\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6515 - acc: 0.6162 - val_loss: 0.6549 - val_acc: 0.6151\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6509 - acc: 0.6182 - val_loss: 0.6542 - val_acc: 0.6155\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6502 - acc: 0.6182 - val_loss: 0.6542 - val_acc: 0.6156\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6493 - acc: 0.6191 - val_loss: 0.6534 - val_acc: 0.6169\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6482 - acc: 0.6207 - val_loss: 0.6535 - val_acc: 0.6160\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6477 - acc: 0.6214 - val_loss: 0.6529 - val_acc: 0.6173\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6468 - acc: 0.6221 - val_loss: 0.6534 - val_acc: 0.6168\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6460 - acc: 0.6234 - val_loss: 0.6531 - val_acc: 0.6162\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6450 - acc: 0.6231 - val_loss: 0.6529 - val_acc: 0.6181\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6447 - acc: 0.6246 - val_loss: 0.6523 - val_acc: 0.6182\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6443 - acc: 0.6244 - val_loss: 0.6524 - val_acc: 0.6168\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6429 - acc: 0.6265 - val_loss: 0.6522 - val_acc: 0.6184\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6417 - acc: 0.6273 - val_loss: 0.6525 - val_acc: 0.6180\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6414 - acc: 0.6273 - val_loss: 0.6519 - val_acc: 0.6193\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6401 - acc: 0.6287 - val_loss: 0.6520 - val_acc: 0.6187\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6400 - acc: 0.6292 - val_loss: 0.6521 - val_acc: 0.6190\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6386 - acc: 0.6308 - val_loss: 0.6518 - val_acc: 0.6201\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6380 - acc: 0.6316 - val_loss: 0.6520 - val_acc: 0.6200\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6366 - acc: 0.6323 - val_loss: 0.6521 - val_acc: 0.6194\n",
            "Epoch 38/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6361 - acc: 0.6339 - val_loss: 0.6520 - val_acc: 0.6204\n",
            "Epoch 39/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6359 - acc: 0.6331 - val_loss: 0.6527 - val_acc: 0.6186\n",
            "Epoch 40/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6347 - acc: 0.6349 - val_loss: 0.6522 - val_acc: 0.6210\n",
            "[20/Nov/2018 02:38:05] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc1725aa3c8>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n",
            "\n",
            "-----------------Training MLP 19-----------------\n",
            "\n",
            "Train on 204078 samples, validate on 127550 samples\n",
            "Epoch 1/10000\n",
            "204078/204078 [==============================] - 2s 12us/step - loss: 0.6828 - acc: 0.5658 - val_loss: 0.6685 - val_acc: 0.5917\n",
            "Epoch 2/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6705 - acc: 0.5859 - val_loss: 0.6676 - val_acc: 0.5907\n",
            "Epoch 3/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6683 - acc: 0.5890 - val_loss: 0.6656 - val_acc: 0.5930\n",
            "Epoch 4/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6658 - acc: 0.5936 - val_loss: 0.6626 - val_acc: 0.6006\n",
            "Epoch 5/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6632 - acc: 0.5977 - val_loss: 0.6605 - val_acc: 0.6034\n",
            "Epoch 6/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6613 - acc: 0.6002 - val_loss: 0.6590 - val_acc: 0.6050\n",
            "Epoch 7/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6601 - acc: 0.6016 - val_loss: 0.6585 - val_acc: 0.6041\n",
            "Epoch 8/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6588 - acc: 0.6043 - val_loss: 0.6581 - val_acc: 0.6076\n",
            "Epoch 9/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6580 - acc: 0.6048 - val_loss: 0.6570 - val_acc: 0.6079\n",
            "Epoch 10/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6566 - acc: 0.6074 - val_loss: 0.6559 - val_acc: 0.6104\n",
            "Epoch 11/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6559 - acc: 0.6086 - val_loss: 0.6553 - val_acc: 0.6100\n",
            "Epoch 12/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6545 - acc: 0.6106 - val_loss: 0.6546 - val_acc: 0.6127\n",
            "Epoch 13/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6538 - acc: 0.6115 - val_loss: 0.6547 - val_acc: 0.6114\n",
            "Epoch 14/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6530 - acc: 0.6128 - val_loss: 0.6541 - val_acc: 0.6137\n",
            "Epoch 15/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6519 - acc: 0.6139 - val_loss: 0.6537 - val_acc: 0.6143\n",
            "Epoch 16/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6516 - acc: 0.6148 - val_loss: 0.6537 - val_acc: 0.6129\n",
            "Epoch 17/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6506 - acc: 0.6165 - val_loss: 0.6532 - val_acc: 0.6139\n",
            "Epoch 18/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6501 - acc: 0.6167 - val_loss: 0.6530 - val_acc: 0.6139\n",
            "Epoch 19/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6492 - acc: 0.6180 - val_loss: 0.6531 - val_acc: 0.6156\n",
            "Epoch 20/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6486 - acc: 0.6185 - val_loss: 0.6533 - val_acc: 0.6140\n",
            "Epoch 21/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6478 - acc: 0.6189 - val_loss: 0.6529 - val_acc: 0.6148\n",
            "Epoch 22/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6475 - acc: 0.6193 - val_loss: 0.6525 - val_acc: 0.6160\n",
            "Epoch 23/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6465 - acc: 0.6208 - val_loss: 0.6520 - val_acc: 0.6159\n",
            "Epoch 24/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6452 - acc: 0.6223 - val_loss: 0.6519 - val_acc: 0.6169\n",
            "Epoch 25/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6446 - acc: 0.6237 - val_loss: 0.6518 - val_acc: 0.6166\n",
            "Epoch 26/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6441 - acc: 0.6238 - val_loss: 0.6521 - val_acc: 0.6172\n",
            "Epoch 27/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6431 - acc: 0.6259 - val_loss: 0.6524 - val_acc: 0.6161\n",
            "Epoch 28/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6425 - acc: 0.6251 - val_loss: 0.6516 - val_acc: 0.6171\n",
            "Epoch 29/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6414 - acc: 0.6268 - val_loss: 0.6513 - val_acc: 0.6159\n",
            "Epoch 30/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6407 - acc: 0.6277 - val_loss: 0.6514 - val_acc: 0.6167\n",
            "Epoch 31/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6402 - acc: 0.6284 - val_loss: 0.6515 - val_acc: 0.6165\n",
            "Epoch 32/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6394 - acc: 0.6285 - val_loss: 0.6510 - val_acc: 0.6181\n",
            "Epoch 33/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6389 - acc: 0.6299 - val_loss: 0.6517 - val_acc: 0.6172\n",
            "Epoch 34/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6377 - acc: 0.6310 - val_loss: 0.6512 - val_acc: 0.6162\n",
            "Epoch 35/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6372 - acc: 0.6313 - val_loss: 0.6514 - val_acc: 0.6162\n",
            "Epoch 36/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6361 - acc: 0.6320 - val_loss: 0.6511 - val_acc: 0.6181\n",
            "Epoch 37/10000\n",
            "204078/204078 [==============================] - 2s 10us/step - loss: 0.6350 - acc: 0.6331 - val_loss: 0.6516 - val_acc: 0.6153\n",
            "[20/Nov/2018 02:39:22] WARNING - This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc16f71fe48>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
            "\n",
            "Consider using a TensorFlow optimizer from `tf.train`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rhjtCfbUAVeF",
        "colab_type": "code",
        "outputId": "543b2433-a8a8-4c0c-b9d0-d9f47232ea5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "cell_type": "code",
      "source": [
        "evaluate_model(features, params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "127548/127548 [==============================] - 0s 2us/step\n",
            "Accuray: 0.6367093172766332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BynnaZnV_suQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}